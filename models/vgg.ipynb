{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MbtUcKfE_I4g"
      },
      "source": [
        "**Installs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tzg4cO9xLvUG",
        "trusted": false,
        "colab": {}
      },
      "source": [
        "!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fxs_3zcG_NZd"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C7N0hU-VLx8W",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vgg19\n",
        "from torchvision.models import vgg19_bn\n",
        "from torchvision.models import vgg16\n",
        "from torchvision.models import alexnet\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "#NUM_CLASSES = 102\n",
        "NUM_CLASSES = 6\n",
        "DEVICE = 'cuda'\n",
        "MOMENTUM = 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uvABcepY_Vfe"
      },
      "source": [
        "**Model definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vztVCv3fQXjR",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def get_datasets(train_data_dir, test_data_dir, compose=[transforms.Resize(224),\n",
        "                                                         transforms.CenterCrop(224),\n",
        "                                                         transforms.ToTensor()#,\n",
        "                                                         #transforms.Normalize((75.29522728, 26.30439561, 70.34910019), (80.67869619, 35.54419227, 54.88938911))\n",
        "                                                         ]):\n",
        "    train_transform = transforms.Compose(compose)\n",
        "    eval_transform = transforms.Compose([\n",
        "          transforms.Resize(224),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor()\n",
        "          ])\n",
        "\n",
        "    '''\n",
        "    if not os.path.isdir('./Homework2-Caltech101'):\n",
        "        !git clone https://github.com/MachineLearning2020/Homework2-Caltech101.git\n",
        "\n",
        "    '''\n",
        "    if not os.path.isdir('./AIML_project'):\n",
        "        !git clone https://github.com/anphetamina/AIML_project.git\n",
        "    \n",
        "    train_dataset = torchvision.datasets.ImageFolder(train_data_dir, transform=train_transform)\n",
        "    test_dataset = torchvision.datasets.ImageFolder(test_data_dir, transform=eval_transform)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "def test_network(net, test_dataset, batch_size):\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "    net.train(False)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    sum_test_losses = 0.0\n",
        "    running_corrects = 0\n",
        "    for images, labels in test_dataloader:\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      # Forward Pass\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      test_loss = criterion(outputs, labels)\n",
        "      sum_test_losses += test_loss.item()*images.size(0)\n",
        "\n",
        "      # Update Corrects\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "    # Calculate loss\n",
        "    test_loss = sum_test_losses / float(len(test_dataset))\n",
        "\n",
        "    return accuracy, test_loss\n",
        "\n",
        "def train_network(net, parameters_to_optimize, learning_rate, num_epochs, batch_size, weight_decay, step_size, gamma, train_dataset, val_dataset=None, verbosity=False, plot=False):\n",
        "  \n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=False)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=learning_rate, momentum=MOMENTUM, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    net = net.to(DEVICE)\n",
        "    best_net = vgg19()\n",
        "    best_net = best_net.to(DEVICE)\n",
        "    best_net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    train_accuracies = []\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "    val_losses = []\n",
        "\n",
        "    current_step = 0\n",
        "    best_val_accuracy = 0.0\n",
        "    best_val_loss = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_running_corrects = 0\n",
        "        sum_train_losses = 0.0\n",
        "\n",
        "        for images, labels in train_dataloader:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            net.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            train_running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "            loss = criterion(outputs, labels)\n",
        "            sum_train_losses += loss.item()*images.size(0)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            current_step += 1\n",
        "        \n",
        "        if val_dataset is not None:\n",
        "            val_accuracy, val_loss = test_network(net, val_dataset, batch_size)\n",
        "            if val_accuracy > best_val_accuracy:\n",
        "                best_val_accuracy = val_accuracy\n",
        "                best_val_loss = val_loss\n",
        "                best_net.load_state_dict(net.state_dict())\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "        # Calculate accuracy on train set\n",
        "        train_accuracy = train_running_corrects / float(len(train_dataset))\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Calculate loss on training set\n",
        "        train_loss = sum_train_losses/float(len(train_dataset))\n",
        "        train_losses.append(loss)\n",
        "\n",
        "        if verbosity:\n",
        "            if val_dataset is not None:\n",
        "                print(\"train_acc: {}, val_acc: {}, train_loss: {}, val_loss: {} ({} / {})\".format(train_accuracy, val_accuracy, train_loss, val_loss, epoch+1, num_epochs))\n",
        "            else:\n",
        "                print(\"train_acc: {}, train_loss: {} ({} / {})\".format(train_accuracy, train_loss, epoch+1, num_epochs))\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    if plot:\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        line1, = ax.plot(train_losses, label='Loss on training set')\n",
        "        line2, = ax.plot(train_accuracies, label='Accuracy on training set')\n",
        "        ax.legend()\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.show()\n",
        "\n",
        "        if val_dataset is not None:\n",
        "            fig, ax = plt.subplots()\n",
        "            line1, = ax.plot(val_accuracies, label='Accuracy on validation set', color='C2')\n",
        "            line2, = ax.plot(train_accuracies, label='Accuracy on training set', color='C3')\n",
        "            ax.legend()\n",
        "            plt.xlabel(\"Epochs\")\n",
        "            plt.show()\n",
        "        \n",
        "            fig, ax = plt.subplots()\n",
        "            line1, = ax.plot(val_losses, label='Loss on validation set', color='C1')\n",
        "            line2, = ax.plot(train_losses, label='Loss on training set', color='C7')\n",
        "            ax.legend()\n",
        "            plt.xlabel(\"Epochs\")\n",
        "            plt.show()\n",
        "\n",
        "    \n",
        "    return best_net, best_val_accuracy, best_val_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I6fTm2sD_BOt"
      },
      "source": [
        "**Train + validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XtBXC1cO_A6A",
        "outputId": "1d813cf7-0e95-435c-ec92-f807a4ad2489",
        "trusted": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "LR = 0.001\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-5\n",
        "NUM_EPOCHS = 100\n",
        "STEP_SIZE = 60\n",
        "GAMMA = 0.05\n",
        "\n",
        "TRAIN_DATA_DIR = 'AIML_project/ravdess-emotional-song-spec'\n",
        "#TRAIN_DATA_DIR = 'Homework2-Caltech101/101_ObjectCategories'\n",
        "compose=[#transforms.Resize(224),\n",
        "         transforms.CenterCrop(224),\n",
        "         transforms.RandomGrayscale(),\n",
        "         transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
        "         transforms.ToTensor()\n",
        "         ]\n",
        "train_dataset, val_dataset = get_datasets(TRAIN_DATA_DIR, TRAIN_DATA_DIR, compose)\n",
        "train_indexes = [idx for idx in range(len(train_dataset)) if idx % 5]\n",
        "val_indexes = [idx for idx in range(len(train_dataset)) if not idx % 5]\n",
        "val_dataset = Subset(val_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "print('training set {}'.format(len(train_dataset)))\n",
        "print('validation set {}'.format(len(val_dataset)))\n",
        "\n",
        "net = vgg19()\n",
        "net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "best_net, val_accuracy, val_loss = train_network(net, net.parameters(), LR, NUM_EPOCHS, BATCH_SIZE, WEIGHT_DECAY, STEP_SIZE, GAMMA, train_dataset, val_dataset=val_dataset, verbosity=True, plot=True)\n",
        "\n",
        "print('val accuracy {}'.format(val_accuracy))\n",
        "print('val loss {}'.format(val_loss))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set 809\n",
            "validation set 203\n",
            "train_acc: 0.15945611866501855, val_acc: 0.18226600985221675, train_loss: 1.7863519095961922, val_loss: 1.7749869318431235 (1 / 100)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7699571374909693, val_loss: 1.751361764710525 (2 / 100)\n",
            "train_acc: 0.2200247218788628, val_acc: 0.21182266009852216, train_loss: 1.750950076671407, val_loss: 1.7378945344774595 (3 / 100)\n",
            "train_acc: 0.20642768850432633, val_acc: 0.2857142857142857, train_loss: 1.7436997010769744, val_loss: 1.7207825301315984 (4 / 100)\n",
            "train_acc: 0.2484548825710754, val_acc: 0.2660098522167488, train_loss: 1.7210341087819914, val_loss: 1.6762404946858072 (5 / 100)\n",
            "train_acc: 0.30778739184178, val_acc: 0.32019704433497537, train_loss: 1.6530373551789557, val_loss: 1.5750827325388717 (6 / 100)\n",
            "train_acc: 0.311495673671199, val_acc: 0.3054187192118227, train_loss: 1.6322292289863558, val_loss: 1.5859174687286903 (7 / 100)\n",
            "train_acc: 0.33250927070457353, val_acc: 0.3497536945812808, train_loss: 1.5649067415442544, val_loss: 1.4448580366050081 (8 / 100)\n",
            "train_acc: 0.3572311495673671, val_acc: 0.4236453201970443, train_loss: 1.53604441755929, val_loss: 1.4351222380041488 (9 / 100)\n",
            "train_acc: 0.3943139678615575, val_acc: 0.33004926108374383, train_loss: 1.4905946559752759, val_loss: 1.469487732267145 (10 / 100)\n",
            "train_acc: 0.3819530284301607, val_acc: 0.28078817733990147, train_loss: 1.4592278413336415, val_loss: 1.60974129961042 (11 / 100)\n",
            "train_acc: 0.3720642768850433, val_acc: 0.4187192118226601, train_loss: 1.4416143017439966, val_loss: 1.4361123087370924 (12 / 100)\n",
            "train_acc: 0.3794808405438813, val_acc: 0.2955665024630542, train_loss: 1.4468485368933754, val_loss: 1.6338433913996655 (13 / 100)\n",
            "train_acc: 0.3930778739184178, val_acc: 0.42857142857142855, train_loss: 1.394741000734242, val_loss: 1.3097851091417774 (14 / 100)\n",
            "train_acc: 0.42027194066749074, val_acc: 0.39408866995073893, train_loss: 1.3279362569191253, val_loss: 1.4098898057867153 (15 / 100)\n",
            "train_acc: 0.4177997527812114, val_acc: 0.41379310344827586, train_loss: 1.364502541509046, val_loss: 1.3334131129269529 (16 / 100)\n",
            "train_acc: 0.4746600741656366, val_acc: 0.43842364532019706, train_loss: 1.304370577462082, val_loss: 1.3117745057702652 (17 / 100)\n",
            "train_acc: 0.42398022249690975, val_acc: 0.4433497536945813, train_loss: 1.347145487705179, val_loss: 1.3016220930174653 (18 / 100)\n",
            "train_acc: 0.45241038318912236, val_acc: 0.43842364532019706, train_loss: 1.3113536713886615, val_loss: 1.306008049419948 (19 / 100)\n",
            "train_acc: 0.4820766378244747, val_acc: 0.4630541871921182, train_loss: 1.2758328864542015, val_loss: 1.2775594269113588 (20 / 100)\n",
            "train_acc: 0.46971569839307786, val_acc: 0.458128078817734, train_loss: 1.2627111126672206, val_loss: 1.2411712907217993 (21 / 100)\n",
            "train_acc: 0.4932014833127318, val_acc: 0.49261083743842365, train_loss: 1.214012973830196, val_loss: 1.209344199431941 (22 / 100)\n",
            "train_acc: 0.519159456118665, val_acc: 0.4729064039408867, train_loss: 1.2126881637148863, val_loss: 1.2273626436153655 (23 / 100)\n",
            "train_acc: 0.5166872682323856, val_acc: 0.4729064039408867, train_loss: 1.1452423497684512, val_loss: 1.2183772642624202 (24 / 100)\n",
            "train_acc: 0.5352286773794809, val_acc: 0.4975369458128079, train_loss: 1.1430756913127358, val_loss: 1.1469960932073922 (25 / 100)\n",
            "train_acc: 0.5241038318912238, val_acc: 0.47783251231527096, train_loss: 1.1165387920749792, val_loss: 1.2050917453953784 (26 / 100)\n",
            "train_acc: 0.5302843016069221, val_acc: 0.4827586206896552, train_loss: 1.1216493895086281, val_loss: 1.1961556323643388 (27 / 100)\n",
            "train_acc: 0.5488257107540173, val_acc: 0.47783251231527096, train_loss: 1.0708802488295197, val_loss: 1.128071941618849 (28 / 100)\n",
            "train_acc: 0.580964153275649, val_acc: 0.5172413793103449, train_loss: 0.9958879327302516, val_loss: 1.1779632506699398 (29 / 100)\n",
            "train_acc: 0.5747836835599506, val_acc: 0.5566502463054187, train_loss: 1.021386972463912, val_loss: 1.0431414742775151 (30 / 100)\n",
            "train_acc: 0.6044499381953028, val_acc: 0.6059113300492611, train_loss: 0.9889386270485349, val_loss: 1.107494134033842 (31 / 100)\n",
            "train_acc: 0.6440049443757726, val_acc: 0.5615763546798029, train_loss: 0.9081361330631195, val_loss: 1.0876345117691115 (32 / 100)\n",
            "train_acc: 0.6143386897404203, val_acc: 0.6059113300492611, train_loss: 0.9205240954280047, val_loss: 1.0676302498784558 (33 / 100)\n",
            "train_acc: 0.7181705809641533, val_acc: 0.5566502463054187, train_loss: 0.7698110290451898, val_loss: 1.1168967699769683 (34 / 100)\n",
            "train_acc: 0.6724351050679852, val_acc: 0.5714285714285714, train_loss: 0.8617587779891506, val_loss: 1.109305125151949 (35 / 100)\n",
            "train_acc: 0.6872682323856613, val_acc: 0.5862068965517241, train_loss: 0.7981782442486036, val_loss: 1.026625087695756 (36 / 100)\n",
            "train_acc: 0.7243510506798516, val_acc: 0.5862068965517241, train_loss: 0.7077748308370375, val_loss: 1.1424094644086114 (37 / 100)\n",
            "train_acc: 0.7490729295426453, val_acc: 0.5911330049261084, train_loss: 0.6826536944239632, val_loss: 1.1152384102050894 (38 / 100)\n",
            "train_acc: 0.6946847960444994, val_acc: 0.5911330049261084, train_loss: 0.7840710042875688, val_loss: 1.0281178663516868 (39 / 100)\n",
            "train_acc: 0.7861557478368356, val_acc: 0.5862068965517241, train_loss: 0.5788913877549661, val_loss: 1.071558723015151 (40 / 100)\n",
            "train_acc: 0.8195302843016069, val_acc: 0.6551724137931034, train_loss: 0.46663336568179326, val_loss: 1.04499184703592 (41 / 100)\n",
            "train_acc: 0.8244746600741656, val_acc: 0.5615763546798029, train_loss: 0.4583147772310396, val_loss: 1.4779623623551994 (42 / 100)\n",
            "train_acc: 0.7787391841779975, val_acc: 0.6108374384236454, train_loss: 0.6364205924630607, val_loss: 1.1932886166525591 (43 / 100)\n",
            "train_acc: 0.8294190358467244, val_acc: 0.5665024630541872, train_loss: 0.47499116772626915, val_loss: 1.2461572818714997 (44 / 100)\n",
            "train_acc: 0.8665018541409147, val_acc: 0.645320197044335, train_loss: 0.37296016158810064, val_loss: 1.1575070819244009 (45 / 100)\n",
            "train_acc: 0.896168108776267, val_acc: 0.6600985221674877, train_loss: 0.29526734598929566, val_loss: 1.352015057220835 (46 / 100)\n",
            "train_acc: 0.8702101359703337, val_acc: 0.6108374384236454, train_loss: 0.3153956113072349, val_loss: 1.5862098773712008 (47 / 100)\n",
            "train_acc: 0.8899876390605687, val_acc: 0.6305418719211823, train_loss: 0.30259224923491035, val_loss: 1.2774403021840626 (48 / 100)\n",
            "train_acc: 0.9196538936959209, val_acc: 0.6305418719211823, train_loss: 0.18420899363766494, val_loss: 1.5177540300514898 (49 / 100)\n",
            "train_acc: 0.9332509270704573, val_acc: 0.645320197044335, train_loss: 0.16377416017323695, val_loss: 1.3074814169277698 (50 / 100)\n",
            "train_acc: 0.9221260815822002, val_acc: 0.6157635467980296, train_loss: 0.21651858898116869, val_loss: 1.4983219566016361 (51 / 100)\n",
            "train_acc: 0.9542645241038319, val_acc: 0.6650246305418719, train_loss: 0.1325169602738249, val_loss: 1.6363280019149404 (52 / 100)\n",
            "train_acc: 0.9295426452410384, val_acc: 0.6502463054187192, train_loss: 0.1915694343844804, val_loss: 1.4759723282799933 (53 / 100)\n",
            "train_acc: 0.9381953028430161, val_acc: 0.6699507389162561, train_loss: 0.17590373334542783, val_loss: 1.2860583084557444 (54 / 100)\n",
            "train_acc: 0.9592088998763906, val_acc: 0.6551724137931034, train_loss: 0.09263374226349688, val_loss: 2.033003178136102 (55 / 100)\n",
            "train_acc: 0.9480840543881335, val_acc: 0.5911330049261084, train_loss: 0.13352113951121922, val_loss: 1.6995241970851505 (56 / 100)\n",
            "train_acc: 0.9456118665018541, val_acc: 0.645320197044335, train_loss: 0.15100306429991173, val_loss: 1.4345445574210782 (57 / 100)\n",
            "train_acc: 0.9320148331273177, val_acc: 0.5812807881773399, train_loss: 0.18696495242260294, val_loss: 2.1212253547067124 (58 / 100)\n",
            "train_acc: 0.9542645241038319, val_acc: 0.5960591133004927, train_loss: 0.13430220853811883, val_loss: 2.131609613672266 (59 / 100)\n",
            "train_acc: 0.9666254635352287, val_acc: 0.645320197044335, train_loss: 0.10808846961973956, val_loss: 1.8407391327355296 (60 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.6699507389162561, train_loss: 0.034953993803859496, val_loss: 1.6225143054435993 (61 / 100)\n",
            "train_acc: 0.9975278121137207, val_acc: 0.6748768472906403, train_loss: 0.013458512473922817, val_loss: 1.5880556036098836 (62 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.6699507389162561, train_loss: 0.013537410458311559, val_loss: 1.617177378367908 (63 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.6847290640394089, train_loss: 0.017741209782466606, val_loss: 1.5724905996487057 (64 / 100)\n",
            "train_acc: 0.9987639060568603, val_acc: 0.6798029556650246, train_loss: 0.00820389312717313, val_loss: 1.6199622665132796 (65 / 100)\n",
            "train_acc: 0.9962917181705809, val_acc: 0.6847290640394089, train_loss: 0.013393460460955782, val_loss: 1.6374526129567564 (66 / 100)\n",
            "train_acc: 0.9962917181705809, val_acc: 0.6748768472906403, train_loss: 0.013119790707663052, val_loss: 1.6689862224268797 (67 / 100)\n",
            "train_acc: 0.9987639060568603, val_acc: 0.6847290640394089, train_loss: 0.008827262225057768, val_loss: 1.6748121301528855 (68 / 100)\n",
            "train_acc: 0.9987639060568603, val_acc: 0.6945812807881774, train_loss: 0.006438095283887967, val_loss: 1.6820654346437878 (69 / 100)\n",
            "train_acc: 0.9975278121137207, val_acc: 0.6847290640394089, train_loss: 0.00870845521361612, val_loss: 1.7084377410963838 (70 / 100)\n",
            "train_acc: 0.9987639060568603, val_acc: 0.6847290640394089, train_loss: 0.005137910359556791, val_loss: 1.7407155265949044 (71 / 100)\n",
            "train_acc: 0.9987639060568603, val_acc: 0.6896551724137931, train_loss: 0.005759227408466881, val_loss: 1.7760758963711742 (72 / 100)\n",
            "train_acc: 0.9987639060568603, val_acc: 0.6847290640394089, train_loss: 0.007439297117403396, val_loss: 1.8094189008468478 (73 / 100)\n",
            "train_acc: 0.9987639060568603, val_acc: 0.6798029556650246, train_loss: 0.006254252750738507, val_loss: 1.8156042421979857 (74 / 100)\n",
            "train_acc: 0.9975278121137207, val_acc: 0.6896551724137931, train_loss: 0.007581719656590584, val_loss: 1.779734945649584 (75 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6847290640394089, train_loss: 0.0037344562981107324, val_loss: 1.783031029654254 (76 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6699507389162561, train_loss: 0.004623856915394708, val_loss: 1.8027003844970553 (77 / 100)\n",
            "train_acc: 0.9987639060568603, val_acc: 0.6748768472906403, train_loss: 0.00507655986601991, val_loss: 1.8104389524224944 (78 / 100)\n",
            "train_acc: 0.9987639060568603, val_acc: 0.6798029556650246, train_loss: 0.0061420209622943065, val_loss: 1.8184796835988613 (79 / 100)\n",
            "train_acc: 0.9987639060568603, val_acc: 0.6748768472906403, train_loss: 0.006279773853474028, val_loss: 1.8363854703057576 (80 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6847290640394089, train_loss: 0.003933279417059183, val_loss: 1.841786221330389 (81 / 100)\n",
            "train_acc: 0.9962917181705809, val_acc: 0.6748768472906403, train_loss: 0.007889524820561168, val_loss: 1.864583964418308 (82 / 100)\n",
            "train_acc: 0.9975278121137207, val_acc: 0.6798029556650246, train_loss: 0.007088077672191968, val_loss: 1.8988974446733597 (83 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6748768472906403, train_loss: 0.0038878331225916107, val_loss: 1.901077770247248 (84 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6699507389162561, train_loss: 0.0028643154241869916, val_loss: 1.8945963335741918 (85 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6699507389162561, train_loss: 0.0037702016983692374, val_loss: 1.8973881147177936 (86 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6748768472906403, train_loss: 0.0028545275890027783, val_loss: 1.9186426360031654 (87 / 100)\n",
            "train_acc: 0.9987639060568603, val_acc: 0.6748768472906403, train_loss: 0.0028987649934151143, val_loss: 1.9108921835575197 (88 / 100)\n",
            "train_acc: 0.9975278121137207, val_acc: 0.6798029556650246, train_loss: 0.004177268562413641, val_loss: 1.9393046154764486 (89 / 100)\n",
            "train_acc: 0.9975278121137207, val_acc: 0.6748768472906403, train_loss: 0.005585760062314945, val_loss: 1.9650477194433729 (90 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6798029556650246, train_loss: 0.0028847204014769445, val_loss: 1.9629235232404887 (91 / 100)\n",
            "train_acc: 0.9975278121137207, val_acc: 0.6798029556650246, train_loss: 0.0047207332506306165, val_loss: 1.971753804554493 (92 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6748768472906403, train_loss: 0.002441572465123557, val_loss: 1.9692478737807626 (93 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6699507389162561, train_loss: 0.0024427188795211876, val_loss: 1.9634154876464693 (94 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6847290640394089, train_loss: 0.002623536825330861, val_loss: 1.973495115200287 (95 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6748768472906403, train_loss: 0.0027453737294276066, val_loss: 1.9928546327675505 (96 / 100)\n",
            "train_acc: 0.9975278121137207, val_acc: 0.6798029556650246, train_loss: 0.007606157414098265, val_loss: 1.974074276210052 (97 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6847290640394089, train_loss: 0.001823382854907185, val_loss: 1.9519985109714453 (98 / 100)\n",
            "train_acc: 0.9975278121137207, val_acc: 0.6847290640394089, train_loss: 0.0035283468279190943, val_loss: 1.988735902485589 (99 / 100)\n",
            "train_acc: 1.0, val_acc: 0.6847290640394089, train_loss: 0.0016152177959381532, val_loss: 1.999770189740975 (100 / 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvTHGeJwFemOM8TgL9MYY43H/D51Vp4jwkRO3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nO+dcJDyhO+dcJDyhO+dcJDyhO+dcJP4HYdyd4hxJk4kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "val accuracy 0.6945812807881774\n",
            "val loss 1.6820654346437878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1kgj76dvQxIJ"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_dKdDvgnQw7d",
        "trusted": false,
        "colab": {}
      },
      "source": [
        "# todo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qxj7-SlSKb_3"
      },
      "source": [
        "**Grid search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aPmSObkPKbu3",
        "outputId": "1db6d341-f7ee-46a2-aa3e-2c683cc186b6",
        "trusted": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "NUM_CLASSES = 6\n",
        "DEVICE = 'cuda'\n",
        "#BATCH_SIZE = 16\n",
        "#LR = 0.001\n",
        "MOMENTUM = 0.9\n",
        "#WEIGHT_DECAY = 5e-5\n",
        "NUM_EPOCHS = 100\n",
        "STEP_SIZE = 60\n",
        "#GAMMA = 0.1\n",
        "\n",
        "lr_range = [0.0005, 0.001]\n",
        "batch_size_range = [8, 16]\n",
        "weight_decay_range = [1e-5, 1e-3]\n",
        "gamma_range = [0.05, 0.1]\n",
        "hyperparameters_sets = []\n",
        "\n",
        "for lr in lr_range:\n",
        "  for batch_size in batch_size_range:\n",
        "    for weight_decay in weight_decay_range:\n",
        "      for gamma in gamma_range:\n",
        "        hyperparameters_sets.append({'lr': lr, 'batch_size': batch_size, 'weight_decay': weight_decay, 'gamma': gamma})\n",
        "\n",
        "for set in hyperparameters_sets:\n",
        "  print(set)\n",
        "\n",
        "\n",
        "TRAIN_DATA_DIR = 'AIML_project/ravdess-emotional-song-spec'\n",
        "compose=[#transforms.Resize(224),\n",
        "         transforms.CenterCrop(224),\n",
        "         transforms.RandomGrayscale(),\n",
        "         transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
        "         transforms.ToTensor()\n",
        "         ]\n",
        "train_dataset, val_dataset = get_datasets(TRAIN_DATA_DIR, TRAIN_DATA_DIR, compose)\n",
        "\n",
        "train_indexes = [idx for idx in range(len(train_dataset)) if idx % 5]\n",
        "val_indexes = [idx for idx in range(len(train_dataset)) if not idx % 5]\n",
        "val_dataset = Subset(val_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "print('training set {}'.format(len(train_dataset)))\n",
        "print('validation set {}'.format(len(val_dataset)))\n",
        "\n",
        "best_net = vgg19()\n",
        "best_net = best_net.to(DEVICE)\n",
        "best_net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "best_set = {}\n",
        "best_accuracy = 0.0\n",
        "best_loss = 0.0\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "\n",
        "for set in hyperparameters_sets:\n",
        "\n",
        "  net = vgg19()\n",
        "  net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "  current_net, val_accuracy, val_loss = train_network(net, net.parameters(), set['lr'], NUM_EPOCHS, set['batch_size'], set['weight_decay'], STEP_SIZE, set['gamma'], train_dataset, val_dataset=val_dataset, verbosity=True)\n",
        "  val_accuracies.append(val_accuracy)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  if val_accuracy > best_accuracy:\n",
        "    best_accuracy = val_accuracy\n",
        "    best_loss = val_loss\n",
        "    best_net = copy.deepcopy(current_net)\n",
        "    best_set = copy.deepcopy(set)\n",
        "  \n",
        "  print(\"({}), val accuracy {}, val loss {}\".format(set, val_accuracy, val_loss))\n",
        "\n",
        "print(\"\\n({}), best val accuracy {}, best val loss {}\".format(best_set, best_accuracy, best_loss))\n",
        "print(\"\\nval_accuracies\")\n",
        "print(val_accuracies)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'lr': 0.0005, 'batch_size': 8, 'weight_decay': 1e-05, 'gamma': 0.05}\n",
            "{'lr': 0.0005, 'batch_size': 8, 'weight_decay': 1e-05, 'gamma': 0.1}\n",
            "{'lr': 0.0005, 'batch_size': 8, 'weight_decay': 0.001, 'gamma': 0.05}\n",
            "{'lr': 0.0005, 'batch_size': 8, 'weight_decay': 0.001, 'gamma': 0.1}\n",
            "{'lr': 0.0005, 'batch_size': 16, 'weight_decay': 1e-05, 'gamma': 0.05}\n",
            "{'lr': 0.0005, 'batch_size': 16, 'weight_decay': 1e-05, 'gamma': 0.1}\n",
            "{'lr': 0.0005, 'batch_size': 16, 'weight_decay': 0.001, 'gamma': 0.05}\n",
            "{'lr': 0.0005, 'batch_size': 16, 'weight_decay': 0.001, 'gamma': 0.1}\n",
            "{'lr': 0.001, 'batch_size': 8, 'weight_decay': 1e-05, 'gamma': 0.05}\n",
            "{'lr': 0.001, 'batch_size': 8, 'weight_decay': 1e-05, 'gamma': 0.1}\n",
            "{'lr': 0.001, 'batch_size': 8, 'weight_decay': 0.001, 'gamma': 0.05}\n",
            "{'lr': 0.001, 'batch_size': 8, 'weight_decay': 0.001, 'gamma': 0.1}\n",
            "{'lr': 0.001, 'batch_size': 16, 'weight_decay': 1e-05, 'gamma': 0.05}\n",
            "{'lr': 0.001, 'batch_size': 16, 'weight_decay': 1e-05, 'gamma': 0.1}\n",
            "{'lr': 0.001, 'batch_size': 16, 'weight_decay': 0.001, 'gamma': 0.05}\n",
            "{'lr': 0.001, 'batch_size': 16, 'weight_decay': 0.001, 'gamma': 0.1}\n",
            "training set 809\n",
            "validation set 203\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.78373383488437, val_loss: 1.7685335005445433 (1 / 100)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7591753850319183, val_loss: 1.7403155025002992 (2 / 100)\n",
            "train_acc: 0.19777503090234858, val_acc: 0.22660098522167488, train_loss: 1.744106983961664, val_loss: 1.7206945889101828 (3 / 100)\n",
            "train_acc: 0.2249690976514215, val_acc: 0.2561576354679803, train_loss: 1.7316292724739046, val_loss: 1.7059643556331765 (4 / 100)\n",
            "train_acc: 0.26452410383189123, val_acc: 0.23645320197044334, train_loss: 1.6886965863047483, val_loss: 1.6405535777801363 (5 / 100)\n",
            "train_acc: 0.2954264524103832, val_acc: 0.35467980295566504, train_loss: 1.6677118656219747, val_loss: 1.5781844431543586 (6 / 100)\n",
            "train_acc: 0.3411619283065513, val_acc: 0.2857142857142857, train_loss: 1.6157246720510892, val_loss: 1.672465663238112 (7 / 100)\n",
            "train_acc: 0.3288009888751545, val_acc: 0.39901477832512317, train_loss: 1.5780016092493743, val_loss: 1.4462852219642677 (8 / 100)\n",
            "train_acc: 0.35970333745364647, val_acc: 0.3694581280788177, train_loss: 1.5130871963147328, val_loss: 1.5083793994828398 (9 / 100)\n",
            "train_acc: 0.3621755253399258, val_acc: 0.3793103448275862, train_loss: 1.4855279285916587, val_loss: 1.4945245711087005 (10 / 100)\n",
            "train_acc: 0.3819530284301607, val_acc: 0.3891625615763547, train_loss: 1.4494802842770873, val_loss: 1.4610776871883224 (11 / 100)\n",
            "train_acc: 0.37082818294190356, val_acc: 0.3694581280788177, train_loss: 1.4268492022609829, val_loss: 1.3766332694462367 (12 / 100)\n",
            "train_acc: 0.3930778739184178, val_acc: 0.4236453201970443, train_loss: 1.4145337064717107, val_loss: 1.330364547926804 (13 / 100)\n",
            "train_acc: 0.415327564894932, val_acc: 0.4630541871921182, train_loss: 1.3620626835062712, val_loss: 1.2550151941224272 (14 / 100)\n",
            "train_acc: 0.41409147095179233, val_acc: 0.4433497536945813, train_loss: 1.3534568816386578, val_loss: 1.3153020895173397 (15 / 100)\n",
            "train_acc: 0.43016069221260816, val_acc: 0.458128078817734, train_loss: 1.3038736656952847, val_loss: 1.2371725716027133 (16 / 100)\n",
            "train_acc: 0.415327564894932, val_acc: 0.5024630541871922, train_loss: 1.301666410508645, val_loss: 1.186180835286972 (17 / 100)\n",
            "train_acc: 0.41903584672435107, val_acc: 0.4433497536945813, train_loss: 1.3121248370195349, val_loss: 1.2594843630133004 (18 / 100)\n",
            "train_acc: 0.48331273176761436, val_acc: 0.4630541871921182, train_loss: 1.2533173805703635, val_loss: 1.2994039281835696 (19 / 100)\n",
            "train_acc: 0.4758961681087763, val_acc: 0.5024630541871922, train_loss: 1.269942855952844, val_loss: 1.262349520998048 (20 / 100)\n",
            "train_acc: 0.4796044499381953, val_acc: 0.5221674876847291, train_loss: 1.2272651214682126, val_loss: 1.1525027164684727 (21 / 100)\n",
            "train_acc: 0.47095179233621753, val_acc: 0.4187192118226601, train_loss: 1.2258059014790728, val_loss: 1.257804590786619 (22 / 100)\n",
            "train_acc: 0.49814585908529047, val_acc: 0.4729064039408867, train_loss: 1.1554269392793937, val_loss: 1.1893765386102235 (23 / 100)\n",
            "train_acc: 0.5092707045735476, val_acc: 0.458128078817734, train_loss: 1.1806158600985193, val_loss: 1.1806066573547025 (24 / 100)\n",
            "train_acc: 0.522867737948084, val_acc: 0.5467980295566502, train_loss: 1.1481974172061689, val_loss: 1.0868582666801114 (25 / 100)\n",
            "train_acc: 0.5315203955500618, val_acc: 0.45320197044334976, train_loss: 1.0985861632084228, val_loss: 1.4302576621764986 (26 / 100)\n",
            "train_acc: 0.5302843016069221, val_acc: 0.5073891625615764, train_loss: 1.0957239089701762, val_loss: 1.1519955495014567 (27 / 100)\n",
            "train_acc: 0.5525339925834364, val_acc: 0.5221674876847291, train_loss: 1.0485507926186466, val_loss: 1.3735314137829935 (28 / 100)\n",
            "train_acc: 0.5488257107540173, val_acc: 0.5270935960591133, train_loss: 1.0503360069873748, val_loss: 1.2819593457752847 (29 / 100)\n",
            "train_acc: 0.5735475896168108, val_acc: 0.5270935960591133, train_loss: 1.0089199177561643, val_loss: 1.145402090302829 (30 / 100)\n",
            "train_acc: 0.5550061804697157, val_acc: 0.5369458128078818, train_loss: 1.0493555493348903, val_loss: 1.1409553001666892 (31 / 100)\n",
            "train_acc: 0.619283065512979, val_acc: 0.5911330049261084, train_loss: 0.9332560962445952, val_loss: 1.017917294807622 (32 / 100)\n",
            "train_acc: 0.6291718170580964, val_acc: 0.5812807881773399, train_loss: 0.900799279454612, val_loss: 1.129364417691536 (33 / 100)\n",
            "train_acc: 0.6143386897404203, val_acc: 0.5566502463054187, train_loss: 0.9232618882423278, val_loss: 1.1204004002909356 (34 / 100)\n",
            "train_acc: 0.6365883807169345, val_acc: 0.541871921182266, train_loss: 0.9190937417694016, val_loss: 1.0148128911192194 (35 / 100)\n",
            "train_acc: 0.6266996291718171, val_acc: 0.5615763546798029, train_loss: 0.8882167863904766, val_loss: 1.1216303380252106 (36 / 100)\n",
            "train_acc: 0.6823238566131026, val_acc: 0.5714285714285714, train_loss: 0.8083949066947211, val_loss: 1.2078805812473954 (37 / 100)\n",
            "train_acc: 0.6909765142150803, val_acc: 0.6157635467980296, train_loss: 0.7784091129585898, val_loss: 1.0325541152742697 (38 / 100)\n",
            "train_acc: 0.7070457354758962, val_acc: 0.5812807881773399, train_loss: 0.7140360190930266, val_loss: 1.0717352799006872 (39 / 100)\n",
            "train_acc: 0.7107540173053152, val_acc: 0.6206896551724138, train_loss: 0.7227502564711977, val_loss: 1.0377472515763908 (40 / 100)\n",
            "train_acc: 0.7330037082818294, val_acc: 0.6157635467980296, train_loss: 0.7050815044139018, val_loss: 1.0168454046613478 (41 / 100)\n",
            "train_acc: 0.7107540173053152, val_acc: 0.6157635467980296, train_loss: 0.7150252873287507, val_loss: 1.3187641554278107 (42 / 100)\n",
            "train_acc: 0.7527812113720643, val_acc: 0.6059113300492611, train_loss: 0.6126328554966836, val_loss: 1.3502651340708944 (43 / 100)\n",
            "train_acc: 0.6773794808405439, val_acc: 0.645320197044335, train_loss: 0.8108952592123572, val_loss: 1.1171635007623382 (44 / 100)\n",
            "train_acc: 0.7725587144622992, val_acc: 0.6157635467980296, train_loss: 0.5690765827341634, val_loss: 1.0725170814345035 (45 / 100)\n",
            "train_acc: 0.7948084054388134, val_acc: 0.6354679802955665, train_loss: 0.5098226926530101, val_loss: 1.0795075526378426 (46 / 100)\n",
            "train_acc: 0.7873918417799752, val_acc: 0.6551724137931034, train_loss: 0.548065011374588, val_loss: 1.0642458956523482 (47 / 100)\n",
            "train_acc: 0.8096415327564895, val_acc: 0.6502463054187192, train_loss: 0.49149149236337214, val_loss: 1.1301951446556693 (48 / 100)\n",
            "train_acc: 0.830655129789864, val_acc: 0.6600985221674877, train_loss: 0.42063818299136735, val_loss: 1.363823467581143 (49 / 100)\n",
            "train_acc: 0.8467243510506799, val_acc: 0.6600985221674877, train_loss: 0.4218120778299527, val_loss: 1.1632297764270765 (50 / 100)\n",
            "train_acc: 0.8355995055624228, val_acc: 0.6354679802955665, train_loss: 0.44429056326155314, val_loss: 1.4594083575192343 (51 / 100)\n",
            "train_acc: 0.7218788627935723, val_acc: 0.6995073891625616, train_loss: 0.7073465802318822, val_loss: 1.2190375210616389 (52 / 100)\n",
            "train_acc: 0.8257107540173053, val_acc: 0.6847290640394089, train_loss: 0.4458410415720144, val_loss: 1.1707994130444643 (53 / 100)\n",
            "train_acc: 0.8417799752781211, val_acc: 0.6798029556650246, train_loss: 0.4121690959069873, val_loss: 1.0205895486723613 (54 / 100)\n",
            "train_acc: 0.8603213844252163, val_acc: 0.6600985221674877, train_loss: 0.37352726132377556, val_loss: 1.207179679365581 (55 / 100)\n",
            "train_acc: 0.8677379480840544, val_acc: 0.6650246305418719, train_loss: 0.34558123755366604, val_loss: 1.6535105034341953 (56 / 100)\n",
            "train_acc: 0.861557478368356, val_acc: 0.6847290640394089, train_loss: 0.3679037079380823, val_loss: 1.139225322771542 (57 / 100)\n",
            "train_acc: 0.8825710754017305, val_acc: 0.645320197044335, train_loss: 0.3138643249740412, val_loss: 1.2116599494013294 (58 / 100)\n",
            "train_acc: 0.9122373300370828, val_acc: 0.6748768472906403, train_loss: 0.2798436766649206, val_loss: 1.2507030344361743 (59 / 100)\n",
            "train_acc: 0.9110012360939431, val_acc: 0.6896551724137931, train_loss: 0.23642536500475758, val_loss: 1.3558491439067673 (60 / 100)\n",
            "train_acc: 0.9443757725587144, val_acc: 0.7438423645320197, train_loss: 0.17392502476464686, val_loss: 1.1848883557642622 (61 / 100)\n",
            "train_acc: 0.957972805933251, val_acc: 0.7438423645320197, train_loss: 0.15073673230019136, val_loss: 1.2302069595001015 (62 / 100)\n",
            "train_acc: 0.9431396786155748, val_acc: 0.729064039408867, train_loss: 0.13937576547983402, val_loss: 1.2848408173751362 (63 / 100)\n",
            "train_acc: 0.9567367119901112, val_acc: 0.7339901477832512, train_loss: 0.10456823182783846, val_loss: 1.299033915614847 (64 / 100)\n",
            "train_acc: 0.9616810877626699, val_acc: 0.7438423645320197, train_loss: 0.11547666574438069, val_loss: 1.3156936710397598 (65 / 100)\n",
            "train_acc: 0.9542645241038319, val_acc: 0.7389162561576355, train_loss: 0.12036131338961192, val_loss: 1.3614150560254534 (66 / 100)\n",
            "train_acc: 0.9468479604449939, val_acc: 0.7438423645320197, train_loss: 0.13936495471501675, val_loss: 1.3456075354456314 (67 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.7438423645320197, train_loss: 0.0959784862284902, val_loss: 1.3696311765116425 (68 / 100)\n",
            "train_acc: 0.9555006180469716, val_acc: 0.7586206896551724, train_loss: 0.13473185930028098, val_loss: 1.3178652379606746 (69 / 100)\n",
            "train_acc: 0.965389369592089, val_acc: 0.7536945812807881, train_loss: 0.10577929712491219, val_loss: 1.4150441807185488 (70 / 100)\n",
            "train_acc: 0.965389369592089, val_acc: 0.7438423645320197, train_loss: 0.0970323240506487, val_loss: 1.4057781625851034 (71 / 100)\n",
            "train_acc: 0.9530284301606922, val_acc: 0.7536945812807881, train_loss: 0.1009780223054261, val_loss: 1.4293723162171876 (72 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.7389162561576355, train_loss: 0.07237879012806896, val_loss: 1.4640146866807797 (73 / 100)\n",
            "train_acc: 0.9666254635352287, val_acc: 0.7241379310344828, train_loss: 0.10341844408414862, val_loss: 1.4986663862989453 (74 / 100)\n",
            "train_acc: 0.9629171817058096, val_acc: 0.7389162561576355, train_loss: 0.09607517910828844, val_loss: 1.4947347852396848 (75 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.7389162561576355, train_loss: 0.098310959648585, val_loss: 1.4613136881090738 (76 / 100)\n",
            "train_acc: 0.9666254635352287, val_acc: 0.7339901477832512, train_loss: 0.0882847982519784, val_loss: 1.5054220930108884 (77 / 100)\n",
            "train_acc: 0.9629171817058096, val_acc: 0.7438423645320197, train_loss: 0.08725621894793988, val_loss: 1.496209300503942 (78 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.729064039408867, train_loss: 0.08901232488370502, val_loss: 1.5293833842418465 (79 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.7536945812807881, train_loss: 0.07757288092293757, val_loss: 1.5217512932610628 (80 / 100)\n",
            "train_acc: 0.965389369592089, val_acc: 0.7586206896551724, train_loss: 0.09169566719729468, val_loss: 1.5278293522707935 (81 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.7438423645320197, train_loss: 0.07371234982211157, val_loss: 1.5192823713929782 (82 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.7635467980295566, train_loss: 0.07704800551547698, val_loss: 1.4869063125161701 (83 / 100)\n",
            "train_acc: 0.9567367119901112, val_acc: 0.7339901477832512, train_loss: 0.10360821834748106, val_loss: 1.4953251977272222 (84 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.7487684729064039, train_loss: 0.09746902554822057, val_loss: 1.4749785385695584 (85 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.7586206896551724, train_loss: 0.06043395698438911, val_loss: 1.4974648305054368 (86 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.7536945812807881, train_loss: 0.06924164707787399, val_loss: 1.5231841736532785 (87 / 100)\n",
            "train_acc: 0.9641532756489494, val_acc: 0.7487684729064039, train_loss: 0.08718352883648961, val_loss: 1.5591367287882443 (88 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.7487684729064039, train_loss: 0.048029029914563606, val_loss: 1.5867895751163876 (89 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.7487684729064039, train_loss: 0.0792457923311533, val_loss: 1.6006375745012256 (90 / 100)\n",
            "train_acc: 0.965389369592089, val_acc: 0.7389162561576355, train_loss: 0.09129807695617487, val_loss: 1.622169793826606 (91 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.7635467980295566, train_loss: 0.080816844190449, val_loss: 1.5736170563791774 (92 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.7586206896551724, train_loss: 0.07245363676356444, val_loss: 1.5628343898380919 (93 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.7635467980295566, train_loss: 0.060810725827446975, val_loss: 1.5828983796934777 (94 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.7487684729064039, train_loss: 0.06913261493145315, val_loss: 1.5900617773309718 (95 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.7487684729064039, train_loss: 0.08122996479383358, val_loss: 1.597121342062363 (96 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.7389162561576355, train_loss: 0.0702357115350636, val_loss: 1.5828301238602605 (97 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.7635467980295566, train_loss: 0.07719301440659795, val_loss: 1.5754121983873433 (98 / 100)\n",
            "train_acc: 0.9678615574783683, val_acc: 0.7635467980295566, train_loss: 0.08986866547828552, val_loss: 1.575839596869323 (99 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.7487684729064039, train_loss: 0.06889708640401532, val_loss: 1.5847745859270612 (100 / 100)\n",
            "({'lr': 0.0005, 'batch_size': 8, 'weight_decay': 1e-05, 'gamma': 0.05}), val accuracy 0.7635467980295566, val loss 1.4869063125161701\n",
            "train_acc: 0.17181705809641531, val_acc: 0.21182266009852216, train_loss: 1.784361404747839, val_loss: 1.7642770218731734 (1 / 100)\n",
            "train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.766925573348999, val_loss: 1.7531924670552972 (2 / 100)\n",
            "train_acc: 0.22620519159456118, val_acc: 0.2561576354679803, train_loss: 1.7576426865880657, val_loss: 1.7382822242276421 (3 / 100)\n",
            "train_acc: 0.23856613102595797, val_acc: 0.2315270935960591, train_loss: 1.7471613552425935, val_loss: 1.7220872734567803 (4 / 100)\n",
            "train_acc: 0.2595797280593325, val_acc: 0.33004926108374383, train_loss: 1.7158655118293904, val_loss: 1.6193340922811348 (5 / 100)\n",
            "train_acc: 0.25339925834363414, val_acc: 0.35467980295566504, train_loss: 1.713133012997942, val_loss: 1.6003223880758426 (6 / 100)\n",
            "train_acc: 0.2843016069221261, val_acc: 0.3054187192118227, train_loss: 1.67195953868965, val_loss: 1.6007734096696224 (7 / 100)\n",
            "train_acc: 0.36093943139678614, val_acc: 0.3399014778325123, train_loss: 1.561879686431036, val_loss: 1.5899436467974057 (8 / 100)\n",
            "train_acc: 0.3263288009888752, val_acc: 0.35467980295566504, train_loss: 1.5805941249885134, val_loss: 1.49422952048297 (9 / 100)\n",
            "train_acc: 0.3572311495673671, val_acc: 0.3448275862068966, train_loss: 1.5032355272578368, val_loss: 1.4546788038291367 (10 / 100)\n",
            "train_acc: 0.37453646477132263, val_acc: 0.43842364532019706, train_loss: 1.4788648392567676, val_loss: 1.465018196646216 (11 / 100)\n",
            "train_acc: 0.34857849196538937, val_acc: 0.3891625615763547, train_loss: 1.4724658371049482, val_loss: 1.4341557031781802 (12 / 100)\n",
            "train_acc: 0.37082818294190356, val_acc: 0.3251231527093596, train_loss: 1.4437153908171967, val_loss: 1.63305669876155 (13 / 100)\n",
            "train_acc: 0.3683559950556242, val_acc: 0.4039408866995074, train_loss: 1.4667615312875717, val_loss: 1.445153016762193 (14 / 100)\n",
            "train_acc: 0.3943139678615575, val_acc: 0.458128078817734, train_loss: 1.3949429054637479, val_loss: 1.3602121780658591 (15 / 100)\n",
            "train_acc: 0.38813349814585907, val_acc: 0.4039408866995074, train_loss: 1.3866008325619221, val_loss: 1.3289326141620506 (16 / 100)\n",
            "train_acc: 0.40667490729295425, val_acc: 0.39408866995073893, train_loss: 1.3840013941374638, val_loss: 1.3159581522636226 (17 / 100)\n",
            "train_acc: 0.44128553770086526, val_acc: 0.4975369458128079, train_loss: 1.3321049793983715, val_loss: 1.2944574632080905 (18 / 100)\n",
            "train_acc: 0.4276885043263288, val_acc: 0.43349753694581283, train_loss: 1.3421970009067004, val_loss: 1.3620289646345993 (19 / 100)\n",
            "train_acc: 0.43139678615574784, val_acc: 0.46798029556650245, train_loss: 1.368697890981903, val_loss: 1.2580579789401276 (20 / 100)\n",
            "train_acc: 0.44499381953028433, val_acc: 0.4630541871921182, train_loss: 1.2987654828023263, val_loss: 1.2446271275064629 (21 / 100)\n",
            "train_acc: 0.4388133498145859, val_acc: 0.4630541871921182, train_loss: 1.306820211658077, val_loss: 1.2424459645313581 (22 / 100)\n",
            "train_acc: 0.4758961681087763, val_acc: 0.4630541871921182, train_loss: 1.2632981053978316, val_loss: 1.224477032135273 (23 / 100)\n",
            "train_acc: 0.4746600741656366, val_acc: 0.4630541871921182, train_loss: 1.2645783478898378, val_loss: 1.2030680649386252 (24 / 100)\n",
            "train_acc: 0.4907292954264524, val_acc: 0.4630541871921182, train_loss: 1.2322985028159632, val_loss: 1.196917113118571 (25 / 100)\n",
            "train_acc: 0.5030902348578492, val_acc: 0.4482758620689655, train_loss: 1.201461157486377, val_loss: 1.2674795033896498 (26 / 100)\n",
            "train_acc: 0.5203955500618047, val_acc: 0.3842364532019704, train_loss: 1.192048879135377, val_loss: 1.3550262929770747 (27 / 100)\n",
            "train_acc: 0.5377008652657602, val_acc: 0.5073891625615764, train_loss: 1.186145384762579, val_loss: 1.1494043516408046 (28 / 100)\n",
            "train_acc: 0.5364647713226205, val_acc: 0.4827586206896552, train_loss: 1.1477680778031885, val_loss: 1.1942138116935204 (29 / 100)\n",
            "train_acc: 0.5339925834363412, val_acc: 0.5714285714285714, train_loss: 1.1408783968063752, val_loss: 1.1324468122914506 (30 / 100)\n",
            "train_acc: 0.5673671199011124, val_acc: 0.5517241379310345, train_loss: 1.0726224229715957, val_loss: 1.1401977010548408 (31 / 100)\n",
            "train_acc: 0.5648949320148331, val_acc: 0.4876847290640394, train_loss: 1.0545946403840563, val_loss: 1.207767148616866 (32 / 100)\n",
            "train_acc: 0.5710754017305315, val_acc: 0.4482758620689655, train_loss: 1.0361261907113055, val_loss: 1.2728984816908249 (33 / 100)\n",
            "train_acc: 0.5859085290482077, val_acc: 0.47783251231527096, train_loss: 0.9921169787903209, val_loss: 1.1980525167117566 (34 / 100)\n",
            "train_acc: 0.5896168108776267, val_acc: 0.5911330049261084, train_loss: 0.9952731232707963, val_loss: 1.0271065176414151 (35 / 100)\n",
            "train_acc: 0.6378244746600742, val_acc: 0.5812807881773399, train_loss: 0.929920085428377, val_loss: 1.1085625704873372 (36 / 100)\n",
            "train_acc: 0.6168108776266996, val_acc: 0.4630541871921182, train_loss: 0.9464848191835382, val_loss: 1.312536914947585 (37 / 100)\n",
            "train_acc: 0.6724351050679852, val_acc: 0.5763546798029556, train_loss: 0.8586398532570366, val_loss: 1.100383582079939 (38 / 100)\n",
            "train_acc: 0.6724351050679852, val_acc: 0.5467980295566502, train_loss: 0.8761506393017668, val_loss: 1.1371152864888383 (39 / 100)\n",
            "train_acc: 0.6934487021013597, val_acc: 0.5714285714285714, train_loss: 0.7881700164455419, val_loss: 1.0968423471074973 (40 / 100)\n",
            "train_acc: 0.7144622991347342, val_acc: 0.5369458128078818, train_loss: 0.7268828580936484, val_loss: 1.1820532353640778 (41 / 100)\n",
            "train_acc: 0.7317676143386898, val_acc: 0.5714285714285714, train_loss: 0.7118231184845065, val_loss: 1.2685390740192581 (42 / 100)\n",
            "train_acc: 0.7404202719406675, val_acc: 0.5862068965517241, train_loss: 0.7130141900702667, val_loss: 1.0530330500579232 (43 / 100)\n",
            "train_acc: 0.7737948084054388, val_acc: 0.5665024630541872, train_loss: 0.6292958386306857, val_loss: 1.175454576614455 (44 / 100)\n",
            "train_acc: 0.7824474660074165, val_acc: 0.5763546798029556, train_loss: 0.58251512139041, val_loss: 1.1369039982997726 (45 / 100)\n",
            "train_acc: 0.7503090234857849, val_acc: 0.5467980295566502, train_loss: 0.6340557824548596, val_loss: 1.2110897919227337 (46 / 100)\n",
            "train_acc: 0.7849196538936959, val_acc: 0.5812807881773399, train_loss: 0.596162891063761, val_loss: 1.1673362685248183 (47 / 100)\n",
            "train_acc: 0.788627935723115, val_acc: 0.6551724137931034, train_loss: 0.5810532530099706, val_loss: 1.0665241635491696 (48 / 100)\n",
            "train_acc: 0.8195302843016069, val_acc: 0.5960591133004927, train_loss: 0.48339614172653744, val_loss: 1.3444474277825191 (49 / 100)\n",
            "train_acc: 0.8529048207663782, val_acc: 0.6108374384236454, train_loss: 0.3781543772653832, val_loss: 1.421452060635454 (50 / 100)\n",
            "train_acc: 0.8751545117428925, val_acc: 0.5566502463054187, train_loss: 0.3557807612330716, val_loss: 1.4056067490225355 (51 / 100)\n",
            "train_acc: 0.8800988875154512, val_acc: 0.6108374384236454, train_loss: 0.3494974835988765, val_loss: 1.7603676524655572 (52 / 100)\n",
            "train_acc: 0.8800988875154512, val_acc: 0.645320197044335, train_loss: 0.34615789753544907, val_loss: 1.101978027027816 (53 / 100)\n",
            "train_acc: 0.8504326328800988, val_acc: 0.6157635467980296, train_loss: 0.42364104539708536, val_loss: 1.5226379427416572 (54 / 100)\n",
            "train_acc: 0.899876390605686, val_acc: 0.6403940886699507, train_loss: 0.2896549798944235, val_loss: 1.3372012884746045 (55 / 100)\n",
            "train_acc: 0.9122373300370828, val_acc: 0.6650246305418719, train_loss: 0.2293389679032881, val_loss: 1.1813663896081483 (56 / 100)\n",
            "train_acc: 0.9221260815822002, val_acc: 0.6354679802955665, train_loss: 0.22531891767999299, val_loss: 2.166120241428244 (57 / 100)\n",
            "train_acc: 0.907292954264524, val_acc: 0.6403940886699507, train_loss: 0.28814430510894623, val_loss: 1.2511681603093452 (58 / 100)\n",
            "train_acc: 0.8936959208899876, val_acc: 0.5320197044334976, train_loss: 0.28790176121059835, val_loss: 1.616968880072603 (59 / 100)\n",
            "train_acc: 0.9134734239802225, val_acc: 0.625615763546798, train_loss: 0.24483985305569228, val_loss: 1.5444904162085116 (60 / 100)\n",
            "train_acc: 0.9604449938195303, val_acc: 0.6600985221674877, train_loss: 0.14654318024997215, val_loss: 1.4372705271091368 (61 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6600985221674877, train_loss: 0.09201892830974828, val_loss: 1.5129883315762862 (62 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.6748768472906403, train_loss: 0.08014525648100562, val_loss: 1.6631151931039219 (63 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.6748768472906403, train_loss: 0.07908427140621378, val_loss: 1.7420732176362588 (64 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6600985221674877, train_loss: 0.08322204927578845, val_loss: 1.7954089541740605 (65 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6748768472906403, train_loss: 0.055183452345679514, val_loss: 1.7982734418267687 (66 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6600985221674877, train_loss: 0.04767672962547086, val_loss: 1.985860453450621 (67 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6847290640394089, train_loss: 0.06361225184757718, val_loss: 1.9847033951670079 (68 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6798029556650246, train_loss: 0.04938634526449611, val_loss: 2.0113645169535292 (69 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6798029556650246, train_loss: 0.03977053981776291, val_loss: 2.131243826133277 (70 / 100)\n",
            "train_acc: 0.9802224969097652, val_acc: 0.6798029556650246, train_loss: 0.061296846574846985, val_loss: 2.138686766765388 (71 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6798029556650246, train_loss: 0.04338561236047921, val_loss: 2.1515172479187914 (72 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6748768472906403, train_loss: 0.04200774129153182, val_loss: 2.1457340124205415 (73 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.6798029556650246, train_loss: 0.03715569657654638, val_loss: 2.1897675961696454 (74 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6798029556650246, train_loss: 0.03290504032955476, val_loss: 2.1348195246287753 (75 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6748768472906403, train_loss: 0.04886827348041888, val_loss: 2.1383886765963926 (76 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6600985221674877, train_loss: 0.04572392998283961, val_loss: 2.2607418639319286 (77 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6748768472906403, train_loss: 0.04128748053231257, val_loss: 2.256043868699097 (78 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6699507389162561, train_loss: 0.046727414184211946, val_loss: 2.2030920536060052 (79 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6748768472906403, train_loss: 0.05670898158117042, val_loss: 2.1223957876266515 (80 / 100)\n",
            "train_acc: 0.9802224969097652, val_acc: 0.6748768472906403, train_loss: 0.05207846662759486, val_loss: 2.184562521027814 (81 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6699507389162561, train_loss: 0.039389598354863, val_loss: 2.170117300132225 (82 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6600985221674877, train_loss: 0.05842442329793395, val_loss: 2.0433699415235096 (83 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.6650246305418719, train_loss: 0.028148641250336864, val_loss: 2.238167628278873 (84 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6699507389162561, train_loss: 0.04450215087980216, val_loss: 2.2522780460677123 (85 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6650246305418719, train_loss: 0.04113083333697985, val_loss: 2.195215009409806 (86 / 100)\n",
            "train_acc: 0.9962917181705809, val_acc: 0.6650246305418719, train_loss: 0.019868418371721604, val_loss: 2.3690920416357484 (87 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6748768472906403, train_loss: 0.036123196333094194, val_loss: 2.4030740860060518 (88 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6748768472906403, train_loss: 0.0338847466246012, val_loss: 2.4827363079991835 (89 / 100)\n",
            "train_acc: 0.9950556242274413, val_acc: 0.6798029556650246, train_loss: 0.02622190246770644, val_loss: 2.570009220409863 (90 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.6847290640394089, train_loss: 0.02586098683926614, val_loss: 2.435141711399473 (91 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.6748768472906403, train_loss: 0.028552846354518744, val_loss: 2.550051899966348 (92 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.6847290640394089, train_loss: 0.03419529905425309, val_loss: 2.3714167825106918 (93 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6896551724137931, train_loss: 0.027762692849921944, val_loss: 2.331142272855261 (94 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.6847290640394089, train_loss: 0.02822194906925536, val_loss: 2.422904704591911 (95 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.6847290640394089, train_loss: 0.021782379362580213, val_loss: 2.3505594078543153 (96 / 100)\n",
            "train_acc: 0.9962917181705809, val_acc: 0.6896551724137931, train_loss: 0.018172311399716826, val_loss: 2.4447800566997433 (97 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6945812807881774, train_loss: 0.033632485474585305, val_loss: 2.382970290231 (98 / 100)\n",
            "train_acc: 0.992583436341162, val_acc: 0.6748768472906403, train_loss: 0.02485905265336573, val_loss: 2.485448380996441 (99 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.6847290640394089, train_loss: 0.021002554480902786, val_loss: 2.518056775548775 (100 / 100)\n",
            "({'lr': 0.0005, 'batch_size': 8, 'weight_decay': 1e-05, 'gamma': 0.1}), val accuracy 0.6945812807881774, val loss 2.382970290231\n",
            "train_acc: 0.1903584672435105, val_acc: 0.18719211822660098, train_loss: 1.7802917099706321, val_loss: 1.7550017052683338 (1 / 100)\n",
            "train_acc: 0.21013597033374537, val_acc: 0.3103448275862069, train_loss: 1.7606686957834383, val_loss: 1.742342155555199 (2 / 100)\n",
            "train_acc: 0.24103831891223734, val_acc: 0.22167487684729065, train_loss: 1.744329983283327, val_loss: 1.7066170153359475 (3 / 100)\n",
            "train_acc: 0.2521631644004944, val_acc: 0.3251231527093596, train_loss: 1.7033872042951832, val_loss: 1.5980333230765582 (4 / 100)\n",
            "train_acc: 0.32014833127317677, val_acc: 0.3497536945812808, train_loss: 1.6489227684525536, val_loss: 1.5922433026318479 (5 / 100)\n",
            "train_acc: 0.2954264524103832, val_acc: 0.33497536945812806, train_loss: 1.642521990539117, val_loss: 1.560525747942807 (6 / 100)\n",
            "train_acc: 0.32138442521631644, val_acc: 0.37438423645320196, train_loss: 1.5807261932618244, val_loss: 1.4477067692526455 (7 / 100)\n",
            "train_acc: 0.34857849196538937, val_acc: 0.3694581280788177, train_loss: 1.5347427770734865, val_loss: 1.4325491201701424 (8 / 100)\n",
            "train_acc: 0.3473423980222497, val_acc: 0.35960591133004927, train_loss: 1.527747131690696, val_loss: 1.4899539518826113 (9 / 100)\n",
            "train_acc: 0.34610630407911, val_acc: 0.3448275862068966, train_loss: 1.475723144887257, val_loss: 1.4426588871208905 (10 / 100)\n",
            "train_acc: 0.39555006180469715, val_acc: 0.3793103448275862, train_loss: 1.4402727615111248, val_loss: 1.39484624616031 (11 / 100)\n",
            "train_acc: 0.3757725587144623, val_acc: 0.4088669950738916, train_loss: 1.43104218581993, val_loss: 1.397059611499016 (12 / 100)\n",
            "train_acc: 0.4054388133498146, val_acc: 0.37438423645320196, train_loss: 1.4137783616376012, val_loss: 1.5516478375261054 (13 / 100)\n",
            "train_acc: 0.411619283065513, val_acc: 0.43842364532019706, train_loss: 1.404482913400393, val_loss: 1.3259181970445981 (14 / 100)\n",
            "train_acc: 0.411619283065513, val_acc: 0.4236453201970443, train_loss: 1.3893038866988514, val_loss: 1.473584849258949 (15 / 100)\n",
            "train_acc: 0.4264524103831891, val_acc: 0.41379310344827586, train_loss: 1.3923581011952517, val_loss: 1.3377968900896646 (16 / 100)\n",
            "train_acc: 0.4400494437577256, val_acc: 0.41379310344827586, train_loss: 1.346884015906137, val_loss: 1.3069236384236753 (17 / 100)\n",
            "train_acc: 0.4338689740420272, val_acc: 0.4433497536945813, train_loss: 1.354652256694506, val_loss: 1.330927951582547 (18 / 100)\n",
            "train_acc: 0.4437577255871446, val_acc: 0.4876847290640394, train_loss: 1.3197717519272096, val_loss: 1.2718502858589436 (19 / 100)\n",
            "train_acc: 0.43016069221260816, val_acc: 0.4088669950738916, train_loss: 1.2917202809832447, val_loss: 1.3177432767276107 (20 / 100)\n",
            "train_acc: 0.4561186650185414, val_acc: 0.4433497536945813, train_loss: 1.2793013200771677, val_loss: 1.4287691257270099 (21 / 100)\n",
            "train_acc: 0.4610630407911001, val_acc: 0.49261083743842365, train_loss: 1.2812711874251017, val_loss: 1.2129236078027434 (22 / 100)\n",
            "train_acc: 0.4857849196538937, val_acc: 0.4975369458128079, train_loss: 1.2228589858053935, val_loss: 1.1823146419572126 (23 / 100)\n",
            "train_acc: 0.49814585908529047, val_acc: 0.4630541871921182, train_loss: 1.1920867159281143, val_loss: 1.174452129255962 (24 / 100)\n",
            "train_acc: 0.5043263288009888, val_acc: 0.45320197044334976, train_loss: 1.1712899405523047, val_loss: 1.243650295757895 (25 / 100)\n",
            "train_acc: 0.4721878862793572, val_acc: 0.5073891625615764, train_loss: 1.2450520310030584, val_loss: 1.15791585351446 (26 / 100)\n",
            "train_acc: 0.5302843016069221, val_acc: 0.4236453201970443, train_loss: 1.1761995472925701, val_loss: 1.3148603868014708 (27 / 100)\n",
            "train_acc: 0.5203955500618047, val_acc: 0.5221674876847291, train_loss: 1.1618508019465008, val_loss: 1.176203927970285 (28 / 100)\n",
            "train_acc: 0.5451174289245982, val_acc: 0.5467980295566502, train_loss: 1.1035093545029573, val_loss: 1.1250573916388262 (29 / 100)\n",
            "train_acc: 0.5599505562422744, val_acc: 0.5369458128078818, train_loss: 1.0737101224208496, val_loss: 1.1460143421671074 (30 / 100)\n",
            "train_acc: 0.5377008652657602, val_acc: 0.4433497536945813, train_loss: 1.0727192521537336, val_loss: 1.5360845550527713 (31 / 100)\n",
            "train_acc: 0.595797280593325, val_acc: 0.5665024630541872, train_loss: 1.0497298788818057, val_loss: 1.0892694918392913 (32 / 100)\n",
            "train_acc: 0.5908529048207664, val_acc: 0.5862068965517241, train_loss: 1.0200572998178905, val_loss: 1.057923952640571 (33 / 100)\n",
            "train_acc: 0.6056860321384425, val_acc: 0.5714285714285714, train_loss: 0.9994178083535326, val_loss: 1.1132606341333813 (34 / 100)\n",
            "train_acc: 0.6378244746600742, val_acc: 0.5467980295566502, train_loss: 0.9025711079021025, val_loss: 1.1281529342012453 (35 / 100)\n",
            "train_acc: 0.5970333745364648, val_acc: 0.5911330049261084, train_loss: 0.9521632294719682, val_loss: 1.0001714499713166 (36 / 100)\n",
            "train_acc: 0.646477132262052, val_acc: 0.6206896551724138, train_loss: 0.86444861072545, val_loss: 1.0360106570380074 (37 / 100)\n",
            "train_acc: 0.6872682323856613, val_acc: 0.5615763546798029, train_loss: 0.7815758960945497, val_loss: 1.1234036736887665 (38 / 100)\n",
            "train_acc: 0.6835599505562423, val_acc: 0.541871921182266, train_loss: 0.8039133548736572, val_loss: 1.292360291105186 (39 / 100)\n",
            "train_acc: 0.7206427688504327, val_acc: 0.5911330049261084, train_loss: 0.7194221569374848, val_loss: 1.151142398712083 (40 / 100)\n",
            "train_acc: 0.6983930778739185, val_acc: 0.5517241379310345, train_loss: 0.7223634133380187, val_loss: 1.535969267925018 (41 / 100)\n",
            "train_acc: 0.7194066749072929, val_acc: 0.5517241379310345, train_loss: 0.7095816692404163, val_loss: 1.1026959486782844 (42 / 100)\n",
            "train_acc: 0.7725587144622992, val_acc: 0.5911330049261084, train_loss: 0.6013774090850603, val_loss: 1.2366480974141012 (43 / 100)\n",
            "train_acc: 0.799752781211372, val_acc: 0.6305418719211823, train_loss: 0.5740004231225427, val_loss: 1.219193258309012 (44 / 100)\n",
            "train_acc: 0.7836835599505563, val_acc: 0.645320197044335, train_loss: 0.5735913880528567, val_loss: 1.0381434292628848 (45 / 100)\n",
            "train_acc: 0.7169344870210136, val_acc: 0.625615763546798, train_loss: 0.7548026248757418, val_loss: 1.0479087204181503 (46 / 100)\n",
            "train_acc: 0.8182941903584673, val_acc: 0.5615763546798029, train_loss: 0.4633676059755908, val_loss: 1.686431936295749 (47 / 100)\n",
            "train_acc: 0.7812113720642769, val_acc: 0.625615763546798, train_loss: 0.6002130545261322, val_loss: 1.2077961666830654 (48 / 100)\n",
            "train_acc: 0.8294190358467244, val_acc: 0.625615763546798, train_loss: 0.43452977986801394, val_loss: 1.3321759573050909 (49 / 100)\n",
            "train_acc: 0.8603213844252163, val_acc: 0.645320197044335, train_loss: 0.36783617167596616, val_loss: 1.1286644853394607 (50 / 100)\n",
            "train_acc: 0.8689740420271941, val_acc: 0.6403940886699507, train_loss: 0.35492478032931407, val_loss: 1.3160814801460416 (51 / 100)\n",
            "train_acc: 0.8281829419035847, val_acc: 0.6059113300492611, train_loss: 0.5016483591572464, val_loss: 1.4620805467878069 (52 / 100)\n",
            "train_acc: 0.8640296662546354, val_acc: 0.5911330049261084, train_loss: 0.3607119083109833, val_loss: 1.5388852428332926 (53 / 100)\n",
            "train_acc: 0.8899876390605687, val_acc: 0.6354679802955665, train_loss: 0.284395733044675, val_loss: 1.5258153425208454 (54 / 100)\n",
            "train_acc: 0.5648949320148331, val_acc: 0.5911330049261084, train_loss: 1.0984972310154635, val_loss: 1.2321084356073089 (55 / 100)\n",
            "train_acc: 0.799752781211372, val_acc: 0.625615763546798, train_loss: 0.5175953972324895, val_loss: 1.2252592617655036 (56 / 100)\n",
            "train_acc: 0.8603213844252163, val_acc: 0.6206896551724138, train_loss: 0.37476993181796836, val_loss: 1.6315551153544723 (57 / 100)\n",
            "train_acc: 0.8887515451174289, val_acc: 0.645320197044335, train_loss: 0.31677921784970314, val_loss: 1.6614157959745435 (58 / 100)\n",
            "train_acc: 0.9134734239802225, val_acc: 0.645320197044335, train_loss: 0.2578839916823379, val_loss: 1.3747265127492068 (59 / 100)\n",
            "train_acc: 0.9320148331273177, val_acc: 0.6108374384236454, train_loss: 0.19152593347433913, val_loss: 1.9139833741000134 (60 / 100)\n",
            "train_acc: 0.9505562422744128, val_acc: 0.6403940886699507, train_loss: 0.14367968042937138, val_loss: 1.5704651664336915 (61 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6502463054187192, train_loss: 0.07675840857591854, val_loss: 1.6483866710380968 (62 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.6403940886699507, train_loss: 0.08550155442783652, val_loss: 1.6767743009651823 (63 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6305418719211823, train_loss: 0.06758400786792391, val_loss: 1.7048859386314899 (64 / 100)\n",
            "train_acc: 0.9666254635352287, val_acc: 0.6354679802955665, train_loss: 0.10324416187107195, val_loss: 1.6668834204744236 (65 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.645320197044335, train_loss: 0.07266708431196743, val_loss: 1.7472999951815957 (66 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.6502463054187192, train_loss: 0.0802251913933583, val_loss: 1.7875163635890472 (67 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.6600985221674877, train_loss: 0.08249868465442445, val_loss: 1.8337557841404317 (68 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6551724137931034, train_loss: 0.06404276124037094, val_loss: 1.8685219252638041 (69 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.6600985221674877, train_loss: 0.06462162917270355, val_loss: 1.8860435907183022 (70 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.645320197044335, train_loss: 0.059976821482107874, val_loss: 1.899600329804303 (71 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.6551724137931034, train_loss: 0.06760666013795454, val_loss: 1.9487235484452083 (72 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6403940886699507, train_loss: 0.05035990572388299, val_loss: 1.9638090013283227 (73 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6650246305418719, train_loss: 0.0499923354468328, val_loss: 1.9800524033349136 (74 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6600985221674877, train_loss: 0.06381634448750499, val_loss: 1.9782643647029483 (75 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6551724137931034, train_loss: 0.06334621030998466, val_loss: 1.9715670038913857 (76 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.6650246305418719, train_loss: 0.08616977907965888, val_loss: 1.9410701593741995 (77 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6600985221674877, train_loss: 0.05959910883744361, val_loss: 1.9802100414713029 (78 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6650246305418719, train_loss: 0.04024693520903145, val_loss: 2.0525342939522466 (79 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6600985221674877, train_loss: 0.05127278688664195, val_loss: 2.0334990479676005 (80 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6600985221674877, train_loss: 0.04982811470408964, val_loss: 2.0681655445122367 (81 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6502463054187192, train_loss: 0.047011445420339464, val_loss: 2.056621800208914 (82 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6354679802955665, train_loss: 0.05786443007154429, val_loss: 2.0968810252368155 (83 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6551724137931034, train_loss: 0.04585717797132004, val_loss: 2.0686201297590885 (84 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6600985221674877, train_loss: 0.04845309714304355, val_loss: 2.1148313818306756 (85 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6650246305418719, train_loss: 0.05354637357006851, val_loss: 2.1458175778388977 (86 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6600985221674877, train_loss: 0.03895192933170993, val_loss: 2.200171543165968 (87 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6650246305418719, train_loss: 0.05398323877780193, val_loss: 2.1789375311635397 (88 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.6699507389162561, train_loss: 0.03054168300958735, val_loss: 2.239895169958105 (89 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.6600985221674877, train_loss: 0.03616975529674252, val_loss: 2.330162925085998 (90 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6650246305418719, train_loss: 0.029777096140075232, val_loss: 2.3546106965083795 (91 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6650246305418719, train_loss: 0.04006298554989846, val_loss: 2.320199875115174 (92 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6600985221674877, train_loss: 0.04451572570871511, val_loss: 2.3043625293106866 (93 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6748768472906403, train_loss: 0.04519947468129312, val_loss: 2.3527262028802203 (94 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6699507389162561, train_loss: 0.04379329206916988, val_loss: 2.331494937976593 (95 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6748768472906403, train_loss: 0.060130589677317915, val_loss: 2.375539594095916 (96 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6699507389162561, train_loss: 0.057621635524245214, val_loss: 2.31642453365138 (97 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6748768472906403, train_loss: 0.04225212961840541, val_loss: 2.3627748906318775 (98 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6699507389162561, train_loss: 0.0445089278321331, val_loss: 2.421446453174347 (99 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6798029556650246, train_loss: 0.03755956866095771, val_loss: 2.4103085865528127 (100 / 100)\n",
            "({'lr': 0.0005, 'batch_size': 8, 'weight_decay': 0.001, 'gamma': 0.05}), val accuracy 0.6798029556650246, val loss 2.4103085865528127\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7761770797748353, val_loss: 1.7523032591260712 (1 / 100)\n",
            "train_acc: 0.20519159456118666, val_acc: 0.18226600985221675, train_loss: 1.763675312323977, val_loss: 1.7430592258575515 (2 / 100)\n",
            "train_acc: 0.2249690976514215, val_acc: 0.29064039408866993, train_loss: 1.737621975622896, val_loss: 1.736579603162305 (3 / 100)\n",
            "train_acc: 0.2484548825710754, val_acc: 0.3103448275862069, train_loss: 1.7165320709402982, val_loss: 1.640741604302317 (4 / 100)\n",
            "train_acc: 0.2843016069221261, val_acc: 0.2561576354679803, train_loss: 1.667753151969061, val_loss: 1.6117521007659987 (5 / 100)\n",
            "train_acc: 0.3374536464771323, val_acc: 0.3103448275862069, train_loss: 1.6139834699288875, val_loss: 1.5677423013254927 (6 / 100)\n",
            "train_acc: 0.3374536464771323, val_acc: 0.30049261083743845, train_loss: 1.5707768658625034, val_loss: 1.6389580947424978 (7 / 100)\n",
            "train_acc: 0.3572311495673671, val_acc: 0.4187192118226601, train_loss: 1.5445162701813047, val_loss: 1.5012876294516577 (8 / 100)\n",
            "train_acc: 0.34363411619283063, val_acc: 0.3103448275862069, train_loss: 1.5083576373028371, val_loss: 1.5924437527586086 (9 / 100)\n",
            "train_acc: 0.3856613102595797, val_acc: 0.3399014778325123, train_loss: 1.4808991901364699, val_loss: 1.457246813574448 (10 / 100)\n",
            "train_acc: 0.3794808405438813, val_acc: 0.35467980295566504, train_loss: 1.4776602360167816, val_loss: 1.4019290126603225 (11 / 100)\n",
            "train_acc: 0.40173053152039556, val_acc: 0.3645320197044335, train_loss: 1.4461971099945465, val_loss: 1.4003686581926393 (12 / 100)\n",
            "train_acc: 0.39555006180469715, val_acc: 0.3694581280788177, train_loss: 1.414133751642866, val_loss: 1.4759686856434262 (13 / 100)\n",
            "train_acc: 0.415327564894932, val_acc: 0.45320197044334976, train_loss: 1.3804527705325773, val_loss: 1.3674742735078182 (14 / 100)\n",
            "train_acc: 0.4326328800988875, val_acc: 0.46798029556650245, train_loss: 1.3667268764840068, val_loss: 1.3823110534639782 (15 / 100)\n",
            "train_acc: 0.4338689740420272, val_acc: 0.47783251231527096, train_loss: 1.35214987467187, val_loss: 1.291908036899097 (16 / 100)\n",
            "train_acc: 0.44128553770086526, val_acc: 0.4187192118226601, train_loss: 1.3321546982186392, val_loss: 1.3186463669603095 (17 / 100)\n",
            "train_acc: 0.4511742892459827, val_acc: 0.4236453201970443, train_loss: 1.3428641131840766, val_loss: 1.321784789632694 (18 / 100)\n",
            "train_acc: 0.453646477132262, val_acc: 0.43842364532019706, train_loss: 1.3031875930993606, val_loss: 1.305860425744738 (19 / 100)\n",
            "train_acc: 0.4400494437577256, val_acc: 0.5073891625615764, train_loss: 1.3262761617620442, val_loss: 1.2284348075613012 (20 / 100)\n",
            "train_acc: 0.45488257107540175, val_acc: 0.4975369458128079, train_loss: 1.3040157532957193, val_loss: 1.2515232827275844 (21 / 100)\n",
            "train_acc: 0.4622991347342398, val_acc: 0.4729064039408867, train_loss: 1.269902616260373, val_loss: 1.1940573668245025 (22 / 100)\n",
            "train_acc: 0.46971569839307786, val_acc: 0.5024630541871922, train_loss: 1.2503423229578252, val_loss: 1.211716471047237 (23 / 100)\n",
            "train_acc: 0.5018541409147095, val_acc: 0.4630541871921182, train_loss: 1.2176432780488606, val_loss: 1.1879400778286562 (24 / 100)\n",
            "train_acc: 0.47713226205191595, val_acc: 0.4630541871921182, train_loss: 1.1984081701530955, val_loss: 1.2538204028688629 (25 / 100)\n",
            "train_acc: 0.5352286773794809, val_acc: 0.4039408866995074, train_loss: 1.1460416187314668, val_loss: 1.320281270102327 (26 / 100)\n",
            "train_acc: 0.5142150803461063, val_acc: 0.5172413793103449, train_loss: 1.1738257081016474, val_loss: 1.133832203343584 (27 / 100)\n",
            "train_acc: 0.5352286773794809, val_acc: 0.5320197044334976, train_loss: 1.1183652659723873, val_loss: 1.121002358462423 (28 / 100)\n",
            "train_acc: 0.5451174289245982, val_acc: 0.49261083743842365, train_loss: 1.1111136290287353, val_loss: 1.1832656502136456 (29 / 100)\n",
            "train_acc: 0.584672435105068, val_acc: 0.5073891625615764, train_loss: 1.036487243378855, val_loss: 1.1585325516503433 (30 / 100)\n",
            "train_acc: 0.5908529048207664, val_acc: 0.5812807881773399, train_loss: 1.0142986550466682, val_loss: 1.0409710721429346 (31 / 100)\n",
            "train_acc: 0.5772558714462299, val_acc: 0.5270935960591133, train_loss: 1.0440882755298402, val_loss: 1.3250831735545192 (32 / 100)\n",
            "train_acc: 0.6279357231149567, val_acc: 0.5665024630541872, train_loss: 0.9572591563532468, val_loss: 1.0984589113977743 (33 / 100)\n",
            "train_acc: 0.6588380716934487, val_acc: 0.5172413793103449, train_loss: 0.9042403143916937, val_loss: 1.2745202641768996 (34 / 100)\n",
            "train_acc: 0.6106304079110012, val_acc: 0.5714285714285714, train_loss: 0.95850663161543, val_loss: 1.124686754689428 (35 / 100)\n",
            "train_acc: 0.6724351050679852, val_acc: 0.5911330049261084, train_loss: 0.8801800677154503, val_loss: 1.045541217468055 (36 / 100)\n",
            "train_acc: 0.681087762669963, val_acc: 0.5566502463054187, train_loss: 0.8316371280271722, val_loss: 1.1234774307664392 (37 / 100)\n",
            "train_acc: 0.6440049443757726, val_acc: 0.5714285714285714, train_loss: 0.9058380779730817, val_loss: 1.059775812872525 (38 / 100)\n",
            "train_acc: 0.6847960444993819, val_acc: 0.5665024630541872, train_loss: 0.8128616174749744, val_loss: 1.11329992974333 (39 / 100)\n",
            "train_acc: 0.7206427688504327, val_acc: 0.5763546798029556, train_loss: 0.740440401659613, val_loss: 1.1354931443195624 (40 / 100)\n",
            "train_acc: 0.7552533992583437, val_acc: 0.5221674876847291, train_loss: 0.6461030888763729, val_loss: 1.3819274673320976 (41 / 100)\n",
            "train_acc: 0.7082818294190358, val_acc: 0.6403940886699507, train_loss: 0.7747244780378967, val_loss: 1.0034258116055004 (42 / 100)\n",
            "train_acc: 0.757725587144623, val_acc: 0.5467980295566502, train_loss: 0.598588752363462, val_loss: 1.2262857408065515 (43 / 100)\n",
            "train_acc: 0.7898640296662547, val_acc: 0.5960591133004927, train_loss: 0.5280212367273527, val_loss: 1.1660183829627013 (44 / 100)\n",
            "train_acc: 0.7799752781211372, val_acc: 0.5714285714285714, train_loss: 0.5495916485639084, val_loss: 1.2743532293535806 (45 / 100)\n",
            "train_acc: 0.8170580964153276, val_acc: 0.6403940886699507, train_loss: 0.4773473337347929, val_loss: 1.0683577210445123 (46 / 100)\n",
            "train_acc: 0.823238566131026, val_acc: 0.645320197044335, train_loss: 0.5097835921533912, val_loss: 1.0974046673093523 (47 / 100)\n",
            "train_acc: 0.857849196538937, val_acc: 0.5862068965517241, train_loss: 0.38019685264865605, val_loss: 1.1799184724027887 (48 / 100)\n",
            "train_acc: 0.8702101359703337, val_acc: 0.6305418719211823, train_loss: 0.36127450675398515, val_loss: 1.34804679578161 (49 / 100)\n",
            "train_acc: 0.8751545117428925, val_acc: 0.6305418719211823, train_loss: 0.37289710201055953, val_loss: 1.168285218072055 (50 / 100)\n",
            "train_acc: 0.8640296662546354, val_acc: 0.6502463054187192, train_loss: 0.361205554126367, val_loss: 1.3932546045392604 (51 / 100)\n",
            "train_acc: 0.8887515451174289, val_acc: 0.5763546798029556, train_loss: 0.30603227877646355, val_loss: 1.330662494222519 (52 / 100)\n",
            "train_acc: 0.899876390605686, val_acc: 0.6108374384236454, train_loss: 0.28831181110646137, val_loss: 1.4213312787228618 (53 / 100)\n",
            "train_acc: 0.9221260815822002, val_acc: 0.6206896551724138, train_loss: 0.22642887670118522, val_loss: 1.557025447850267 (54 / 100)\n",
            "train_acc: 0.9060568603213844, val_acc: 0.5221674876847291, train_loss: 0.24244077199763803, val_loss: 2.541313206474182 (55 / 100)\n",
            "train_acc: 0.8986402966625463, val_acc: 0.6354679802955665, train_loss: 0.2858858693515415, val_loss: 1.704751534708615 (56 / 100)\n",
            "train_acc: 0.9060568603213844, val_acc: 0.6650246305418719, train_loss: 0.24985001378949404, val_loss: 1.323801743573156 (57 / 100)\n",
            "train_acc: 0.9085290482076638, val_acc: 0.5862068965517241, train_loss: 0.25363707380330164, val_loss: 1.1230083639398585 (58 / 100)\n",
            "train_acc: 0.899876390605686, val_acc: 0.6798029556650246, train_loss: 0.2892750482476686, val_loss: 1.4046068793447146 (59 / 100)\n",
            "train_acc: 0.9134734239802225, val_acc: 0.6206896551724138, train_loss: 0.21856480033789635, val_loss: 1.2807389315713216 (60 / 100)\n",
            "train_acc: 0.957972805933251, val_acc: 0.6748768472906403, train_loss: 0.1249027754673999, val_loss: 1.435417378770894 (61 / 100)\n",
            "train_acc: 0.9592088998763906, val_acc: 0.6748768472906403, train_loss: 0.11010197066553444, val_loss: 1.4270406538629767 (62 / 100)\n",
            "train_acc: 0.9641532756489494, val_acc: 0.6896551724137931, train_loss: 0.08922649431876994, val_loss: 1.4662814680578673 (63 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6847290640394089, train_loss: 0.06884433798795872, val_loss: 1.4415936379009866 (64 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6650246305418719, train_loss: 0.04147968186141534, val_loss: 1.5525748597577287 (65 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6798029556650246, train_loss: 0.06792074154569723, val_loss: 1.626547404991582 (66 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.6748768472906403, train_loss: 0.06986702033704231, val_loss: 1.6191803742512105 (67 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6600985221674877, train_loss: 0.055029255498030276, val_loss: 1.6065440647707785 (68 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.6699507389162561, train_loss: 0.07637244912396256, val_loss: 1.6377662153079593 (69 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6945812807881774, train_loss: 0.05826743571513663, val_loss: 1.727535083376128 (70 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6896551724137931, train_loss: 0.038835632049551115, val_loss: 1.7598296809079024 (71 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6995073891625616, train_loss: 0.05732220006077487, val_loss: 1.769244699348957 (72 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6896551724137931, train_loss: 0.042092393296316026, val_loss: 1.8278221611318917 (73 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6798029556650246, train_loss: 0.037434993774546094, val_loss: 1.8312079281055282 (74 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6847290640394089, train_loss: 0.05101681993387833, val_loss: 1.8020402366304633 (75 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6650246305418719, train_loss: 0.042715595590758826, val_loss: 1.9054022284564127 (76 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6748768472906403, train_loss: 0.037762753748333794, val_loss: 1.8703224133388163 (77 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6847290640394089, train_loss: 0.045077824622061106, val_loss: 1.819984798067309 (78 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6945812807881774, train_loss: 0.04840157500598281, val_loss: 1.7572949577141277 (79 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.6847290640394089, train_loss: 0.03309222956375669, val_loss: 1.7864535314402556 (80 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6847290640394089, train_loss: 0.04195780202985842, val_loss: 1.8977523269911705 (81 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6798029556650246, train_loss: 0.036295725329697354, val_loss: 1.9093948843443922 (82 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6748768472906403, train_loss: 0.036119519853768746, val_loss: 1.9575519417894298 (83 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.6798029556650246, train_loss: 0.027347891528173195, val_loss: 2.0809678144642874 (84 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.6847290640394089, train_loss: 0.026563972566272184, val_loss: 2.0761067368126853 (85 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6995073891625616, train_loss: 0.03982419387077077, val_loss: 2.156247888879823 (86 / 100)\n",
            "train_acc: 0.992583436341162, val_acc: 0.6798029556650246, train_loss: 0.024609537884979812, val_loss: 2.0539305456753434 (87 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6650246305418719, train_loss: 0.0464735396565554, val_loss: 2.0732113639709397 (88 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6699507389162561, train_loss: 0.03202277974528347, val_loss: 2.1508406136423495 (89 / 100)\n",
            "train_acc: 0.992583436341162, val_acc: 0.6748768472906403, train_loss: 0.030073359958615674, val_loss: 2.1176841593728275 (90 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6748768472906403, train_loss: 0.038388290275602026, val_loss: 2.140389942770521 (91 / 100)\n",
            "train_acc: 0.992583436341162, val_acc: 0.6798029556650246, train_loss: 0.031249152420477756, val_loss: 2.189601905827452 (92 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6847290640394089, train_loss: 0.03379299805691864, val_loss: 2.1561623306697224 (93 / 100)\n",
            "train_acc: 0.992583436341162, val_acc: 0.6847290640394089, train_loss: 0.03299475320337435, val_loss: 2.1345168827789758 (94 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.6798029556650246, train_loss: 0.027340762694774954, val_loss: 2.15826400497864 (95 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.6699507389162561, train_loss: 0.032893426191379464, val_loss: 2.24660271875964 (96 / 100)\n",
            "train_acc: 0.9962917181705809, val_acc: 0.6699507389162561, train_loss: 0.017919134592685772, val_loss: 2.223163961776959 (97 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.6699507389162561, train_loss: 0.019473741906240342, val_loss: 2.1980306341730316 (98 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.6798029556650246, train_loss: 0.019049164391271262, val_loss: 2.3518697449139188 (99 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6748768472906403, train_loss: 0.04588226128567577, val_loss: 2.092450952295012 (100 / 100)\n",
            "({'lr': 0.0005, 'batch_size': 8, 'weight_decay': 0.001, 'gamma': 0.1}), val accuracy 0.6995073891625616, val loss 1.769244699348957\n",
            "train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.7867061913529196, val_loss: 1.7791771178175075 (1 / 100)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7720697596577102, val_loss: 1.7614627694848723 (2 / 100)\n",
            "train_acc: 0.20395550061804696, val_acc: 0.21182266009852216, train_loss: 1.7664495644669598, val_loss: 1.7573017275392129 (3 / 100)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.7659737813310659, val_loss: 1.7498102505218807 (4 / 100)\n",
            "train_acc: 0.19901112484548825, val_acc: 0.18226600985221675, train_loss: 1.760421065081772, val_loss: 1.7454664683694323 (5 / 100)\n",
            "train_acc: 0.2200247218788628, val_acc: 0.21674876847290642, train_loss: 1.750442646489892, val_loss: 1.731674015815622 (6 / 100)\n",
            "train_acc: 0.23362175525339926, val_acc: 0.22167487684729065, train_loss: 1.74693813680571, val_loss: 1.7249166642503786 (7 / 100)\n",
            "train_acc: 0.2719406674907293, val_acc: 0.2955665024630542, train_loss: 1.7270258645928835, val_loss: 1.6690115875798492 (8 / 100)\n",
            "train_acc: 0.2978986402966625, val_acc: 0.2561576354679803, train_loss: 1.6576331276239364, val_loss: 1.70579773275723 (9 / 100)\n",
            "train_acc: 0.32014833127317677, val_acc: 0.26108374384236455, train_loss: 1.6355017615189806, val_loss: 1.604662971543561 (10 / 100)\n",
            "train_acc: 0.3003708281829419, val_acc: 0.28078817733990147, train_loss: 1.601203126429922, val_loss: 1.5613761024522077 (11 / 100)\n",
            "train_acc: 0.3288009888751545, val_acc: 0.3645320197044335, train_loss: 1.5696446093994254, val_loss: 1.505413718411488 (12 / 100)\n",
            "train_acc: 0.3411619283065513, val_acc: 0.3399014778325123, train_loss: 1.5446122242287446, val_loss: 1.488220132630447 (13 / 100)\n",
            "train_acc: 0.34239802224969096, val_acc: 0.3399014778325123, train_loss: 1.5430579537661615, val_loss: 1.500946936936214 (14 / 100)\n",
            "train_acc: 0.3683559950556242, val_acc: 0.3448275862068966, train_loss: 1.4966775345419188, val_loss: 1.4993030819399604 (15 / 100)\n",
            "train_acc: 0.377008652657602, val_acc: 0.3793103448275862, train_loss: 1.5006215747708296, val_loss: 1.3990886387566628 (16 / 100)\n",
            "train_acc: 0.3683559950556242, val_acc: 0.39901477832512317, train_loss: 1.469282195506196, val_loss: 1.4177656432090722 (17 / 100)\n",
            "train_acc: 0.3967861557478368, val_acc: 0.3645320197044335, train_loss: 1.4385593073035052, val_loss: 1.4859479812565695 (18 / 100)\n",
            "train_acc: 0.3831891223733004, val_acc: 0.4630541871921182, train_loss: 1.4326753396775724, val_loss: 1.4042026127500487 (19 / 100)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.35467980295566504, train_loss: 1.4499045140368976, val_loss: 1.4209468352970818 (20 / 100)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.39408866995073893, train_loss: 1.396391005244921, val_loss: 1.4266862775304634 (21 / 100)\n",
            "train_acc: 0.3967861557478368, val_acc: 0.4039408866995074, train_loss: 1.3956549357719563, val_loss: 1.4264585102720213 (22 / 100)\n",
            "train_acc: 0.4103831891223733, val_acc: 0.4482758620689655, train_loss: 1.3828928025602851, val_loss: 1.3465086433100584 (23 / 100)\n",
            "train_acc: 0.40914709517923364, val_acc: 0.3891625615763547, train_loss: 1.4008320253181221, val_loss: 1.345886016126924 (24 / 100)\n",
            "train_acc: 0.4227441285537701, val_acc: 0.45320197044334976, train_loss: 1.3455022557262142, val_loss: 1.3319464499140021 (25 / 100)\n",
            "train_acc: 0.43139678615574784, val_acc: 0.4088669950738916, train_loss: 1.328508162380591, val_loss: 1.3394833447897962 (26 / 100)\n",
            "train_acc: 0.446229913473424, val_acc: 0.43842364532019706, train_loss: 1.3123493850304848, val_loss: 1.3016286113579285 (27 / 100)\n",
            "train_acc: 0.4511742892459827, val_acc: 0.4039408866995074, train_loss: 1.3228467029458366, val_loss: 1.3226266262566515 (28 / 100)\n",
            "train_acc: 0.45982694684796044, val_acc: 0.47783251231527096, train_loss: 1.3006151792292837, val_loss: 1.3117484729278264 (29 / 100)\n",
            "train_acc: 0.45241038318912236, val_acc: 0.42857142857142855, train_loss: 1.2864911405942938, val_loss: 1.2555958520015473 (30 / 100)\n",
            "train_acc: 0.46600741656365885, val_acc: 0.4630541871921182, train_loss: 1.3370155972809667, val_loss: 1.2945422110299172 (31 / 100)\n",
            "train_acc: 0.45982694684796044, val_acc: 0.46798029556650245, train_loss: 1.272229936715257, val_loss: 1.2673285741524156 (32 / 100)\n",
            "train_acc: 0.4684796044499382, val_acc: 0.47783251231527096, train_loss: 1.2663371651370092, val_loss: 1.3187076945610234 (33 / 100)\n",
            "train_acc: 0.5006180469715699, val_acc: 0.43349753694581283, train_loss: 1.254769768791529, val_loss: 1.2367622811218788 (34 / 100)\n",
            "train_acc: 0.4820766378244747, val_acc: 0.5172413793103449, train_loss: 1.2586540495067355, val_loss: 1.2603292932064074 (35 / 100)\n",
            "train_acc: 0.47713226205191595, val_acc: 0.4876847290640394, train_loss: 1.2310883649642153, val_loss: 1.2222722387078948 (36 / 100)\n",
            "train_acc: 0.4721878862793572, val_acc: 0.47783251231527096, train_loss: 1.2333804078685633, val_loss: 1.255423502969037 (37 / 100)\n",
            "train_acc: 0.5129789864029666, val_acc: 0.4482758620689655, train_loss: 1.2032629041500822, val_loss: 1.2001735024851532 (38 / 100)\n",
            "train_acc: 0.4857849196538937, val_acc: 0.5024630541871922, train_loss: 1.2149709860974984, val_loss: 1.2068492300404703 (39 / 100)\n",
            "train_acc: 0.515451174289246, val_acc: 0.4630541871921182, train_loss: 1.1833696530392792, val_loss: 1.236448057619809 (40 / 100)\n",
            "train_acc: 0.5018541409147095, val_acc: 0.47783251231527096, train_loss: 1.1853578470103967, val_loss: 1.1723746542860134 (41 / 100)\n",
            "train_acc: 0.5290482076637825, val_acc: 0.5369458128078818, train_loss: 1.120998598147087, val_loss: 1.1161465098705199 (42 / 100)\n",
            "train_acc: 0.522867737948084, val_acc: 0.49261083743842365, train_loss: 1.114392774214114, val_loss: 1.1793885985618742 (43 / 100)\n",
            "train_acc: 0.5562422744128553, val_acc: 0.4975369458128079, train_loss: 1.1130529779144211, val_loss: 1.1684427361182979 (44 / 100)\n",
            "train_acc: 0.5364647713226205, val_acc: 0.541871921182266, train_loss: 1.1141966344841625, val_loss: 1.134100947474024 (45 / 100)\n",
            "train_acc: 0.5772558714462299, val_acc: 0.49261083743842365, train_loss: 1.0509094344670753, val_loss: 1.1838229157654523 (46 / 100)\n",
            "train_acc: 0.5500618046971569, val_acc: 0.5320197044334976, train_loss: 1.0849164381015433, val_loss: 1.2440890693312208 (47 / 100)\n",
            "train_acc: 0.5636588380716935, val_acc: 0.4876847290640394, train_loss: 1.0332722192346977, val_loss: 1.3864203148287506 (48 / 100)\n",
            "train_acc: 0.5834363411619283, val_acc: 0.5665024630541872, train_loss: 1.0056097283498908, val_loss: 1.063759797605975 (49 / 100)\n",
            "train_acc: 0.630407911001236, val_acc: 0.5123152709359606, train_loss: 0.9219692154926776, val_loss: 1.096938664396408 (50 / 100)\n",
            "train_acc: 0.630407911001236, val_acc: 0.5320197044334976, train_loss: 0.9297880701287863, val_loss: 1.2912816352444916 (51 / 100)\n",
            "train_acc: 0.584672435105068, val_acc: 0.5862068965517241, train_loss: 1.0464417449623458, val_loss: 1.107157943577602 (52 / 100)\n",
            "train_acc: 0.619283065512979, val_acc: 0.4827586206896552, train_loss: 0.9670158443403775, val_loss: 1.3208740051156782 (53 / 100)\n",
            "train_acc: 0.6563658838071693, val_acc: 0.5714285714285714, train_loss: 0.8825650866453079, val_loss: 1.1135601592181352 (54 / 100)\n",
            "train_acc: 0.6711990111248455, val_acc: 0.5270935960591133, train_loss: 0.8735643067524961, val_loss: 1.2401824464351672 (55 / 100)\n",
            "train_acc: 0.6971569839307787, val_acc: 0.5467980295566502, train_loss: 0.8067885930517548, val_loss: 1.196848145553044 (56 / 100)\n",
            "train_acc: 0.6909765142150803, val_acc: 0.5911330049261084, train_loss: 0.7814809393381749, val_loss: 1.0750915968946635 (57 / 100)\n",
            "train_acc: 0.7095179233621756, val_acc: 0.5960591133004927, train_loss: 0.7632226355659357, val_loss: 0.9929653861252545 (58 / 100)\n",
            "train_acc: 0.715698393077874, val_acc: 0.5911330049261084, train_loss: 0.7356826056656054, val_loss: 1.0308979403209217 (59 / 100)\n",
            "train_acc: 0.7404202719406675, val_acc: 0.5615763546798029, train_loss: 0.6882686417536034, val_loss: 1.0941929042045706 (60 / 100)\n",
            "train_acc: 0.7626699629171817, val_acc: 0.5714285714285714, train_loss: 0.6229946158725045, val_loss: 1.0588257315710847 (61 / 100)\n",
            "train_acc: 0.7948084054388134, val_acc: 0.5763546798029556, train_loss: 0.5374221288229538, val_loss: 1.0818868308818985 (62 / 100)\n",
            "train_acc: 0.8207663782447466, val_acc: 0.6009852216748769, train_loss: 0.47850792345732485, val_loss: 1.0911050556328497 (63 / 100)\n",
            "train_acc: 0.8220024721878862, val_acc: 0.5960591133004927, train_loss: 0.4859554612518683, val_loss: 1.0781594300504975 (64 / 100)\n",
            "train_acc: 0.8467243510506799, val_acc: 0.5960591133004927, train_loss: 0.4502160026057542, val_loss: 1.1288301571836612 (65 / 100)\n",
            "train_acc: 0.8343634116192831, val_acc: 0.5911330049261084, train_loss: 0.43685992428634607, val_loss: 1.1312250189593274 (66 / 100)\n",
            "train_acc: 0.8380716934487021, val_acc: 0.5960591133004927, train_loss: 0.4350547468927499, val_loss: 1.129136641037288 (67 / 100)\n",
            "train_acc: 0.8689740420271941, val_acc: 0.6009852216748769, train_loss: 0.37153786641852965, val_loss: 1.1601834194413547 (68 / 100)\n",
            "train_acc: 0.8590852904820766, val_acc: 0.5812807881773399, train_loss: 0.3830832622401941, val_loss: 1.1607770990268351 (69 / 100)\n",
            "train_acc: 0.857849196538937, val_acc: 0.5911330049261084, train_loss: 0.4073346132695749, val_loss: 1.1730335005398453 (70 / 100)\n",
            "train_acc: 0.8677379480840544, val_acc: 0.5862068965517241, train_loss: 0.34137476212604084, val_loss: 1.1524767338348727 (71 / 100)\n",
            "train_acc: 0.8677379480840544, val_acc: 0.5862068965517241, train_loss: 0.36262630299194193, val_loss: 1.1709032587229913 (72 / 100)\n",
            "train_acc: 0.8665018541409147, val_acc: 0.6009852216748769, train_loss: 0.35522270515763715, val_loss: 1.1934796968117136 (73 / 100)\n",
            "train_acc: 0.8702101359703337, val_acc: 0.5960591133004927, train_loss: 0.33860316209504276, val_loss: 1.1809903568234936 (74 / 100)\n",
            "train_acc: 0.8776266996291718, val_acc: 0.6059113300492611, train_loss: 0.332035284807125, val_loss: 1.2076430631975823 (75 / 100)\n",
            "train_acc: 0.8862793572311496, val_acc: 0.6059113300492611, train_loss: 0.32872251136635966, val_loss: 1.1761481331780625 (76 / 100)\n",
            "train_acc: 0.857849196538937, val_acc: 0.6206896551724138, train_loss: 0.3500476162719491, val_loss: 1.1709582884910659 (77 / 100)\n",
            "train_acc: 0.8974042027194067, val_acc: 0.6108374384236454, train_loss: 0.292691414045315, val_loss: 1.2080847644453565 (78 / 100)\n",
            "train_acc: 0.9060568603213844, val_acc: 0.6108374384236454, train_loss: 0.2665395858629672, val_loss: 1.229655185062897 (79 / 100)\n",
            "train_acc: 0.9085290482076638, val_acc: 0.5862068965517241, train_loss: 0.2680315102539192, val_loss: 1.2235180992504646 (80 / 100)\n",
            "train_acc: 0.8838071693448702, val_acc: 0.6206896551724138, train_loss: 0.2948787060597329, val_loss: 1.2275345136085754 (81 / 100)\n",
            "train_acc: 0.9122373300370828, val_acc: 0.6157635467980296, train_loss: 0.26230429722587906, val_loss: 1.2566347315980884 (82 / 100)\n",
            "train_acc: 0.8887515451174289, val_acc: 0.6206896551724138, train_loss: 0.3069091743532895, val_loss: 1.2438168879506624 (83 / 100)\n",
            "train_acc: 0.9097651421508035, val_acc: 0.6009852216748769, train_loss: 0.2645287722607036, val_loss: 1.267398925250387 (84 / 100)\n",
            "train_acc: 0.8887515451174289, val_acc: 0.6059113300492611, train_loss: 0.29874491750530907, val_loss: 1.2604633764974003 (85 / 100)\n",
            "train_acc: 0.9221260815822002, val_acc: 0.6059113300492611, train_loss: 0.24895509202562835, val_loss: 1.3823545384289595 (86 / 100)\n",
            "train_acc: 0.892459826946848, val_acc: 0.6305418719211823, train_loss: 0.26489595300776997, val_loss: 1.2895299554458393 (87 / 100)\n",
            "train_acc: 0.9048207663782447, val_acc: 0.6157635467980296, train_loss: 0.2761364952880977, val_loss: 1.3012558177774176 (88 / 100)\n",
            "train_acc: 0.9295426452410384, val_acc: 0.6059113300492611, train_loss: 0.22071441569640698, val_loss: 1.3154685059791715 (89 / 100)\n",
            "train_acc: 0.92336217552534, val_acc: 0.6206896551724138, train_loss: 0.21897461726382025, val_loss: 1.3614792227745056 (90 / 100)\n",
            "train_acc: 0.9159456118665018, val_acc: 0.6157635467980296, train_loss: 0.22278431275276966, val_loss: 1.3214189824212361 (91 / 100)\n",
            "train_acc: 0.9147095179233622, val_acc: 0.6108374384236454, train_loss: 0.22428327259159794, val_loss: 1.386951793003552 (92 / 100)\n",
            "train_acc: 0.9258343634116193, val_acc: 0.6206896551724138, train_loss: 0.2181726547305752, val_loss: 1.353686641149333 (93 / 100)\n",
            "train_acc: 0.9394313967861557, val_acc: 0.6157635467980296, train_loss: 0.18654921251189133, val_loss: 1.4066446959385144 (94 / 100)\n",
            "train_acc: 0.9258343634116193, val_acc: 0.6157635467980296, train_loss: 0.20479241536633194, val_loss: 1.4080676895937896 (95 / 100)\n",
            "train_acc: 0.9159456118665018, val_acc: 0.6354679802955665, train_loss: 0.2201563859604787, val_loss: 1.358423742167468 (96 / 100)\n",
            "train_acc: 0.9196538936959209, val_acc: 0.6108374384236454, train_loss: 0.24322783448750657, val_loss: 1.4080765830178565 (97 / 100)\n",
            "train_acc: 0.9208899876390606, val_acc: 0.6157635467980296, train_loss: 0.2165315687859014, val_loss: 1.3669103313549398 (98 / 100)\n",
            "train_acc: 0.9122373300370828, val_acc: 0.6108374384236454, train_loss: 0.23265955522564344, val_loss: 1.39363078002272 (99 / 100)\n",
            "train_acc: 0.927070457354759, val_acc: 0.6206896551724138, train_loss: 0.2056829401548625, val_loss: 1.4008251697265457 (100 / 100)\n",
            "({'lr': 0.0005, 'batch_size': 16, 'weight_decay': 1e-05, 'gamma': 0.05}), val accuracy 0.6354679802955665, val loss 1.358423742167468\n",
            "train_acc: 0.19283065512978986, val_acc: 0.33497536945812806, train_loss: 1.7881716809549792, val_loss: 1.7779768288429147 (1 / 100)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7734147214476936, val_loss: 1.7583331591977274 (2 / 100)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7631002272604717, val_loss: 1.7497676041325911 (3 / 100)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.21182266009852216, train_loss: 1.761546286281166, val_loss: 1.7403714310359486 (4 / 100)\n",
            "train_acc: 0.2373300370828183, val_acc: 0.2857142857142857, train_loss: 1.7535580490958704, val_loss: 1.727417577076428 (5 / 100)\n",
            "train_acc: 0.22249690976514216, val_acc: 0.3103448275862069, train_loss: 1.7354290900920024, val_loss: 1.7050834136643434 (6 / 100)\n",
            "train_acc: 0.27935723114956734, val_acc: 0.2660098522167488, train_loss: 1.7142355845796753, val_loss: 1.6951643909726823 (7 / 100)\n",
            "train_acc: 0.315203955500618, val_acc: 0.31527093596059114, train_loss: 1.6588046513912262, val_loss: 1.6063124757682161 (8 / 100)\n",
            "train_acc: 0.28182941903584674, val_acc: 0.3103448275862069, train_loss: 1.6548274804104686, val_loss: 1.5538728566005313 (9 / 100)\n",
            "train_acc: 0.3374536464771323, val_acc: 0.2955665024630542, train_loss: 1.5986272764146991, val_loss: 1.5790962145246308 (10 / 100)\n",
            "train_acc: 0.33250927070457353, val_acc: 0.37438423645320196, train_loss: 1.5502133575741235, val_loss: 1.4930586715049932 (11 / 100)\n",
            "train_acc: 0.33127317676143386, val_acc: 0.3399014778325123, train_loss: 1.561934000187368, val_loss: 1.5445310229738358 (12 / 100)\n",
            "train_acc: 0.34857849196538937, val_acc: 0.35467980295566504, train_loss: 1.5348186510602388, val_loss: 1.4349320686509457 (13 / 100)\n",
            "train_acc: 0.36093943139678614, val_acc: 0.3694581280788177, train_loss: 1.5088818967710762, val_loss: 1.4566901886991679 (14 / 100)\n",
            "train_acc: 0.380716934487021, val_acc: 0.4236453201970443, train_loss: 1.4648187889893654, val_loss: 1.4202055707940915 (15 / 100)\n",
            "train_acc: 0.3868974042027194, val_acc: 0.3448275862068966, train_loss: 1.4487018480583822, val_loss: 1.485700742364517 (16 / 100)\n",
            "train_acc: 0.35599505562422745, val_acc: 0.4088669950738916, train_loss: 1.4959891680881325, val_loss: 1.4378071929433662 (17 / 100)\n",
            "train_acc: 0.40914709517923364, val_acc: 0.3793103448275862, train_loss: 1.4267218865923887, val_loss: 1.4076308417202803 (18 / 100)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.4039408866995074, train_loss: 1.4248939478500517, val_loss: 1.411674170658506 (19 / 100)\n",
            "train_acc: 0.42027194066749074, val_acc: 0.3448275862068966, train_loss: 1.4081092977111214, val_loss: 1.468852196420942 (20 / 100)\n",
            "train_acc: 0.41656365883807167, val_acc: 0.4088669950738916, train_loss: 1.3822018216065923, val_loss: 1.3848294865321644 (21 / 100)\n",
            "train_acc: 0.39184177997527814, val_acc: 0.4039408866995074, train_loss: 1.4133257221969302, val_loss: 1.4318008276042093 (22 / 100)\n",
            "train_acc: 0.41409147095179233, val_acc: 0.3793103448275862, train_loss: 1.3843383708017865, val_loss: 1.4365125583310432 (23 / 100)\n",
            "train_acc: 0.3930778739184178, val_acc: 0.45320197044334976, train_loss: 1.3970913030897878, val_loss: 1.333253586233543 (24 / 100)\n",
            "train_acc: 0.3930778739184178, val_acc: 0.39901477832512317, train_loss: 1.3616378787716181, val_loss: 1.3256547644807788 (25 / 100)\n",
            "train_acc: 0.43757725587144625, val_acc: 0.43842364532019706, train_loss: 1.3408546771932444, val_loss: 1.3316951579061047 (26 / 100)\n",
            "train_acc: 0.44252163164400493, val_acc: 0.458128078817734, train_loss: 1.318897257481871, val_loss: 1.3378781080245972 (27 / 100)\n",
            "train_acc: 0.44746600741656367, val_acc: 0.4433497536945813, train_loss: 1.3189381002937761, val_loss: 1.2844100403668257 (28 / 100)\n",
            "train_acc: 0.4264524103831891, val_acc: 0.43842364532019706, train_loss: 1.323630181466988, val_loss: 1.2848032660084991 (29 / 100)\n",
            "train_acc: 0.45488257107540175, val_acc: 0.43842364532019706, train_loss: 1.3023898420286708, val_loss: 1.3630911041363118 (30 / 100)\n",
            "train_acc: 0.44746600741656367, val_acc: 0.41379310344827586, train_loss: 1.3076156170023387, val_loss: 1.3195855623395571 (31 / 100)\n",
            "train_acc: 0.4363411619283066, val_acc: 0.4876847290640394, train_loss: 1.3391347083232017, val_loss: 1.2600826958717384 (32 / 100)\n",
            "train_acc: 0.47713226205191595, val_acc: 0.43349753694581283, train_loss: 1.2605528722440063, val_loss: 1.262712636604685 (33 / 100)\n",
            "train_acc: 0.4783683559950556, val_acc: 0.45320197044334976, train_loss: 1.2518303472415184, val_loss: 1.3437247164730954 (34 / 100)\n",
            "train_acc: 0.4894932014833127, val_acc: 0.4729064039408867, train_loss: 1.25080969041888, val_loss: 1.1931246196107912 (35 / 100)\n",
            "train_acc: 0.4894932014833127, val_acc: 0.47783251231527096, train_loss: 1.2251961077983653, val_loss: 1.2056321428327137 (36 / 100)\n",
            "train_acc: 0.5080346106304079, val_acc: 0.4482758620689655, train_loss: 1.2078783833346938, val_loss: 1.269682253816445 (37 / 100)\n",
            "train_acc: 0.48825710754017304, val_acc: 0.4630541871921182, train_loss: 1.2414368508920093, val_loss: 1.264318249495746 (38 / 100)\n",
            "train_acc: 0.511742892459827, val_acc: 0.4827586206896552, train_loss: 1.1690988487602016, val_loss: 1.1597017590048277 (39 / 100)\n",
            "train_acc: 0.5574783683559951, val_acc: 0.4827586206896552, train_loss: 1.1609088667243608, val_loss: 1.2080183481347972 (40 / 100)\n",
            "train_acc: 0.5142150803461063, val_acc: 0.458128078817734, train_loss: 1.1940429357279954, val_loss: 1.2926069054697535 (41 / 100)\n",
            "train_acc: 0.5265760197775031, val_acc: 0.4975369458128079, train_loss: 1.138552622683116, val_loss: 1.146689494255141 (42 / 100)\n",
            "train_acc: 0.5488257107540173, val_acc: 0.5172413793103449, train_loss: 1.135077241324671, val_loss: 1.1679491016077879 (43 / 100)\n",
            "train_acc: 0.5624227441285538, val_acc: 0.46798029556650245, train_loss: 1.1035882181525967, val_loss: 1.2205086746826548 (44 / 100)\n",
            "train_acc: 0.5512978986402967, val_acc: 0.5172413793103449, train_loss: 1.099695562432222, val_loss: 1.1848316685906772 (45 / 100)\n",
            "train_acc: 0.5661310259579728, val_acc: 0.4876847290640394, train_loss: 1.0399584373820698, val_loss: 1.1969605848707001 (46 / 100)\n",
            "train_acc: 0.5587144622991347, val_acc: 0.4876847290640394, train_loss: 1.0789792754299414, val_loss: 1.278105392244649 (47 / 100)\n",
            "train_acc: 0.5908529048207664, val_acc: 0.5467980295566502, train_loss: 1.005557960573911, val_loss: 1.1506430330534874 (48 / 100)\n",
            "train_acc: 0.6007416563658838, val_acc: 0.5320197044334976, train_loss: 1.011663081472088, val_loss: 1.091199424172857 (49 / 100)\n",
            "train_acc: 0.5896168108776267, val_acc: 0.5517241379310345, train_loss: 0.9906982232083201, val_loss: 1.087663529541692 (50 / 100)\n",
            "train_acc: 0.6266996291718171, val_acc: 0.5517241379310345, train_loss: 0.9540937812572946, val_loss: 1.0762777451811165 (51 / 100)\n",
            "train_acc: 0.6106304079110012, val_acc: 0.5862068965517241, train_loss: 0.9327894114740699, val_loss: 1.0285662741496646 (52 / 100)\n",
            "train_acc: 0.6440049443757726, val_acc: 0.5467980295566502, train_loss: 0.9088691694038024, val_loss: 1.1537557288343683 (53 / 100)\n",
            "train_acc: 0.6477132262051916, val_acc: 0.5566502463054187, train_loss: 0.9138181623628614, val_loss: 1.097275420949964 (54 / 100)\n",
            "train_acc: 0.6427688504326329, val_acc: 0.5714285714285714, train_loss: 0.8923869119733756, val_loss: 1.0661568183617052 (55 / 100)\n",
            "train_acc: 0.688504326328801, val_acc: 0.5172413793103449, train_loss: 0.8679714138045151, val_loss: 1.1385827874902434 (56 / 100)\n",
            "train_acc: 0.6860321384425216, val_acc: 0.5665024630541872, train_loss: 0.7727720128148978, val_loss: 1.1353192940134134 (57 / 100)\n",
            "train_acc: 0.6897404202719407, val_acc: 0.5467980295566502, train_loss: 0.797271771763989, val_loss: 1.2765797687868767 (58 / 100)\n",
            "train_acc: 0.7058096415327565, val_acc: 0.541871921182266, train_loss: 0.7713309604541039, val_loss: 1.1246428947730605 (59 / 100)\n",
            "train_acc: 0.723114956736712, val_acc: 0.6059113300492611, train_loss: 0.7205950556638362, val_loss: 1.1215850367334677 (60 / 100)\n",
            "train_acc: 0.7601977750309024, val_acc: 0.6108374384236454, train_loss: 0.6041977638367227, val_loss: 1.0954970053851312 (61 / 100)\n",
            "train_acc: 0.8121137206427689, val_acc: 0.625615763546798, train_loss: 0.5147940539311714, val_loss: 1.081510189425182 (62 / 100)\n",
            "train_acc: 0.8454882571075402, val_acc: 0.6059113300492611, train_loss: 0.42254629007302347, val_loss: 1.1745587023608204 (63 / 100)\n",
            "train_acc: 0.8430160692212608, val_acc: 0.6059113300492611, train_loss: 0.4095366046072084, val_loss: 1.1686384918654493 (64 / 100)\n",
            "train_acc: 0.8541409147095179, val_acc: 0.6009852216748769, train_loss: 0.41736568376071964, val_loss: 1.1648910016261886 (65 / 100)\n",
            "train_acc: 0.8380716934487021, val_acc: 0.6059113300492611, train_loss: 0.4053583110145055, val_loss: 1.187702827559316 (66 / 100)\n",
            "train_acc: 0.8800988875154512, val_acc: 0.6009852216748769, train_loss: 0.32018363225297963, val_loss: 1.2942142283975198 (67 / 100)\n",
            "train_acc: 0.8899876390605687, val_acc: 0.5911330049261084, train_loss: 0.3117759752700886, val_loss: 1.3193764225602738 (68 / 100)\n",
            "train_acc: 0.8726823238566132, val_acc: 0.6354679802955665, train_loss: 0.32688103282849484, val_loss: 1.2678606107904407 (69 / 100)\n",
            "train_acc: 0.8739184177997528, val_acc: 0.6157635467980296, train_loss: 0.33774629835734704, val_loss: 1.2432995206616781 (70 / 100)\n",
            "train_acc: 0.8763906056860321, val_acc: 0.5911330049261084, train_loss: 0.32009298375569406, val_loss: 1.2903834017626759 (71 / 100)\n",
            "train_acc: 0.899876390605686, val_acc: 0.6009852216748769, train_loss: 0.3108402561345707, val_loss: 1.3460353689240705 (72 / 100)\n",
            "train_acc: 0.8813349814585909, val_acc: 0.6059113300492611, train_loss: 0.3140381436670666, val_loss: 1.3171994098888828 (73 / 100)\n",
            "train_acc: 0.9011124845488258, val_acc: 0.6009852216748769, train_loss: 0.2682632165137858, val_loss: 1.3950019408329366 (74 / 100)\n",
            "train_acc: 0.8986402966625463, val_acc: 0.6009852216748769, train_loss: 0.2805325145774483, val_loss: 1.3370464649693719 (75 / 100)\n",
            "train_acc: 0.9035846724351051, val_acc: 0.6009852216748769, train_loss: 0.27302038220156843, val_loss: 1.4200451042264552 (76 / 100)\n",
            "train_acc: 0.899876390605686, val_acc: 0.6059113300492611, train_loss: 0.27446294074005484, val_loss: 1.4049768459620735 (77 / 100)\n",
            "train_acc: 0.8949320148331273, val_acc: 0.6059113300492611, train_loss: 0.260730195435959, val_loss: 1.421966187178795 (78 / 100)\n",
            "train_acc: 0.9159456118665018, val_acc: 0.6108374384236454, train_loss: 0.23466148393226346, val_loss: 1.3987187931103071 (79 / 100)\n",
            "train_acc: 0.9085290482076638, val_acc: 0.6009852216748769, train_loss: 0.24342338790557588, val_loss: 1.5780774258040442 (80 / 100)\n",
            "train_acc: 0.9097651421508035, val_acc: 0.5960591133004927, train_loss: 0.2480047591794996, val_loss: 1.5392199079391404 (81 / 100)\n",
            "train_acc: 0.9147095179233622, val_acc: 0.5862068965517241, train_loss: 0.22350144430497668, val_loss: 1.4937385253131097 (82 / 100)\n",
            "train_acc: 0.927070457354759, val_acc: 0.5960591133004927, train_loss: 0.20687665015790607, val_loss: 1.5024953737047506 (83 / 100)\n",
            "train_acc: 0.9295426452410384, val_acc: 0.6157635467980296, train_loss: 0.17436108413084475, val_loss: 1.6208384031145444 (84 / 100)\n",
            "train_acc: 0.9283065512978986, val_acc: 0.625615763546798, train_loss: 0.22057282884129784, val_loss: 1.5294289189606465 (85 / 100)\n",
            "train_acc: 0.9295426452410384, val_acc: 0.5960591133004927, train_loss: 0.20536529855763513, val_loss: 1.690246874182095 (86 / 100)\n",
            "train_acc: 0.9381953028430161, val_acc: 0.6206896551724138, train_loss: 0.17078344019219666, val_loss: 1.6129020320370866 (87 / 100)\n",
            "train_acc: 0.9406674907292955, val_acc: 0.5960591133004927, train_loss: 0.1969515495140414, val_loss: 1.6860187649726868 (88 / 100)\n",
            "train_acc: 0.930778739184178, val_acc: 0.6059113300492611, train_loss: 0.17904302406001593, val_loss: 1.6627946026219522 (89 / 100)\n",
            "train_acc: 0.927070457354759, val_acc: 0.6305418719211823, train_loss: 0.1841702125367924, val_loss: 1.6803060294372107 (90 / 100)\n",
            "train_acc: 0.9295426452410384, val_acc: 0.6305418719211823, train_loss: 0.17105760586497515, val_loss: 1.6454603551643823 (91 / 100)\n",
            "train_acc: 0.9517923362175525, val_acc: 0.6059113300492611, train_loss: 0.1592800717142368, val_loss: 1.7576656071423309 (92 / 100)\n",
            "train_acc: 0.9381953028430161, val_acc: 0.6059113300492611, train_loss: 0.165865289045058, val_loss: 1.8225454373899939 (93 / 100)\n",
            "train_acc: 0.9357231149567367, val_acc: 0.6354679802955665, train_loss: 0.1795586397403987, val_loss: 1.8138018256337771 (94 / 100)\n",
            "train_acc: 0.9480840543881335, val_acc: 0.6157635467980296, train_loss: 0.15449895493096782, val_loss: 1.672611162286674 (95 / 100)\n",
            "train_acc: 0.9493201483312732, val_acc: 0.6157635467980296, train_loss: 0.15337616920802738, val_loss: 1.7379259951596189 (96 / 100)\n",
            "train_acc: 0.9468479604449939, val_acc: 0.6206896551724138, train_loss: 0.1500540520853106, val_loss: 1.7616617876320637 (97 / 100)\n",
            "train_acc: 0.9480840543881335, val_acc: 0.625615763546798, train_loss: 0.13927228041499742, val_loss: 1.7599373932542473 (98 / 100)\n",
            "train_acc: 0.9505562422744128, val_acc: 0.6206896551724138, train_loss: 0.13107198896749941, val_loss: 1.8928195634498972 (99 / 100)\n",
            "train_acc: 0.9505562422744128, val_acc: 0.6354679802955665, train_loss: 0.13742026215135977, val_loss: 1.846150853950989 (100 / 100)\n",
            "({'lr': 0.0005, 'batch_size': 16, 'weight_decay': 1e-05, 'gamma': 0.1}), val accuracy 0.6354679802955665, val loss 1.2678606107904407\n",
            "train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.789575855723123, val_loss: 1.7863755748776966 (1 / 100)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7830148908794294, val_loss: 1.7777192827516002 (2 / 100)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7758423204917106, val_loss: 1.7657733580161785 (3 / 100)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.7664340015394873, val_loss: 1.7543755534834462 (4 / 100)\n",
            "train_acc: 0.19901112484548825, val_acc: 0.18226600985221675, train_loss: 1.7618015440197015, val_loss: 1.7484107733947303 (5 / 100)\n",
            "train_acc: 0.20519159456118666, val_acc: 0.18226600985221675, train_loss: 1.7565816310781188, val_loss: 1.743395360232574 (6 / 100)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18719211822660098, train_loss: 1.7514766161462432, val_loss: 1.7302689787202281 (7 / 100)\n",
            "train_acc: 0.22744128553770088, val_acc: 0.3448275862068966, train_loss: 1.729438145611578, val_loss: 1.7165056208671607 (8 / 100)\n",
            "train_acc: 0.26081582200247216, val_acc: 0.3103448275862069, train_loss: 1.704325728274983, val_loss: 1.6538423599280747 (9 / 100)\n",
            "train_acc: 0.30407911001236093, val_acc: 0.33497536945812806, train_loss: 1.6523603390998982, val_loss: 1.5966727616164484 (10 / 100)\n",
            "train_acc: 0.34239802224969096, val_acc: 0.2955665024630542, train_loss: 1.5998822721768957, val_loss: 1.5318813576486898 (11 / 100)\n",
            "train_acc: 0.3053152039555006, val_acc: 0.2955665024630542, train_loss: 1.6136718979577347, val_loss: 1.5778710906728735 (12 / 100)\n",
            "train_acc: 0.3510506798516687, val_acc: 0.35467980295566504, train_loss: 1.5493165310587371, val_loss: 1.5179059493717888 (13 / 100)\n",
            "train_acc: 0.3572311495673671, val_acc: 0.32019704433497537, train_loss: 1.520216054468426, val_loss: 1.5122115817563286 (14 / 100)\n",
            "train_acc: 0.3535228677379481, val_acc: 0.3103448275862069, train_loss: 1.503285859659664, val_loss: 1.5595973178083673 (15 / 100)\n",
            "train_acc: 0.34857849196538937, val_acc: 0.3103448275862069, train_loss: 1.5102339865103345, val_loss: 1.5086308588535327 (16 / 100)\n",
            "train_acc: 0.377008652657602, val_acc: 0.4088669950738916, train_loss: 1.4743757843234484, val_loss: 1.4465991757773413 (17 / 100)\n",
            "train_acc: 0.3720642768850433, val_acc: 0.4039408866995074, train_loss: 1.5010449489940083, val_loss: 1.4338858568022403 (18 / 100)\n",
            "train_acc: 0.3794808405438813, val_acc: 0.39408866995073893, train_loss: 1.4610646108466998, val_loss: 1.3862347931697452 (19 / 100)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.35467980295566504, train_loss: 1.4188565089469787, val_loss: 1.5072164852630916 (20 / 100)\n",
            "train_acc: 0.380716934487021, val_acc: 0.3645320197044335, train_loss: 1.4254937789348796, val_loss: 1.4126931687293969 (21 / 100)\n",
            "train_acc: 0.3831891223733004, val_acc: 0.45320197044334976, train_loss: 1.4122310781066292, val_loss: 1.3672625108305456 (22 / 100)\n",
            "train_acc: 0.415327564894932, val_acc: 0.4039408866995074, train_loss: 1.389946107221917, val_loss: 1.3374344004786074 (23 / 100)\n",
            "train_acc: 0.4004944375772559, val_acc: 0.4088669950738916, train_loss: 1.3772097485616563, val_loss: 1.3576608896255493 (24 / 100)\n",
            "train_acc: 0.42398022249690975, val_acc: 0.4236453201970443, train_loss: 1.385506671644996, val_loss: 1.3425419512640666 (25 / 100)\n",
            "train_acc: 0.4054388133498146, val_acc: 0.4236453201970443, train_loss: 1.36810480472626, val_loss: 1.3575198192314561 (26 / 100)\n",
            "train_acc: 0.4276885043263288, val_acc: 0.4482758620689655, train_loss: 1.3438532390759519, val_loss: 1.3338259191348636 (27 / 100)\n",
            "train_acc: 0.41656365883807167, val_acc: 0.49261083743842365, train_loss: 1.3431519409929424, val_loss: 1.2938078958999935 (28 / 100)\n",
            "train_acc: 0.44746600741656367, val_acc: 0.3793103448275862, train_loss: 1.3265417868775697, val_loss: 1.3908077190662254 (29 / 100)\n",
            "train_acc: 0.42398022249690975, val_acc: 0.458128078817734, train_loss: 1.3269846508617895, val_loss: 1.3113992475523737 (30 / 100)\n",
            "train_acc: 0.44870210135970334, val_acc: 0.5123152709359606, train_loss: 1.3102759336511638, val_loss: 1.298353403659877 (31 / 100)\n",
            "train_acc: 0.4746600741656366, val_acc: 0.4827586206896552, train_loss: 1.2866363702215282, val_loss: 1.2468543915913022 (32 / 100)\n",
            "train_acc: 0.45859085290482077, val_acc: 0.46798029556650245, train_loss: 1.291524701640102, val_loss: 1.252133670698833 (33 / 100)\n",
            "train_acc: 0.43757725587144625, val_acc: 0.4876847290640394, train_loss: 1.3269593889545304, val_loss: 1.239396119352632 (34 / 100)\n",
            "train_acc: 0.4932014833127318, val_acc: 0.5024630541871922, train_loss: 1.250757382443573, val_loss: 1.2225021540825003 (35 / 100)\n",
            "train_acc: 0.4684796044499382, val_acc: 0.5024630541871922, train_loss: 1.2460908338666994, val_loss: 1.17262791001738 (36 / 100)\n",
            "train_acc: 0.5129789864029666, val_acc: 0.4482758620689655, train_loss: 1.2254703156143538, val_loss: 1.2555073053378778 (37 / 100)\n",
            "train_acc: 0.5030902348578492, val_acc: 0.5172413793103449, train_loss: 1.2198702071888927, val_loss: 1.1784195961623356 (38 / 100)\n",
            "train_acc: 0.4796044499381953, val_acc: 0.541871921182266, train_loss: 1.2180566089262332, val_loss: 1.140502173325111 (39 / 100)\n",
            "train_acc: 0.5216316440049443, val_acc: 0.5073891625615764, train_loss: 1.1797583646915752, val_loss: 1.1422660509353788 (40 / 100)\n",
            "train_acc: 0.5067985166872683, val_acc: 0.5172413793103449, train_loss: 1.1553284004975308, val_loss: 1.1726168141576456 (41 / 100)\n",
            "train_acc: 0.5265760197775031, val_acc: 0.4876847290640394, train_loss: 1.1137269399369456, val_loss: 1.168400043337216 (42 / 100)\n",
            "train_acc: 0.5290482076637825, val_acc: 0.5320197044334976, train_loss: 1.1468439690114836, val_loss: 1.131948953778873 (43 / 100)\n",
            "train_acc: 0.5166872682323856, val_acc: 0.5024630541871922, train_loss: 1.1311975545140516, val_loss: 1.1599651492875198 (44 / 100)\n",
            "train_acc: 0.5500618046971569, val_acc: 0.46798029556650245, train_loss: 1.1000684552787998, val_loss: 1.3247084852509898 (45 / 100)\n",
            "train_acc: 0.5302843016069221, val_acc: 0.5024630541871922, train_loss: 1.1096454625960777, val_loss: 1.1404862521317205 (46 / 100)\n",
            "train_acc: 0.584672435105068, val_acc: 0.5073891625615764, train_loss: 1.0622549552115876, val_loss: 1.2330677356626012 (47 / 100)\n",
            "train_acc: 0.5710754017305315, val_acc: 0.5172413793103449, train_loss: 1.0734078587943456, val_loss: 1.1611638967626787 (48 / 100)\n",
            "train_acc: 0.5970333745364648, val_acc: 0.5369458128078818, train_loss: 1.027104963772966, val_loss: 1.1278773337749426 (49 / 100)\n",
            "train_acc: 0.6106304079110012, val_acc: 0.5566502463054187, train_loss: 0.975310797154977, val_loss: 1.077500952875673 (50 / 100)\n",
            "train_acc: 0.61557478368356, val_acc: 0.5665024630541872, train_loss: 0.9977686358028643, val_loss: 1.1091162967564436 (51 / 100)\n",
            "train_acc: 0.619283065512979, val_acc: 0.541871921182266, train_loss: 0.97513398288649, val_loss: 1.1566370925292593 (52 / 100)\n",
            "train_acc: 0.6254635352286774, val_acc: 0.5566502463054187, train_loss: 0.9567216978821678, val_loss: 1.0337940378142108 (53 / 100)\n",
            "train_acc: 0.657601977750309, val_acc: 0.5862068965517241, train_loss: 0.890862897609456, val_loss: 1.1782524653256232 (54 / 100)\n",
            "train_acc: 0.6588380716934487, val_acc: 0.6206896551724138, train_loss: 0.9054778444457555, val_loss: 1.0397945683577965 (55 / 100)\n",
            "train_acc: 0.6662546353522868, val_acc: 0.5862068965517241, train_loss: 0.84894519653839, val_loss: 1.0239554153287351 (56 / 100)\n",
            "train_acc: 0.7132262051915945, val_acc: 0.5566502463054187, train_loss: 0.7779270322862161, val_loss: 1.0968305923668622 (57 / 100)\n",
            "train_acc: 0.6835599505562423, val_acc: 0.5615763546798029, train_loss: 0.7961198938055003, val_loss: 1.0819378286746923 (58 / 100)\n",
            "train_acc: 0.7132262051915945, val_acc: 0.6059113300492611, train_loss: 0.7576521993862242, val_loss: 1.0149756675870547 (59 / 100)\n",
            "train_acc: 0.7428924598269468, val_acc: 0.5911330049261084, train_loss: 0.6763121847463921, val_loss: 1.0511740998094306 (60 / 100)\n",
            "train_acc: 0.7775030902348579, val_acc: 0.6206896551724138, train_loss: 0.5852553992542554, val_loss: 1.0427535610833192 (61 / 100)\n",
            "train_acc: 0.8182941903584673, val_acc: 0.6206896551724138, train_loss: 0.46920297452339577, val_loss: 1.041386896459927 (62 / 100)\n",
            "train_acc: 0.8244746600741656, val_acc: 0.6305418719211823, train_loss: 0.46754158257848694, val_loss: 1.0386985135195879 (63 / 100)\n",
            "train_acc: 0.8491965389369592, val_acc: 0.6305418719211823, train_loss: 0.4791653483406133, val_loss: 1.048466730881207 (64 / 100)\n",
            "train_acc: 0.8244746600741656, val_acc: 0.6305418719211823, train_loss: 0.4466899086724695, val_loss: 1.0657895430555484 (65 / 100)\n",
            "train_acc: 0.8430160692212608, val_acc: 0.6108374384236454, train_loss: 0.4305742257752436, val_loss: 1.0893711606269987 (66 / 100)\n",
            "train_acc: 0.8442521631644005, val_acc: 0.6157635467980296, train_loss: 0.44785939906671995, val_loss: 1.1140025275681407 (67 / 100)\n",
            "train_acc: 0.861557478368356, val_acc: 0.625615763546798, train_loss: 0.39017212505098914, val_loss: 1.117974471869727 (68 / 100)\n",
            "train_acc: 0.8640296662546354, val_acc: 0.6157635467980296, train_loss: 0.3831923614694692, val_loss: 1.125124629495179 (69 / 100)\n",
            "train_acc: 0.8541409147095179, val_acc: 0.645320197044335, train_loss: 0.3918643675569551, val_loss: 1.1126060973247285 (70 / 100)\n",
            "train_acc: 0.8788627935723115, val_acc: 0.6206896551724138, train_loss: 0.3704494236426241, val_loss: 1.124507549361055 (71 / 100)\n",
            "train_acc: 0.8491965389369592, val_acc: 0.6157635467980296, train_loss: 0.37659663993437004, val_loss: 1.12116534222523 (72 / 100)\n",
            "train_acc: 0.8776266996291718, val_acc: 0.6354679802955665, train_loss: 0.3483384258225468, val_loss: 1.142818760108478 (73 / 100)\n",
            "train_acc: 0.8788627935723115, val_acc: 0.6157635467980296, train_loss: 0.3206241101358081, val_loss: 1.2048840158678629 (74 / 100)\n",
            "train_acc: 0.8714462299134734, val_acc: 0.6354679802955665, train_loss: 0.33039566961074496, val_loss: 1.2120699218928521 (75 / 100)\n",
            "train_acc: 0.8800988875154512, val_acc: 0.625615763546798, train_loss: 0.35065066475969164, val_loss: 1.1751325905616647 (76 / 100)\n",
            "train_acc: 0.8912237330037083, val_acc: 0.6305418719211823, train_loss: 0.30376007567230057, val_loss: 1.192779784132107 (77 / 100)\n",
            "train_acc: 0.8838071693448702, val_acc: 0.625615763546798, train_loss: 0.31017835745852723, val_loss: 1.1959079915079578 (78 / 100)\n",
            "train_acc: 0.8887515451174289, val_acc: 0.6059113300492611, train_loss: 0.33991507907733043, val_loss: 1.1895771160207946 (79 / 100)\n",
            "train_acc: 0.8677379480840544, val_acc: 0.6305418719211823, train_loss: 0.33122406631008217, val_loss: 1.2201595649930643 (80 / 100)\n",
            "train_acc: 0.9035846724351051, val_acc: 0.6108374384236454, train_loss: 0.2815208190082767, val_loss: 1.277238632070607 (81 / 100)\n",
            "train_acc: 0.8751545117428925, val_acc: 0.6108374384236454, train_loss: 0.32906102658939007, val_loss: 1.2530471735399933 (82 / 100)\n",
            "train_acc: 0.8776266996291718, val_acc: 0.6108374384236454, train_loss: 0.3083384231524945, val_loss: 1.2991282094288341 (83 / 100)\n",
            "train_acc: 0.8912237330037083, val_acc: 0.6305418719211823, train_loss: 0.30885015838815194, val_loss: 1.2462493830126495 (84 / 100)\n",
            "train_acc: 0.9122373300370828, val_acc: 0.6206896551724138, train_loss: 0.2699255311518576, val_loss: 1.2692280300145078 (85 / 100)\n",
            "train_acc: 0.8899876390605687, val_acc: 0.625615763546798, train_loss: 0.3024872248009197, val_loss: 1.2662653509031963 (86 / 100)\n",
            "train_acc: 0.9060568603213844, val_acc: 0.6009852216748769, train_loss: 0.26714869773726824, val_loss: 1.250855379210317 (87 / 100)\n",
            "train_acc: 0.9134734239802225, val_acc: 0.6157635467980296, train_loss: 0.24084221121333585, val_loss: 1.3069189710570086 (88 / 100)\n",
            "train_acc: 0.9147095179233622, val_acc: 0.6157635467980296, train_loss: 0.23049256194507237, val_loss: 1.3399947103608418 (89 / 100)\n",
            "train_acc: 0.907292954264524, val_acc: 0.6354679802955665, train_loss: 0.2580780697196759, val_loss: 1.375244399009667 (90 / 100)\n",
            "train_acc: 0.9122373300370828, val_acc: 0.6108374384236454, train_loss: 0.25486493623632434, val_loss: 1.3348052713084104 (91 / 100)\n",
            "train_acc: 0.9221260815822002, val_acc: 0.625615763546798, train_loss: 0.22797150152870102, val_loss: 1.3578178715236082 (92 / 100)\n",
            "train_acc: 0.9035846724351051, val_acc: 0.6157635467980296, train_loss: 0.2512942733443417, val_loss: 1.368195419534674 (93 / 100)\n",
            "train_acc: 0.9184177997527813, val_acc: 0.6305418719211823, train_loss: 0.21454428057146013, val_loss: 1.3842737577818884 (94 / 100)\n",
            "train_acc: 0.9085290482076638, val_acc: 0.6354679802955665, train_loss: 0.24378241162070238, val_loss: 1.4029077655576132 (95 / 100)\n",
            "train_acc: 0.9245982694684796, val_acc: 0.6305418719211823, train_loss: 0.21526200942262289, val_loss: 1.355099808993598 (96 / 100)\n",
            "train_acc: 0.9394313967861557, val_acc: 0.6108374384236454, train_loss: 0.18855613759259507, val_loss: 1.443725131709 (97 / 100)\n",
            "train_acc: 0.92336217552534, val_acc: 0.6206896551724138, train_loss: 0.2144348170465533, val_loss: 1.437174994370033 (98 / 100)\n",
            "train_acc: 0.9283065512978986, val_acc: 0.645320197044335, train_loss: 0.20770391567675822, val_loss: 1.3892739779256247 (99 / 100)\n",
            "train_acc: 0.934487021013597, val_acc: 0.645320197044335, train_loss: 0.2053926445276687, val_loss: 1.4028686760681603 (100 / 100)\n",
            "({'lr': 0.0005, 'batch_size': 16, 'weight_decay': 0.001, 'gamma': 0.05}), val accuracy 0.645320197044335, val loss 1.1126060973247285\n",
            "train_acc: 0.17428924598269468, val_acc: 0.18226600985221675, train_loss: 1.790036370344598, val_loss: 1.783659152796703 (1 / 100)\n",
            "train_acc: 0.2027194066749073, val_acc: 0.21182266009852216, train_loss: 1.7808054184589457, val_loss: 1.772779097110767 (2 / 100)\n",
            "train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7701850027324832, val_loss: 1.758118505548374 (3 / 100)\n",
            "train_acc: 0.20642768850432633, val_acc: 0.18226600985221675, train_loss: 1.7643645506706755, val_loss: 1.751966938596641 (4 / 100)\n",
            "train_acc: 0.20642768850432633, val_acc: 0.18226600985221675, train_loss: 1.761258115285112, val_loss: 1.7459762619046741 (5 / 100)\n",
            "train_acc: 0.23980222496909764, val_acc: 0.24630541871921183, train_loss: 1.7489700946583884, val_loss: 1.7312449745356744 (6 / 100)\n",
            "train_acc: 0.207663782447466, val_acc: 0.23645320197044334, train_loss: 1.735748231484657, val_loss: 1.7153491709619908 (7 / 100)\n",
            "train_acc: 0.26452410383189123, val_acc: 0.32019704433497537, train_loss: 1.7158710501249994, val_loss: 1.6688362735832853 (8 / 100)\n",
            "train_acc: 0.2843016069221261, val_acc: 0.3399014778325123, train_loss: 1.6656891271122012, val_loss: 1.5538220411450991 (9 / 100)\n",
            "train_acc: 0.2978986402966625, val_acc: 0.3399014778325123, train_loss: 1.6318226218665632, val_loss: 1.5650260037389294 (10 / 100)\n",
            "train_acc: 0.3300370828182942, val_acc: 0.33497536945812806, train_loss: 1.5807168161029133, val_loss: 1.5135651128045444 (11 / 100)\n",
            "train_acc: 0.3473423980222497, val_acc: 0.3842364532019704, train_loss: 1.5446540086319478, val_loss: 1.4849861390484964 (12 / 100)\n",
            "train_acc: 0.34981458590852904, val_acc: 0.3793103448275862, train_loss: 1.5513836101194838, val_loss: 1.4612098820691037 (13 / 100)\n",
            "train_acc: 0.3621755253399258, val_acc: 0.33497536945812806, train_loss: 1.502500392008476, val_loss: 1.4806333386839317 (14 / 100)\n",
            "train_acc: 0.38442521631644005, val_acc: 0.3251231527093596, train_loss: 1.4884502214318596, val_loss: 1.605460285553204 (15 / 100)\n",
            "train_acc: 0.3794808405438813, val_acc: 0.3891625615763547, train_loss: 1.4817977997811675, val_loss: 1.415337714655646 (16 / 100)\n",
            "train_acc: 0.3794808405438813, val_acc: 0.35960591133004927, train_loss: 1.4398403979644492, val_loss: 1.4682486967500208 (17 / 100)\n",
            "train_acc: 0.40296662546353523, val_acc: 0.35960591133004927, train_loss: 1.4315319224842105, val_loss: 1.3602283987505683 (18 / 100)\n",
            "train_acc: 0.39060568603213847, val_acc: 0.39901477832512317, train_loss: 1.4391407201847128, val_loss: 1.3795887501956208 (19 / 100)\n",
            "train_acc: 0.415327564894932, val_acc: 0.39901477832512317, train_loss: 1.3807057518010086, val_loss: 1.4266910946427895 (20 / 100)\n",
            "train_acc: 0.40667490729295425, val_acc: 0.41379310344827586, train_loss: 1.37841539816155, val_loss: 1.367876975407154 (21 / 100)\n",
            "train_acc: 0.415327564894932, val_acc: 0.37438423645320196, train_loss: 1.3749969752668891, val_loss: 1.4106353674028895 (22 / 100)\n",
            "train_acc: 0.42398022249690975, val_acc: 0.45320197044334976, train_loss: 1.3752392014702701, val_loss: 1.309876381470065 (23 / 100)\n",
            "train_acc: 0.4276885043263288, val_acc: 0.43842364532019706, train_loss: 1.3851672820608754, val_loss: 1.343464712204017 (24 / 100)\n",
            "train_acc: 0.43757725587144625, val_acc: 0.43842364532019706, train_loss: 1.33687121847504, val_loss: 1.3158688075436746 (25 / 100)\n",
            "train_acc: 0.4511742892459827, val_acc: 0.4975369458128079, train_loss: 1.3238935892013153, val_loss: 1.3025257540453832 (26 / 100)\n",
            "train_acc: 0.4672435105067985, val_acc: 0.4236453201970443, train_loss: 1.2989814294725472, val_loss: 1.3567772757243641 (27 / 100)\n",
            "train_acc: 0.4561186650185414, val_acc: 0.46798029556650245, train_loss: 1.3020845188935402, val_loss: 1.267742862842353 (28 / 100)\n",
            "train_acc: 0.4338689740420272, val_acc: 0.49261083743842365, train_loss: 1.291667332607972, val_loss: 1.254759975842067 (29 / 100)\n",
            "train_acc: 0.47095179233621753, val_acc: 0.4729064039408867, train_loss: 1.2562577108517565, val_loss: 1.2339682103377845 (30 / 100)\n",
            "train_acc: 0.46971569839307786, val_acc: 0.4729064039408867, train_loss: 1.3014311878878637, val_loss: 1.4110103974788648 (31 / 100)\n",
            "train_acc: 0.48702101359703337, val_acc: 0.4433497536945813, train_loss: 1.2554876777828108, val_loss: 1.268451984292768 (32 / 100)\n",
            "train_acc: 0.48084054388133496, val_acc: 0.49261083743842365, train_loss: 1.2641153731953227, val_loss: 1.1994943107877458 (33 / 100)\n",
            "train_acc: 0.4746600741656366, val_acc: 0.4630541871921182, train_loss: 1.241727095157166, val_loss: 1.2431951347243022 (34 / 100)\n",
            "train_acc: 0.4956736711990111, val_acc: 0.4039408866995074, train_loss: 1.23685399976148, val_loss: 1.2677123869581175 (35 / 100)\n",
            "train_acc: 0.519159456118665, val_acc: 0.45320197044334976, train_loss: 1.1704028622329015, val_loss: 1.2650268066105583 (36 / 100)\n",
            "train_acc: 0.519159456118665, val_acc: 0.47783251231527096, train_loss: 1.1797258721293862, val_loss: 1.2453192977482461 (37 / 100)\n",
            "train_acc: 0.5092707045735476, val_acc: 0.4876847290640394, train_loss: 1.1525350617537244, val_loss: 1.1201293606476244 (38 / 100)\n",
            "train_acc: 0.5611866501854141, val_acc: 0.5369458128078818, train_loss: 1.1162945485969704, val_loss: 1.144227633922558 (39 / 100)\n",
            "train_acc: 0.5784919653893696, val_acc: 0.5073891625615764, train_loss: 1.0787605287412778, val_loss: 1.1213501691818237 (40 / 100)\n",
            "train_acc: 0.5686032138442522, val_acc: 0.5221674876847291, train_loss: 1.0537998070675598, val_loss: 1.1751152033289078 (41 / 100)\n",
            "train_acc: 0.5611866501854141, val_acc: 0.5566502463054187, train_loss: 1.107048320902882, val_loss: 1.086316958730444 (42 / 100)\n",
            "train_acc: 0.5686032138442522, val_acc: 0.5024630541871922, train_loss: 1.0521436676990854, val_loss: 1.1908996774645275 (43 / 100)\n",
            "train_acc: 0.5784919653893696, val_acc: 0.5024630541871922, train_loss: 1.03633896396835, val_loss: 1.1980781966242298 (44 / 100)\n",
            "train_acc: 0.6093943139678616, val_acc: 0.5172413793103449, train_loss: 0.9990160457136015, val_loss: 1.1491580720018284 (45 / 100)\n",
            "train_acc: 0.6341161928306551, val_acc: 0.5270935960591133, train_loss: 0.9399209090304758, val_loss: 1.1792124251426734 (46 / 100)\n",
            "train_acc: 0.6316440049443758, val_acc: 0.5665024630541872, train_loss: 0.9591627025191657, val_loss: 1.0701323797550109 (47 / 100)\n",
            "train_acc: 0.6637824474660075, val_acc: 0.4975369458128079, train_loss: 0.8974430769571415, val_loss: 1.1691559162633172 (48 / 100)\n",
            "train_acc: 0.6341161928306551, val_acc: 0.49261083743842365, train_loss: 0.9087460892014804, val_loss: 1.173809425584201 (49 / 100)\n",
            "train_acc: 0.6415327564894932, val_acc: 0.49261083743842365, train_loss: 0.9051232978941042, val_loss: 1.2516306606419567 (50 / 100)\n",
            "train_acc: 0.6934487021013597, val_acc: 0.5862068965517241, train_loss: 0.8228797867654722, val_loss: 1.0722250042877761 (51 / 100)\n",
            "train_acc: 0.6724351050679852, val_acc: 0.6157635467980296, train_loss: 0.8299549008329365, val_loss: 1.1391889394210477 (52 / 100)\n",
            "train_acc: 0.7070457354758962, val_acc: 0.5763546798029556, train_loss: 0.7671191246606806, val_loss: 1.231499210073443 (53 / 100)\n",
            "train_acc: 0.7021013597033374, val_acc: 0.5665024630541872, train_loss: 0.7780830299456423, val_loss: 1.14063037263936 (54 / 100)\n",
            "train_acc: 0.7243510506798516, val_acc: 0.541871921182266, train_loss: 0.7348408000578249, val_loss: 1.3710498933134407 (55 / 100)\n",
            "train_acc: 0.7354758961681088, val_acc: 0.5812807881773399, train_loss: 0.7000017825177927, val_loss: 1.0882153111725605 (56 / 100)\n",
            "train_acc: 0.7194066749072929, val_acc: 0.5812807881773399, train_loss: 0.7181576513242073, val_loss: 1.0382346841208454 (57 / 100)\n",
            "train_acc: 0.7280593325092707, val_acc: 0.5862068965517241, train_loss: 0.6853815878572217, val_loss: 1.23256132344307 (58 / 100)\n",
            "train_acc: 0.7713226205191595, val_acc: 0.5073891625615764, train_loss: 0.6064157986965109, val_loss: 1.5712362903679533 (59 / 100)\n",
            "train_acc: 0.7787391841779975, val_acc: 0.5812807881773399, train_loss: 0.607849069704084, val_loss: 1.370203415748521 (60 / 100)\n",
            "train_acc: 0.8541409147095179, val_acc: 0.6059113300492611, train_loss: 0.3943688147810687, val_loss: 1.107720550645161 (61 / 100)\n",
            "train_acc: 0.8689740420271941, val_acc: 0.645320197044335, train_loss: 0.3362807743849949, val_loss: 1.2011404187221246 (62 / 100)\n",
            "train_acc: 0.8850432632880099, val_acc: 0.6157635467980296, train_loss: 0.3102692358278668, val_loss: 1.2312219944493523 (63 / 100)\n",
            "train_acc: 0.8936959208899876, val_acc: 0.6157635467980296, train_loss: 0.26703268444287614, val_loss: 1.2273976864485905 (64 / 100)\n",
            "train_acc: 0.9147095179233622, val_acc: 0.6206896551724138, train_loss: 0.23459388254894167, val_loss: 1.309953149609965 (65 / 100)\n",
            "train_acc: 0.9060568603213844, val_acc: 0.625615763546798, train_loss: 0.24370424694419643, val_loss: 1.297863312836351 (66 / 100)\n",
            "train_acc: 0.9122373300370828, val_acc: 0.6059113300492611, train_loss: 0.2253769216932384, val_loss: 1.2960992368864896 (67 / 100)\n",
            "train_acc: 0.9184177997527813, val_acc: 0.6157635467980296, train_loss: 0.2158598499038753, val_loss: 1.4464360739797206 (68 / 100)\n",
            "train_acc: 0.9295426452410384, val_acc: 0.625615763546798, train_loss: 0.19553997151194455, val_loss: 1.3698900042496291 (69 / 100)\n",
            "train_acc: 0.9332509270704573, val_acc: 0.625615763546798, train_loss: 0.21250759000683891, val_loss: 1.3933430406553993 (70 / 100)\n",
            "train_acc: 0.9456118665018541, val_acc: 0.6403940886699507, train_loss: 0.16224811961977975, val_loss: 1.48300426934153 (71 / 100)\n",
            "train_acc: 0.9320148331273177, val_acc: 0.6206896551724138, train_loss: 0.1803308997869639, val_loss: 1.4837565755315603 (72 / 100)\n",
            "train_acc: 0.9493201483312732, val_acc: 0.6354679802955665, train_loss: 0.16793237484724471, val_loss: 1.5631447965875636 (73 / 100)\n",
            "train_acc: 0.930778739184178, val_acc: 0.6108374384236454, train_loss: 0.17246076026965942, val_loss: 1.5106852326193467 (74 / 100)\n",
            "train_acc: 0.9283065512978986, val_acc: 0.625615763546798, train_loss: 0.18038434932199485, val_loss: 1.6161598994814117 (75 / 100)\n",
            "train_acc: 0.9468479604449939, val_acc: 0.6403940886699507, train_loss: 0.15550983906234297, val_loss: 1.729526839820035 (76 / 100)\n",
            "train_acc: 0.9357231149567367, val_acc: 0.6403940886699507, train_loss: 0.17165772111255248, val_loss: 1.591974268552705 (77 / 100)\n",
            "train_acc: 0.9245982694684796, val_acc: 0.6403940886699507, train_loss: 0.1911587460295784, val_loss: 1.6096720616218492 (78 / 100)\n",
            "train_acc: 0.9406674907292955, val_acc: 0.6305418719211823, train_loss: 0.16392651891936771, val_loss: 1.5240792366377827 (79 / 100)\n",
            "train_acc: 0.9419035846724351, val_acc: 0.6354679802955665, train_loss: 0.14309879904440218, val_loss: 1.6225027016230993 (80 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.6305418719211823, train_loss: 0.10977120412000177, val_loss: 1.7124007720078154 (81 / 100)\n",
            "train_acc: 0.9456118665018541, val_acc: 0.625615763546798, train_loss: 0.1518749112150504, val_loss: 1.7082997181732666 (82 / 100)\n",
            "train_acc: 0.9555006180469716, val_acc: 0.6009852216748769, train_loss: 0.13691466475741382, val_loss: 1.6626794399886295 (83 / 100)\n",
            "train_acc: 0.9604449938195303, val_acc: 0.6305418719211823, train_loss: 0.12224676081051933, val_loss: 1.6691927613295945 (84 / 100)\n",
            "train_acc: 0.9555006180469716, val_acc: 0.6502463054187192, train_loss: 0.12575173967406245, val_loss: 1.6566329690916786 (85 / 100)\n",
            "train_acc: 0.9505562422744128, val_acc: 0.6305418719211823, train_loss: 0.1102468239509573, val_loss: 1.7386083806970436 (86 / 100)\n",
            "train_acc: 0.9517923362175525, val_acc: 0.6305418719211823, train_loss: 0.11482894876960328, val_loss: 1.835993166627555 (87 / 100)\n",
            "train_acc: 0.9604449938195303, val_acc: 0.6502463054187192, train_loss: 0.11361911344918686, val_loss: 1.7083652097309752 (88 / 100)\n",
            "train_acc: 0.9517923362175525, val_acc: 0.6206896551724138, train_loss: 0.1334752171531743, val_loss: 1.8429296204609236 (89 / 100)\n",
            "train_acc: 0.9530284301606922, val_acc: 0.6403940886699507, train_loss: 0.12426507251975512, val_loss: 1.7754663261286732 (90 / 100)\n",
            "train_acc: 0.9517923362175525, val_acc: 0.6305418719211823, train_loss: 0.11444958090368884, val_loss: 1.8175491373527226 (91 / 100)\n",
            "train_acc: 0.9567367119901112, val_acc: 0.6206896551724138, train_loss: 0.12099441521362446, val_loss: 1.8335086430234862 (92 / 100)\n",
            "train_acc: 0.9555006180469716, val_acc: 0.625615763546798, train_loss: 0.10772527251346554, val_loss: 1.961489537079346 (93 / 100)\n",
            "train_acc: 0.9567367119901112, val_acc: 0.6354679802955665, train_loss: 0.10363551532234927, val_loss: 1.914489648318643 (94 / 100)\n",
            "train_acc: 0.9555006180469716, val_acc: 0.6305418719211823, train_loss: 0.1254790153340592, val_loss: 1.8282695952894652 (95 / 100)\n",
            "train_acc: 0.9616810877626699, val_acc: 0.6502463054187192, train_loss: 0.09640213968842411, val_loss: 1.9138829995845925 (96 / 100)\n",
            "train_acc: 0.9567367119901112, val_acc: 0.625615763546798, train_loss: 0.09542724994895922, val_loss: 1.907483242709061 (97 / 100)\n",
            "train_acc: 0.9517923362175525, val_acc: 0.6403940886699507, train_loss: 0.11935730722395539, val_loss: 1.9492870281482566 (98 / 100)\n",
            "train_acc: 0.9542645241038319, val_acc: 0.6305418719211823, train_loss: 0.1268505957092726, val_loss: 1.7848305452633373 (99 / 100)\n",
            "train_acc: 0.9641532756489494, val_acc: 0.645320197044335, train_loss: 0.10740260405832043, val_loss: 1.9490951866351913 (100 / 100)\n",
            "({'lr': 0.0005, 'batch_size': 16, 'weight_decay': 0.001, 'gamma': 0.1}), val accuracy 0.6502463054187192, val loss 1.6566329690916786\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.782881315323272, val_loss: 1.7616474734151304 (1 / 100)\n",
            "train_acc: 0.20024721878862795, val_acc: 0.18226600985221675, train_loss: 1.7627461886376474, val_loss: 1.749805768135146 (2 / 100)\n",
            "train_acc: 0.23485784919653893, val_acc: 0.2660098522167488, train_loss: 1.7530899529698754, val_loss: 1.7153229155563956 (3 / 100)\n",
            "train_acc: 0.2249690976514215, val_acc: 0.18719211822660098, train_loss: 1.741901600464018, val_loss: 1.7552979027696431 (4 / 100)\n",
            "train_acc: 0.20519159456118666, val_acc: 0.18226600985221675, train_loss: 1.7712897629614077, val_loss: 1.7523370094487232 (5 / 100)\n",
            "train_acc: 0.22620519159456118, val_acc: 0.22660098522167488, train_loss: 1.7425021748018206, val_loss: 1.7039266219867275 (6 / 100)\n",
            "train_acc: 0.27070457354758964, val_acc: 0.2561576354679803, train_loss: 1.6740298377273994, val_loss: 1.7125173836506058 (7 / 100)\n",
            "train_acc: 0.32014833127317677, val_acc: 0.3694581280788177, train_loss: 1.5839541919151843, val_loss: 1.4635233379937158 (8 / 100)\n",
            "train_acc: 0.34363411619283063, val_acc: 0.3694581280788177, train_loss: 1.5110865467411037, val_loss: 1.5641622772357735 (9 / 100)\n",
            "train_acc: 0.34487021013597036, val_acc: 0.35960591133004927, train_loss: 1.5464277500275185, val_loss: 1.4020688504421066 (10 / 100)\n",
            "train_acc: 0.3621755253399258, val_acc: 0.3793103448275862, train_loss: 1.4956206420738116, val_loss: 1.387225291411865 (11 / 100)\n",
            "train_acc: 0.37082818294190356, val_acc: 0.43842364532019706, train_loss: 1.4513616491159786, val_loss: 1.3531718612304462 (12 / 100)\n",
            "train_acc: 0.40296662546353523, val_acc: 0.4236453201970443, train_loss: 1.4125532443797486, val_loss: 1.321752969267333 (13 / 100)\n",
            "train_acc: 0.415327564894932, val_acc: 0.4630541871921182, train_loss: 1.3817537638401367, val_loss: 1.3545908114593017 (14 / 100)\n",
            "train_acc: 0.415327564894932, val_acc: 0.4088669950738916, train_loss: 1.3726256795513025, val_loss: 1.3470290828808187 (15 / 100)\n",
            "train_acc: 0.4227441285537701, val_acc: 0.3793103448275862, train_loss: 1.3760291230398587, val_loss: 1.348023684741241 (16 / 100)\n",
            "train_acc: 0.453646477132262, val_acc: 0.4039408866995074, train_loss: 1.3056324207591186, val_loss: 1.406610155340486 (17 / 100)\n",
            "train_acc: 0.44499381953028433, val_acc: 0.4729064039408867, train_loss: 1.352841689648528, val_loss: 1.2371691917550975 (18 / 100)\n",
            "train_acc: 0.43139678615574784, val_acc: 0.4482758620689655, train_loss: 1.3176255768397536, val_loss: 1.2787332981090827 (19 / 100)\n",
            "train_acc: 0.449938195302843, val_acc: 0.4039408866995074, train_loss: 1.2981361516179202, val_loss: 1.3744321929410173 (20 / 100)\n",
            "train_acc: 0.4820766378244747, val_acc: 0.458128078817734, train_loss: 1.2575165205744494, val_loss: 1.2252734664625722 (21 / 100)\n",
            "train_acc: 0.5067985166872683, val_acc: 0.458128078817734, train_loss: 1.188004083185467, val_loss: 1.1978967316044962 (22 / 100)\n",
            "train_acc: 0.5105067985166872, val_acc: 0.4975369458128079, train_loss: 1.1608989210741776, val_loss: 1.1408153889801702 (23 / 100)\n",
            "train_acc: 0.5055624227441285, val_acc: 0.5221674876847291, train_loss: 1.181423401508402, val_loss: 1.2111586802111471 (24 / 100)\n",
            "train_acc: 0.5253399258343634, val_acc: 0.5073891625615764, train_loss: 1.1277038662041663, val_loss: 1.1704561322780664 (25 / 100)\n",
            "train_acc: 0.5389369592088998, val_acc: 0.4827586206896552, train_loss: 1.0812205649424247, val_loss: 1.261010960405096 (26 / 100)\n",
            "train_acc: 0.5203955500618047, val_acc: 0.46798029556650245, train_loss: 1.1001774863937435, val_loss: 1.2102816416124993 (27 / 100)\n",
            "train_acc: 0.5587144622991347, val_acc: 0.5467980295566502, train_loss: 1.0469135852030977, val_loss: 1.1183714314634576 (28 / 100)\n",
            "train_acc: 0.6131025957972805, val_acc: 0.5320197044334976, train_loss: 0.9512963533696197, val_loss: 1.383552074432373 (29 / 100)\n",
            "train_acc: 0.6118665018541409, val_acc: 0.541871921182266, train_loss: 0.9718651890902054, val_loss: 1.1133185754268629 (30 / 100)\n",
            "train_acc: 0.6106304079110012, val_acc: 0.458128078817734, train_loss: 0.9541620833912061, val_loss: 1.2096222445295362 (31 / 100)\n",
            "train_acc: 0.6032138442521632, val_acc: 0.5221674876847291, train_loss: 0.9275260779117919, val_loss: 1.2025150295548839 (32 / 100)\n",
            "train_acc: 0.6477132262051916, val_acc: 0.5665024630541872, train_loss: 0.841917237186314, val_loss: 1.2161457115793464 (33 / 100)\n",
            "train_acc: 0.6613102595797281, val_acc: 0.5270935960591133, train_loss: 0.8311119943083585, val_loss: 1.1437478682090496 (34 / 100)\n",
            "train_acc: 0.6909765142150803, val_acc: 0.5812807881773399, train_loss: 0.7819828609895647, val_loss: 1.18491270095844 (35 / 100)\n",
            "train_acc: 0.7119901112484549, val_acc: 0.5812807881773399, train_loss: 0.7068677395029622, val_loss: 1.3472842148372106 (36 / 100)\n",
            "train_acc: 0.7330037082818294, val_acc: 0.5714285714285714, train_loss: 0.6721510680850563, val_loss: 1.493814093138784 (37 / 100)\n",
            "train_acc: 0.7478368355995055, val_acc: 0.541871921182266, train_loss: 0.6418719461143975, val_loss: 1.4240675201556954 (38 / 100)\n",
            "train_acc: 0.6563658838071693, val_acc: 0.6009852216748769, train_loss: 0.8965257515276613, val_loss: 1.317479782210195 (39 / 100)\n",
            "train_acc: 0.7688504326328801, val_acc: 0.6009852216748769, train_loss: 0.591836364513864, val_loss: 1.1379343829131479 (40 / 100)\n",
            "train_acc: 0.7911001236093943, val_acc: 0.6009852216748769, train_loss: 0.5420543835101228, val_loss: 1.2556331275131902 (41 / 100)\n",
            "train_acc: 0.830655129789864, val_acc: 0.5714285714285714, train_loss: 0.4880018808933065, val_loss: 1.5658398590651639 (42 / 100)\n",
            "train_acc: 0.8355995055624228, val_acc: 0.5566502463054187, train_loss: 0.46705365063086135, val_loss: 1.170225100270633 (43 / 100)\n",
            "train_acc: 0.8393077873918418, val_acc: 0.6206896551724138, train_loss: 0.44754072349652074, val_loss: 1.2612554477940638 (44 / 100)\n",
            "train_acc: 0.8393077873918418, val_acc: 0.5615763546798029, train_loss: 0.42676417936648076, val_loss: 1.2434127832868416 (45 / 100)\n",
            "train_acc: 0.8504326328800988, val_acc: 0.5517241379310345, train_loss: 0.43158295216460163, val_loss: 1.3686379147280614 (46 / 100)\n",
            "train_acc: 0.8121137206427689, val_acc: 0.5270935960591133, train_loss: 0.47962329797308584, val_loss: 1.3694749542057807 (47 / 100)\n",
            "train_acc: 0.8603213844252163, val_acc: 0.5517241379310345, train_loss: 0.35158658337092075, val_loss: 1.218914138272478 (48 / 100)\n",
            "train_acc: 0.892459826946848, val_acc: 0.5763546798029556, train_loss: 0.29011826963153553, val_loss: 1.6818435579685156 (49 / 100)\n",
            "train_acc: 0.8763906056860321, val_acc: 0.6502463054187192, train_loss: 0.3408468099695496, val_loss: 1.3579412293551592 (50 / 100)\n",
            "train_acc: 0.9048207663782447, val_acc: 0.6354679802955665, train_loss: 0.28613367142577106, val_loss: 1.5230607678150307 (51 / 100)\n",
            "train_acc: 0.9134734239802225, val_acc: 0.6354679802955665, train_loss: 0.2485086895774115, val_loss: 1.6251519108053498 (52 / 100)\n",
            "train_acc: 0.9110012360939431, val_acc: 0.5862068965517241, train_loss: 0.25393048205098645, val_loss: 1.6672952644930685 (53 / 100)\n",
            "train_acc: 0.9011124845488258, val_acc: 0.5369458128078818, train_loss: 0.2974778748265892, val_loss: 2.154646849397368 (54 / 100)\n",
            "train_acc: 0.9196538936959209, val_acc: 0.6108374384236454, train_loss: 0.22059579773797977, val_loss: 1.5509395385023408 (55 / 100)\n",
            "train_acc: 0.9258343634116193, val_acc: 0.5812807881773399, train_loss: 0.18585714495226244, val_loss: 1.4594049838376162 (56 / 100)\n",
            "train_acc: 0.9406674907292955, val_acc: 0.5960591133004927, train_loss: 0.17293981509686107, val_loss: 2.461033284957773 (57 / 100)\n",
            "train_acc: 0.8899876390605687, val_acc: 0.5517241379310345, train_loss: 0.2970237780560374, val_loss: 2.183263268964044 (58 / 100)\n",
            "train_acc: 0.9085290482076638, val_acc: 0.6059113300492611, train_loss: 0.24908862700421083, val_loss: 2.0696091251860698 (59 / 100)\n",
            "train_acc: 0.9369592088998764, val_acc: 0.6305418719211823, train_loss: 0.16812519929907083, val_loss: 1.9920062455050465 (60 / 100)\n",
            "train_acc: 0.9542645241038319, val_acc: 0.6305418719211823, train_loss: 0.12291042221196649, val_loss: 1.9132861245441906 (61 / 100)\n",
            "train_acc: 0.9592088998763906, val_acc: 0.6305418719211823, train_loss: 0.10066253366812197, val_loss: 1.9036951194255811 (62 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6354679802955665, train_loss: 0.07833236610638933, val_loss: 1.8927486829569775 (63 / 100)\n",
            "train_acc: 0.9641532756489494, val_acc: 0.6403940886699507, train_loss: 0.096893712233554, val_loss: 1.9971208751495249 (64 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.6502463054187192, train_loss: 0.08067589784582112, val_loss: 2.0029537666020136 (65 / 100)\n",
            "train_acc: 0.9678615574783683, val_acc: 0.6354679802955665, train_loss: 0.0952027374498629, val_loss: 2.0926983033494997 (66 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.625615763546798, train_loss: 0.07151697194767824, val_loss: 2.0493830316172446 (67 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6403940886699507, train_loss: 0.06607367656434275, val_loss: 2.0219075917610394 (68 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6206896551724138, train_loss: 0.0821464932598496, val_loss: 2.079676838343954 (69 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6157635467980296, train_loss: 0.06095280885991119, val_loss: 2.1578694344154132 (70 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6305418719211823, train_loss: 0.04830947956726489, val_loss: 2.1991110059428096 (71 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6206896551724138, train_loss: 0.06723993537747226, val_loss: 2.195874654013535 (72 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6009852216748769, train_loss: 0.08108539295432299, val_loss: 2.2399850784264173 (73 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6059113300492611, train_loss: 0.07576344747036437, val_loss: 2.2059775452895707 (74 / 100)\n",
            "train_acc: 0.9802224969097652, val_acc: 0.6059113300492611, train_loss: 0.06625888668857192, val_loss: 2.2242922169234367 (75 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6059113300492611, train_loss: 0.06407549958883316, val_loss: 2.271850263837523 (76 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.5960591133004927, train_loss: 0.0720906645170101, val_loss: 2.26259573809619 (77 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.5960591133004927, train_loss: 0.0565322970577754, val_loss: 2.2915802048932155 (78 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6059113300492611, train_loss: 0.06272224899570196, val_loss: 2.3501938930873214 (79 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.6009852216748769, train_loss: 0.06624108044561851, val_loss: 2.343161399728559 (80 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6157635467980296, train_loss: 0.052980653877164, val_loss: 2.38551939355916 (81 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.6206896551724138, train_loss: 0.07270181341135899, val_loss: 2.3295334192919612 (82 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6059113300492611, train_loss: 0.04802965469501811, val_loss: 2.4228000212185488 (83 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6157635467980296, train_loss: 0.06471413926524197, val_loss: 2.4131171644614837 (84 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.5960591133004927, train_loss: 0.066273981620119, val_loss: 2.428780642049066 (85 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.5911330049261084, train_loss: 0.05008179885053222, val_loss: 2.4942433511095095 (86 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6009852216748769, train_loss: 0.06307580017188866, val_loss: 2.4841029191839286 (87 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.5911330049261084, train_loss: 0.04185305640782944, val_loss: 2.5695150620831644 (88 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.5911330049261084, train_loss: 0.04618625293113982, val_loss: 2.6540827522136894 (89 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.5960591133004927, train_loss: 0.048252180863369826, val_loss: 2.6055621319803697 (90 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6059113300492611, train_loss: 0.0361861105460319, val_loss: 2.6402859881593677 (91 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.6108374384236454, train_loss: 0.03213508358991927, val_loss: 2.6413110418272723 (92 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.6108374384236454, train_loss: 0.04315941383871071, val_loss: 2.6067734936188005 (93 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.5960591133004927, train_loss: 0.06671104737648416, val_loss: 2.631737849688882 (94 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6009852216748769, train_loss: 0.06437077097898655, val_loss: 2.578400541115277 (95 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.5960591133004927, train_loss: 0.042280172093394956, val_loss: 2.630060510975974 (96 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.5911330049261084, train_loss: 0.052316891395559416, val_loss: 2.6882299175990627 (97 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.5960591133004927, train_loss: 0.06409127323235511, val_loss: 2.6082129924755377 (98 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.5911330049261084, train_loss: 0.05072733056265285, val_loss: 2.704536243318924 (99 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.5911330049261084, train_loss: 0.03849921621409865, val_loss: 2.780947189025691 (100 / 100)\n",
            "({'lr': 0.001, 'batch_size': 8, 'weight_decay': 1e-05, 'gamma': 0.05}), val accuracy 0.6502463054187192, val loss 1.3579412293551592\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.776161844267686, val_loss: 1.7748959692828175 (1 / 100)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7650602848185009, val_loss: 1.7271266470988984 (2 / 100)\n",
            "train_acc: 0.22249690976514216, val_acc: 0.2413793103448276, train_loss: 1.7041137869779495, val_loss: 1.7549225661554948 (3 / 100)\n",
            "train_acc: 0.29666254635352285, val_acc: 0.33004926108374383, train_loss: 1.6633937968901267, val_loss: 1.6002815873752088 (4 / 100)\n",
            "train_acc: 0.2669962917181706, val_acc: 0.23645320197044334, train_loss: 1.6891572080524948, val_loss: 1.744096542814095 (5 / 100)\n",
            "train_acc: 0.32014833127317677, val_acc: 0.32019704433497537, train_loss: 1.6227412795548974, val_loss: 1.6127175574232204 (6 / 100)\n",
            "train_acc: 0.2694684796044499, val_acc: 0.33004926108374383, train_loss: 1.6718000559635893, val_loss: 1.618563810005564 (7 / 100)\n",
            "train_acc: 0.2558714462299135, val_acc: 0.3448275862068966, train_loss: 1.6936152379209237, val_loss: 1.5750968914313856 (8 / 100)\n",
            "train_acc: 0.30284301606922126, val_acc: 0.3497536945812808, train_loss: 1.584800211843955, val_loss: 1.783346526728475 (9 / 100)\n",
            "train_acc: 0.3176761433868974, val_acc: 0.2857142857142857, train_loss: 1.5961880436344407, val_loss: 1.6227654371355555 (10 / 100)\n",
            "train_acc: 0.34981458590852904, val_acc: 0.1921182266009852, train_loss: 1.5624870092818706, val_loss: 1.7770118449121861 (11 / 100)\n",
            "train_acc: 0.3362175525339926, val_acc: 0.35467980295566504, train_loss: 1.588651460829123, val_loss: 1.514542261955186 (12 / 100)\n",
            "train_acc: 0.36711990111248455, val_acc: 0.32019704433497537, train_loss: 1.5115779737017505, val_loss: 1.5438968001915316 (13 / 100)\n",
            "train_acc: 0.3757725587144623, val_acc: 0.35467980295566504, train_loss: 1.481964731835317, val_loss: 1.4847759783561594 (14 / 100)\n",
            "train_acc: 0.37453646477132263, val_acc: 0.3891625615763547, train_loss: 1.460916026266308, val_loss: 1.4356425636507608 (15 / 100)\n",
            "train_acc: 0.3720642768850433, val_acc: 0.4088669950738916, train_loss: 1.4506589097646612, val_loss: 1.4119974245578784 (16 / 100)\n",
            "train_acc: 0.39555006180469715, val_acc: 0.37438423645320196, train_loss: 1.419062790970867, val_loss: 1.3782921759365814 (17 / 100)\n",
            "train_acc: 0.35970333745364647, val_acc: 0.3645320197044335, train_loss: 1.4780874611862804, val_loss: 1.431219770990569 (18 / 100)\n",
            "train_acc: 0.41285537700865266, val_acc: 0.4236453201970443, train_loss: 1.367166550109354, val_loss: 1.3122546766779106 (19 / 100)\n",
            "train_acc: 0.4289245982694685, val_acc: 0.4236453201970443, train_loss: 1.3242196131990336, val_loss: 1.2724427971346626 (20 / 100)\n",
            "train_acc: 0.41409147095179233, val_acc: 0.3891625615763547, train_loss: 1.3136464935003311, val_loss: 1.28390526888993 (21 / 100)\n",
            "train_acc: 0.45859085290482077, val_acc: 0.4729064039408867, train_loss: 1.2768386639240203, val_loss: 1.2099098404640047 (22 / 100)\n",
            "train_acc: 0.4721878862793572, val_acc: 0.4236453201970443, train_loss: 1.2242332310847506, val_loss: 1.4042892068477686 (23 / 100)\n",
            "train_acc: 0.4721878862793572, val_acc: 0.41379310344827586, train_loss: 1.2248399646674157, val_loss: 1.339648981399724 (24 / 100)\n",
            "train_acc: 0.4932014833127318, val_acc: 0.4482758620689655, train_loss: 1.1755931465823217, val_loss: 1.3043095745476596 (25 / 100)\n",
            "train_acc: 0.47713226205191595, val_acc: 0.5073891625615764, train_loss: 1.1747096498463445, val_loss: 1.1514149642930243 (26 / 100)\n",
            "train_acc: 0.5166872682323856, val_acc: 0.49261083743842365, train_loss: 1.112547199127848, val_loss: 1.1310657447782055 (27 / 100)\n",
            "train_acc: 0.5105067985166872, val_acc: 0.4630541871921182, train_loss: 1.0948514192743855, val_loss: 1.3470302605863862 (28 / 100)\n",
            "train_acc: 0.5562422744128553, val_acc: 0.45320197044334976, train_loss: 1.0360146296775827, val_loss: 1.2551728334332921 (29 / 100)\n",
            "train_acc: 0.5611866501854141, val_acc: 0.5123152709359606, train_loss: 1.0479497971434528, val_loss: 1.0867785685168112 (30 / 100)\n",
            "train_acc: 0.553770086526576, val_acc: 0.49261083743842365, train_loss: 1.0332376426465726, val_loss: 1.4571324651464452 (31 / 100)\n",
            "train_acc: 0.5488257107540173, val_acc: 0.541871921182266, train_loss: 0.9997686523442215, val_loss: 1.237691990847658 (32 / 100)\n",
            "train_acc: 0.6019777503090235, val_acc: 0.5024630541871922, train_loss: 0.91807540518097, val_loss: 1.1704226644168347 (33 / 100)\n",
            "train_acc: 0.6279357231149567, val_acc: 0.41379310344827586, train_loss: 0.9037937858638126, val_loss: 1.3709430406833518 (34 / 100)\n",
            "train_acc: 0.6291718170580964, val_acc: 0.5270935960591133, train_loss: 0.8893562477215553, val_loss: 1.4866380268717048 (35 / 100)\n",
            "train_acc: 0.6217552533992583, val_acc: 0.5369458128078818, train_loss: 0.8765463098165278, val_loss: 1.1742351824426887 (36 / 100)\n",
            "train_acc: 0.657601977750309, val_acc: 0.5320197044334976, train_loss: 0.8075024306553109, val_loss: 1.1492360043408247 (37 / 100)\n",
            "train_acc: 0.6971569839307787, val_acc: 0.5172413793103449, train_loss: 0.7442240293594756, val_loss: 2.228500323342572 (38 / 100)\n",
            "train_acc: 0.7058096415327565, val_acc: 0.46798029556650245, train_loss: 0.7287138248108815, val_loss: 1.2125111976867826 (39 / 100)\n",
            "train_acc: 0.7033374536464772, val_acc: 0.5517241379310345, train_loss: 0.712916081267028, val_loss: 1.282698397565945 (40 / 100)\n",
            "train_acc: 0.7441285537700866, val_acc: 0.5763546798029556, train_loss: 0.6423395876536705, val_loss: 1.3371237034868138 (41 / 100)\n",
            "train_acc: 0.7280593325092707, val_acc: 0.5615763546798029, train_loss: 0.6819353379189452, val_loss: 1.610154151622885 (42 / 100)\n",
            "train_acc: 0.7663782447466008, val_acc: 0.5763546798029556, train_loss: 0.5787705183029175, val_loss: 1.3413369532289177 (43 / 100)\n",
            "train_acc: 0.7527812113720643, val_acc: 0.5369458128078818, train_loss: 0.6222781099111984, val_loss: 1.1965417803214689 (44 / 100)\n",
            "train_acc: 0.7824474660074165, val_acc: 0.5665024630541872, train_loss: 0.5492763989345989, val_loss: 1.4989940286269916 (45 / 100)\n",
            "train_acc: 0.7873918417799752, val_acc: 0.5763546798029556, train_loss: 0.50672099381059, val_loss: 1.6139875949897202 (46 / 100)\n",
            "train_acc: 0.7812113720642769, val_acc: 0.5270935960591133, train_loss: 0.5527455081751085, val_loss: 2.057492254990075 (47 / 100)\n",
            "train_acc: 0.8331273176761433, val_acc: 0.6059113300492611, train_loss: 0.4481331469544079, val_loss: 1.1607159006184544 (48 / 100)\n",
            "train_acc: 0.826946847960445, val_acc: 0.5665024630541872, train_loss: 0.43444221114051357, val_loss: 1.2147139686669035 (49 / 100)\n",
            "train_acc: 0.8355995055624228, val_acc: 0.5369458128078818, train_loss: 0.424743882215804, val_loss: 1.498170271295632 (50 / 100)\n",
            "train_acc: 0.8665018541409147, val_acc: 0.5369458128078818, train_loss: 0.3687011146427527, val_loss: 2.373682717971614 (51 / 100)\n",
            "train_acc: 0.8702101359703337, val_acc: 0.5467980295566502, train_loss: 0.3878139450464614, val_loss: 1.8121903347851607 (52 / 100)\n",
            "train_acc: 0.8776266996291718, val_acc: 0.5714285714285714, train_loss: 0.33790076898850674, val_loss: 1.9895601689521902 (53 / 100)\n",
            "train_acc: 0.8763906056860321, val_acc: 0.5763546798029556, train_loss: 0.3579512091885391, val_loss: 2.1988414837221795 (54 / 100)\n",
            "train_acc: 0.8714462299134734, val_acc: 0.5369458128078818, train_loss: 0.30443016796088485, val_loss: 2.3398136822460907 (55 / 100)\n",
            "train_acc: 0.8405438813349815, val_acc: 0.5714285714285714, train_loss: 0.40789248460008126, val_loss: 2.8357464151429426 (56 / 100)\n",
            "train_acc: 0.8726823238566132, val_acc: 0.5566502463054187, train_loss: 0.37543489081308484, val_loss: 2.385440692525779 (57 / 100)\n",
            "train_acc: 0.8800988875154512, val_acc: 0.6009852216748769, train_loss: 0.3260406249827891, val_loss: 1.3941974076144215 (58 / 100)\n",
            "train_acc: 0.934487021013597, val_acc: 0.5714285714285714, train_loss: 0.1633495665598564, val_loss: 2.628448506147403 (59 / 100)\n",
            "train_acc: 0.9295426452410384, val_acc: 0.5862068965517241, train_loss: 0.21435840860727542, val_loss: 2.15069896892961 (60 / 100)\n",
            "train_acc: 0.9678615574783683, val_acc: 0.5911330049261084, train_loss: 0.11136407124244681, val_loss: 2.068101933143409 (61 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6009852216748769, train_loss: 0.06477826973711162, val_loss: 2.1494226065175286 (62 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6009852216748769, train_loss: 0.06955563432648686, val_loss: 2.2880654076637303 (63 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6009852216748769, train_loss: 0.05703650932524201, val_loss: 2.2909903523369963 (64 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.5960591133004927, train_loss: 0.08217674221184994, val_loss: 2.186065123879851 (65 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.6009852216748769, train_loss: 0.061576074958584366, val_loss: 2.2925187005785297 (66 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6157635467980296, train_loss: 0.06539345686456328, val_loss: 2.2596824668311135 (67 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.625615763546798, train_loss: 0.04589277734273149, val_loss: 2.227667583326988 (68 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6305418719211823, train_loss: 0.056409827121550724, val_loss: 2.309740585646606 (69 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.6059113300492611, train_loss: 0.08036033302656063, val_loss: 2.4858311661358536 (70 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.6009852216748769, train_loss: 0.06359901431169734, val_loss: 2.512324630920523 (71 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.5960591133004927, train_loss: 0.06421315611071876, val_loss: 2.3603664950784204 (72 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6059113300492611, train_loss: 0.056905196093216225, val_loss: 2.416540479718758 (73 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6059113300492611, train_loss: 0.036791685337778664, val_loss: 2.571221536309848 (74 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6059113300492611, train_loss: 0.046858833363678014, val_loss: 2.6504929177279544 (75 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6009852216748769, train_loss: 0.03508801852227436, val_loss: 2.8284334836922254 (76 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6059113300492611, train_loss: 0.041339959290767336, val_loss: 2.883956371857028 (77 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6157635467980296, train_loss: 0.03803020295754941, val_loss: 2.746489129630216 (78 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.5911330049261084, train_loss: 0.03857137319920826, val_loss: 2.769432660394114 (79 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.5862068965517241, train_loss: 0.04686769964668453, val_loss: 2.8826332620799247 (80 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.5911330049261084, train_loss: 0.04082533146159169, val_loss: 2.879913175047325 (81 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.5911330049261084, train_loss: 0.03392728046964214, val_loss: 3.004744695912441 (82 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.5911330049261084, train_loss: 0.04635793316938969, val_loss: 2.9506860337233896 (83 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.5960591133004927, train_loss: 0.03091476609002527, val_loss: 2.962198960370031 (84 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6059113300492611, train_loss: 0.04923775921056533, val_loss: 2.8565292329036542 (85 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.5862068965517241, train_loss: 0.05256450146768238, val_loss: 2.733476843152727 (86 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6157635467980296, train_loss: 0.04343448286445533, val_loss: 2.754196096523642 (87 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.5911330049261084, train_loss: 0.05052425613804123, val_loss: 2.827568061833311 (88 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.6157635467980296, train_loss: 0.03277486805862785, val_loss: 2.767855534999829 (89 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.5763546798029556, train_loss: 0.04195796986592862, val_loss: 2.890530340190004 (90 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.5911330049261084, train_loss: 0.03364660816522699, val_loss: 2.956975247472378 (91 / 100)\n",
            "train_acc: 0.9950556242274413, val_acc: 0.5862068965517241, train_loss: 0.019507511731868033, val_loss: 2.999526096682243 (92 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.5960591133004927, train_loss: 0.03796489733848053, val_loss: 3.09351716135523 (93 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.5911330049261084, train_loss: 0.027082824147086505, val_loss: 3.3059717281698595 (94 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.5911330049261084, train_loss: 0.02287135268613346, val_loss: 3.2251143678655767 (95 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.6009852216748769, train_loss: 0.027580624899846515, val_loss: 3.274923928265501 (96 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.5862068965517241, train_loss: 0.027910671216448395, val_loss: 3.21521171677876 (97 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.5862068965517241, train_loss: 0.04106507578355862, val_loss: 3.1884713337339203 (98 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.6009852216748769, train_loss: 0.03204203035097629, val_loss: 3.0961247271504897 (99 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6157635467980296, train_loss: 0.03201655831708307, val_loss: 3.1107122322608687 (100 / 100)\n",
            "({'lr': 0.001, 'batch_size': 8, 'weight_decay': 1e-05, 'gamma': 0.1}), val accuracy 0.6305418719211823, val loss 2.309740585646606\n",
            "train_acc: 0.17428924598269468, val_acc: 0.18226600985221675, train_loss: 1.7834432752819085, val_loss: 1.7614808928203114 (1 / 100)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.2660098522167488, train_loss: 1.7619402891920581, val_loss: 1.7518073238175491 (2 / 100)\n",
            "train_acc: 0.24103831891223734, val_acc: 0.1921182266009852, train_loss: 1.7311492390626735, val_loss: 1.6811388889557035 (3 / 100)\n",
            "train_acc: 0.2719406674907293, val_acc: 0.26108374384236455, train_loss: 1.6963911305546613, val_loss: 1.6426037121288881 (4 / 100)\n",
            "train_acc: 0.27441285537700866, val_acc: 0.26108374384236455, train_loss: 1.6765215688641788, val_loss: 1.6082694418911863 (5 / 100)\n",
            "train_acc: 0.26823238566131025, val_acc: 0.18226600985221675, train_loss: 1.6919795992347897, val_loss: 1.787228939568468 (6 / 100)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.1921182266009852, train_loss: 1.7762166161472335, val_loss: 1.7614710571730665 (7 / 100)\n",
            "train_acc: 0.1668726823238566, val_acc: 0.18226600985221675, train_loss: 1.7755433873576199, val_loss: 1.7587465905203608 (8 / 100)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7693841679871303, val_loss: 1.7616883974357191 (9 / 100)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.22167487684729065, train_loss: 1.7547679672429823, val_loss: 1.7377729721257251 (10 / 100)\n",
            "train_acc: 0.21631644004944375, val_acc: 0.22167487684729065, train_loss: 1.7589652128655773, val_loss: 1.7250173197591245 (11 / 100)\n",
            "train_acc: 0.24351050679851668, val_acc: 0.22660098522167488, train_loss: 1.7416500642950956, val_loss: 1.7488684436957824 (12 / 100)\n",
            "train_acc: 0.2620519159456119, val_acc: 0.18226600985221675, train_loss: 1.7261633084642578, val_loss: 1.7788329453303897 (13 / 100)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.27586206896551724, train_loss: 1.7724843848915712, val_loss: 1.7569236966776731 (14 / 100)\n",
            "train_acc: 0.28182941903584674, val_acc: 0.2561576354679803, train_loss: 1.6981672455265437, val_loss: 1.6782376214201227 (15 / 100)\n",
            "train_acc: 0.3164400494437577, val_acc: 0.21674876847290642, train_loss: 1.639331555926461, val_loss: 1.803441145149945 (16 / 100)\n",
            "train_acc: 0.32756489493201485, val_acc: 0.3251231527093596, train_loss: 1.596221421940807, val_loss: 1.5674130851999293 (17 / 100)\n",
            "train_acc: 0.3411619283065513, val_acc: 0.35467980295566504, train_loss: 1.524483628561824, val_loss: 1.4669693138799056 (18 / 100)\n",
            "train_acc: 0.37824474660074164, val_acc: 0.3251231527093596, train_loss: 1.4733200851270678, val_loss: 1.5393050362911131 (19 / 100)\n",
            "train_acc: 0.3831891223733004, val_acc: 0.33004926108374383, train_loss: 1.4679459816740528, val_loss: 1.6414922211557774 (20 / 100)\n",
            "train_acc: 0.4042027194066749, val_acc: 0.39901477832512317, train_loss: 1.4650970645829096, val_loss: 1.3530303371950911 (21 / 100)\n",
            "train_acc: 0.37330037082818296, val_acc: 0.3842364532019704, train_loss: 1.4270686263354364, val_loss: 1.3827215949890062 (22 / 100)\n",
            "train_acc: 0.411619283065513, val_acc: 0.43349753694581283, train_loss: 1.3812657849603, val_loss: 1.352374324070409 (23 / 100)\n",
            "train_acc: 0.39184177997527814, val_acc: 0.4187192118226601, train_loss: 1.4169881473512969, val_loss: 1.3482976646846152 (24 / 100)\n",
            "train_acc: 0.4276885043263288, val_acc: 0.45320197044334976, train_loss: 1.3614332929382513, val_loss: 1.2811444566167633 (25 / 100)\n",
            "train_acc: 0.46971569839307786, val_acc: 0.3842364532019704, train_loss: 1.3122887275422312, val_loss: 1.4293286392254194 (26 / 100)\n",
            "train_acc: 0.40914709517923364, val_acc: 0.4482758620689655, train_loss: 1.376066824709087, val_loss: 1.2825472428293652 (27 / 100)\n",
            "train_acc: 0.4796044499381953, val_acc: 0.4039408866995074, train_loss: 1.2643061462232592, val_loss: 1.2893943883515344 (28 / 100)\n",
            "train_acc: 0.4511742892459827, val_acc: 0.4827586206896552, train_loss: 1.2808228941871445, val_loss: 1.2609846953100758 (29 / 100)\n",
            "train_acc: 0.4647713226205192, val_acc: 0.4236453201970443, train_loss: 1.2574245932665096, val_loss: 1.2877892020887929 (30 / 100)\n",
            "train_acc: 0.49938195302843014, val_acc: 0.43842364532019706, train_loss: 1.2094563814559296, val_loss: 1.3242423352349568 (31 / 100)\n",
            "train_acc: 0.522867737948084, val_acc: 0.4975369458128079, train_loss: 1.201468224873207, val_loss: 1.201506071196401 (32 / 100)\n",
            "train_acc: 0.4907292954264524, val_acc: 0.43842364532019706, train_loss: 1.1827514047823082, val_loss: 1.3842360803059168 (33 / 100)\n",
            "train_acc: 0.5339925834363412, val_acc: 0.4876847290640394, train_loss: 1.126331831233021, val_loss: 1.2452590421502814 (34 / 100)\n",
            "train_acc: 0.5587144622991347, val_acc: 0.5024630541871922, train_loss: 1.097945052702141, val_loss: 1.1551126614580014 (35 / 100)\n",
            "train_acc: 0.5698393077873919, val_acc: 0.49261083743842365, train_loss: 1.0648354302230665, val_loss: 1.2334885943699352 (36 / 100)\n",
            "train_acc: 0.5784919653893696, val_acc: 0.4236453201970443, train_loss: 1.023963096852061, val_loss: 1.786777035943393 (37 / 100)\n",
            "train_acc: 0.5698393077873919, val_acc: 0.5024630541871922, train_loss: 1.0402617666718397, val_loss: 1.1702555188991752 (38 / 100)\n",
            "train_acc: 0.6205191594561187, val_acc: 0.4433497536945813, train_loss: 0.9476532538240713, val_loss: 1.246508402777423 (39 / 100)\n",
            "train_acc: 0.6427688504326329, val_acc: 0.5812807881773399, train_loss: 0.9086059660493075, val_loss: 1.0877515007122396 (40 / 100)\n",
            "train_acc: 0.6118665018541409, val_acc: 0.5024630541871922, train_loss: 1.0039234918775901, val_loss: 1.2465763473745637 (41 / 100)\n",
            "train_acc: 0.6909765142150803, val_acc: 0.5369458128078818, train_loss: 0.8007299459466828, val_loss: 1.1983140542589386 (42 / 100)\n",
            "train_acc: 0.7045735475896168, val_acc: 0.5517241379310345, train_loss: 0.7925974361976086, val_loss: 1.4731532276557584 (43 / 100)\n",
            "train_acc: 0.7045735475896168, val_acc: 0.5467980295566502, train_loss: 0.7332053470375806, val_loss: 1.7818738639061087 (44 / 100)\n",
            "train_acc: 0.7367119901112484, val_acc: 0.5172413793103449, train_loss: 0.6961054109377678, val_loss: 1.4673192342513888 (45 / 100)\n",
            "train_acc: 0.7663782447466008, val_acc: 0.5467980295566502, train_loss: 0.675393639300456, val_loss: 1.3497084250003832 (46 / 100)\n",
            "train_acc: 0.7589616810877626, val_acc: 0.5812807881773399, train_loss: 0.6067326338535776, val_loss: 1.4354594452627774 (47 / 100)\n",
            "train_acc: 0.7725587144622992, val_acc: 0.5517241379310345, train_loss: 0.5953981618504, val_loss: 1.1200755358916785 (48 / 100)\n",
            "train_acc: 0.8034610630407911, val_acc: 0.4876847290640394, train_loss: 0.5298173989000073, val_loss: 1.5359079977268069 (49 / 100)\n",
            "train_acc: 0.7948084054388134, val_acc: 0.6059113300492611, train_loss: 0.511688317739772, val_loss: 1.376879744929046 (50 / 100)\n",
            "train_acc: 0.8491965389369592, val_acc: 0.6108374384236454, train_loss: 0.42505098302815253, val_loss: 1.8307266622928564 (51 / 100)\n",
            "train_acc: 0.8726823238566132, val_acc: 0.4876847290640394, train_loss: 0.38311347560623227, val_loss: 1.6856683614512382 (52 / 100)\n",
            "train_acc: 0.8714462299134734, val_acc: 0.5665024630541872, train_loss: 0.38566134916100425, val_loss: 1.3984739102166275 (53 / 100)\n",
            "train_acc: 0.8566131025957973, val_acc: 0.5467980295566502, train_loss: 0.40439465638291555, val_loss: 1.450434387317432 (54 / 100)\n",
            "train_acc: 0.8862793572311496, val_acc: 0.5517241379310345, train_loss: 0.27381420179704213, val_loss: 1.7850651030469997 (55 / 100)\n",
            "train_acc: 0.9060568603213844, val_acc: 0.5911330049261084, train_loss: 0.29632094366146694, val_loss: 2.5134614430037625 (56 / 100)\n",
            "train_acc: 0.8986402966625463, val_acc: 0.6157635467980296, train_loss: 0.29658966792381297, val_loss: 1.3580342606077054 (57 / 100)\n",
            "train_acc: 0.9196538936959209, val_acc: 0.5665024630541872, train_loss: 0.24347730900065417, val_loss: 2.846316261244525 (58 / 100)\n",
            "train_acc: 0.9332509270704573, val_acc: 0.5960591133004927, train_loss: 0.20222875479567332, val_loss: 2.1814553373552896 (59 / 100)\n",
            "train_acc: 0.9184177997527813, val_acc: 0.6206896551724138, train_loss: 0.23048123720991892, val_loss: 1.617329190223675 (60 / 100)\n",
            "train_acc: 0.965389369592089, val_acc: 0.6551724137931034, train_loss: 0.11982141599077524, val_loss: 1.661638236985418 (61 / 100)\n",
            "train_acc: 0.9641532756489494, val_acc: 0.645320197044335, train_loss: 0.10589225080016222, val_loss: 1.692011136139555 (62 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.645320197044335, train_loss: 0.08137721567425062, val_loss: 1.7604998890402281 (63 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6502463054187192, train_loss: 0.05254736745313307, val_loss: 1.8657757895333427 (64 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6403940886699507, train_loss: 0.058268369054617485, val_loss: 1.8884013773772517 (65 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6354679802955665, train_loss: 0.06448851704450119, val_loss: 1.945303231037309 (66 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6305418719211823, train_loss: 0.05842897314371078, val_loss: 1.9988579814657201 (67 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6206896551724138, train_loss: 0.058215193607014395, val_loss: 2.005243030674939 (68 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6305418719211823, train_loss: 0.055141578940732225, val_loss: 2.014126543341012 (69 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6108374384236454, train_loss: 0.0625312366650632, val_loss: 2.033091519853752 (70 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.6108374384236454, train_loss: 0.07365659243980945, val_loss: 2.087424331697924 (71 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6305418719211823, train_loss: 0.05119431284066331, val_loss: 2.049771805114934 (72 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6009852216748769, train_loss: 0.04124562142069172, val_loss: 2.124727731267807 (73 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6354679802955665, train_loss: 0.05278884347790693, val_loss: 2.1463177626943355 (74 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6502463054187192, train_loss: 0.06623352458067376, val_loss: 2.233510060263385 (75 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.645320197044335, train_loss: 0.044546551698512585, val_loss: 2.2280121771572845 (76 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6206896551724138, train_loss: 0.041961811086892786, val_loss: 2.3251772649182474 (77 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6354679802955665, train_loss: 0.046207732853694956, val_loss: 2.3284799347957366 (78 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6305418719211823, train_loss: 0.0504510336370197, val_loss: 2.3297907892119123 (79 / 100)\n",
            "train_acc: 0.9802224969097652, val_acc: 0.6108374384236454, train_loss: 0.05379646714449813, val_loss: 2.358989655677908 (80 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6206896551724138, train_loss: 0.04557949356154547, val_loss: 2.3587735345210934 (81 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.625615763546798, train_loss: 0.046328820168456276, val_loss: 2.3121276732736034 (82 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6157635467980296, train_loss: 0.03654387574850113, val_loss: 2.278150942525253 (83 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.625615763546798, train_loss: 0.04361380561173183, val_loss: 2.287012151603041 (84 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.625615763546798, train_loss: 0.03889089476487545, val_loss: 2.359956179639976 (85 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6403940886699507, train_loss: 0.06833754835376339, val_loss: 2.3259364064103862 (86 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6305418719211823, train_loss: 0.04422017346206495, val_loss: 2.3791646751864204 (87 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.6206896551724138, train_loss: 0.026760266207351967, val_loss: 2.3854758848110444 (88 / 100)\n",
            "train_acc: 0.992583436341162, val_acc: 0.6206896551724138, train_loss: 0.025481609979872356, val_loss: 2.3844849760896465 (89 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6305418719211823, train_loss: 0.03168672389536175, val_loss: 2.431873387303846 (90 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.6403940886699507, train_loss: 0.03760918507027832, val_loss: 2.5197138372313215 (91 / 100)\n",
            "train_acc: 0.9950556242274413, val_acc: 0.6305418719211823, train_loss: 0.03448619432885508, val_loss: 2.4605904098214775 (92 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.625615763546798, train_loss: 0.041919078461466676, val_loss: 2.436519393779961 (93 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.6157635467980296, train_loss: 0.047358505216016165, val_loss: 2.4650697611235635 (94 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6108374384236454, train_loss: 0.040657399021945574, val_loss: 2.4972932520758344 (95 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6206896551724138, train_loss: 0.04763817183019499, val_loss: 2.511592266007597 (96 / 100)\n",
            "train_acc: 0.9950556242274413, val_acc: 0.6354679802955665, train_loss: 0.023426986153252487, val_loss: 2.5867209980640506 (97 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.645320197044335, train_loss: 0.031427139257471114, val_loss: 2.662269934057602 (98 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6551724137931034, train_loss: 0.04001201304281303, val_loss: 2.68223340464343 (99 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.645320197044335, train_loss: 0.039776884139099876, val_loss: 2.6620672497843287 (100 / 100)\n",
            "({'lr': 0.001, 'batch_size': 8, 'weight_decay': 0.001, 'gamma': 0.05}), val accuracy 0.6551724137931034, val loss 1.661638236985418\n",
            "train_acc: 0.1792336217552534, val_acc: 0.1921182266009852, train_loss: 1.7727910782704395, val_loss: 1.7787025632529423 (1 / 100)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7657721797084927, val_loss: 1.7435122810561081 (2 / 100)\n",
            "train_acc: 0.2027194066749073, val_acc: 0.28078817733990147, train_loss: 1.7477506889843086, val_loss: 1.720449569190077 (3 / 100)\n",
            "train_acc: 0.24598269468479605, val_acc: 0.26108374384236455, train_loss: 1.7279260547553064, val_loss: 1.7385558988073189 (4 / 100)\n",
            "train_acc: 0.26081582200247216, val_acc: 0.29064039408866993, train_loss: 1.7342421650444475, val_loss: 1.6655045923928322 (5 / 100)\n",
            "train_acc: 0.2830655129789864, val_acc: 0.32019704433497537, train_loss: 1.6927475590346328, val_loss: 1.6508340312929577 (6 / 100)\n",
            "train_acc: 0.32014833127317677, val_acc: 0.29064039408866993, train_loss: 1.613093926084351, val_loss: 1.4781100791076134 (7 / 100)\n",
            "train_acc: 0.3164400494437577, val_acc: 0.27586206896551724, train_loss: 1.628039315398161, val_loss: 1.5758587392092926 (8 / 100)\n",
            "train_acc: 0.35599505562422745, val_acc: 0.458128078817734, train_loss: 1.5122769524346766, val_loss: 1.3364846882561745 (9 / 100)\n",
            "train_acc: 0.37453646477132263, val_acc: 0.43349753694581283, train_loss: 1.4651927143445858, val_loss: 1.3559628161303516 (10 / 100)\n",
            "train_acc: 0.3757725587144623, val_acc: 0.4236453201970443, train_loss: 1.4518716747887497, val_loss: 1.3385731733491268 (11 / 100)\n",
            "train_acc: 0.3831891223733004, val_acc: 0.4482758620689655, train_loss: 1.3877006540781782, val_loss: 1.2693437279151578 (12 / 100)\n",
            "train_acc: 0.411619283065513, val_acc: 0.4236453201970443, train_loss: 1.3466089157887235, val_loss: 1.2902693648643682 (13 / 100)\n",
            "train_acc: 0.41409147095179233, val_acc: 0.4630541871921182, train_loss: 1.328338275733778, val_loss: 1.256711541138259 (14 / 100)\n",
            "train_acc: 0.44870210135970334, val_acc: 0.46798029556650245, train_loss: 1.2829181641082386, val_loss: 1.267885207542645 (15 / 100)\n",
            "train_acc: 0.46600741656365885, val_acc: 0.47783251231527096, train_loss: 1.247659779596977, val_loss: 1.2520383954635395 (16 / 100)\n",
            "train_acc: 0.4647713226205192, val_acc: 0.4039408866995074, train_loss: 1.2317509433100338, val_loss: 1.393817775061565 (17 / 100)\n",
            "train_acc: 0.48084054388133496, val_acc: 0.4975369458128079, train_loss: 1.2257503070701627, val_loss: 1.1941900047762641 (18 / 100)\n",
            "train_acc: 0.4969097651421508, val_acc: 0.5123152709359606, train_loss: 1.1530950244778608, val_loss: 1.1302594078585433 (19 / 100)\n",
            "train_acc: 0.5030902348578492, val_acc: 0.4876847290640394, train_loss: 1.165955651675815, val_loss: 1.1459585856921568 (20 / 100)\n",
            "train_acc: 0.5364647713226205, val_acc: 0.5467980295566502, train_loss: 1.1106738761270003, val_loss: 1.0720049791735382 (21 / 100)\n",
            "train_acc: 0.5574783683559951, val_acc: 0.5123152709359606, train_loss: 1.0998479320917496, val_loss: 1.1311706695063362 (22 / 100)\n",
            "train_acc: 0.5500618046971569, val_acc: 0.5320197044334976, train_loss: 1.071355936406422, val_loss: 1.2770753124077332 (23 / 100)\n",
            "train_acc: 0.5859085290482077, val_acc: 0.4827586206896552, train_loss: 0.997863982015546, val_loss: 1.4159774046226088 (24 / 100)\n",
            "train_acc: 0.5784919653893696, val_acc: 0.5517241379310345, train_loss: 0.9971866327693643, val_loss: 1.110081438654162 (25 / 100)\n",
            "train_acc: 0.5908529048207664, val_acc: 0.5172413793103449, train_loss: 0.9748490509497663, val_loss: 1.4899200606228682 (26 / 100)\n",
            "train_acc: 0.6341161928306551, val_acc: 0.49261083743842365, train_loss: 0.9100432611071725, val_loss: 1.4449528347095246 (27 / 100)\n",
            "train_acc: 0.6402966625463535, val_acc: 0.541871921182266, train_loss: 0.9248699384507791, val_loss: 1.1492546638244479 (28 / 100)\n",
            "train_acc: 0.6266996291718171, val_acc: 0.541871921182266, train_loss: 0.9144450073925909, val_loss: 1.1602623679955018 (29 / 100)\n",
            "train_acc: 0.6761433868974042, val_acc: 0.5073891625615764, train_loss: 0.828600457925879, val_loss: 1.687030100470106 (30 / 100)\n",
            "train_acc: 0.69221260815822, val_acc: 0.5270935960591133, train_loss: 0.7636397733676566, val_loss: 1.5336248263936911 (31 / 100)\n",
            "train_acc: 0.7107540173053152, val_acc: 0.5123152709359606, train_loss: 0.7469976601111432, val_loss: 1.7191274130872904 (32 / 100)\n",
            "train_acc: 0.7206427688504327, val_acc: 0.5615763546798029, train_loss: 0.690856229094846, val_loss: 1.4498775246108107 (33 / 100)\n",
            "train_acc: 0.73053152039555, val_acc: 0.541871921182266, train_loss: 0.6665461328621995, val_loss: 1.2686917576296577 (34 / 100)\n",
            "train_acc: 0.7787391841779975, val_acc: 0.5960591133004927, train_loss: 0.5925575749688449, val_loss: 1.3940192895569825 (35 / 100)\n",
            "train_acc: 0.7428924598269468, val_acc: 0.4876847290640394, train_loss: 0.629541956450058, val_loss: 1.3439480624175424 (36 / 100)\n",
            "train_acc: 0.7787391841779975, val_acc: 0.5566502463054187, train_loss: 0.5967750474165927, val_loss: 1.3998248553628405 (37 / 100)\n",
            "train_acc: 0.8182941903584673, val_acc: 0.5960591133004927, train_loss: 0.48476531935858047, val_loss: 1.9569936045284928 (38 / 100)\n",
            "train_acc: 0.8220024721878862, val_acc: 0.5024630541871922, train_loss: 0.4927492554314499, val_loss: 1.3334912949888578 (39 / 100)\n",
            "train_acc: 0.8022249690976514, val_acc: 0.6059113300492611, train_loss: 0.506501287258747, val_loss: 1.8142997408148103 (40 / 100)\n",
            "train_acc: 0.8405438813349815, val_acc: 0.6059113300492611, train_loss: 0.42669815863902844, val_loss: 1.3552285221409914 (41 / 100)\n",
            "train_acc: 0.8133498145859085, val_acc: 0.5763546798029556, train_loss: 0.5263565227628786, val_loss: 1.6193206606827346 (42 / 100)\n",
            "train_acc: 0.8454882571075402, val_acc: 0.541871921182266, train_loss: 0.42733485383362646, val_loss: 1.9970645141131773 (43 / 100)\n",
            "train_acc: 0.8380716934487021, val_acc: 0.6403940886699507, train_loss: 0.41092723998504754, val_loss: 2.3548815361971926 (44 / 100)\n",
            "train_acc: 0.8553770086526576, val_acc: 0.5665024630541872, train_loss: 0.3860694759413692, val_loss: 2.3114598396376436 (45 / 100)\n",
            "train_acc: 0.8813349814585909, val_acc: 0.4975369458128079, train_loss: 0.3663758264925925, val_loss: 2.935842461186677 (46 / 100)\n",
            "train_acc: 0.8751545117428925, val_acc: 0.5320197044334976, train_loss: 0.3316151152729841, val_loss: 2.0240486243675493 (47 / 100)\n",
            "train_acc: 0.8912237330037083, val_acc: 0.6009852216748769, train_loss: 0.32555463361209636, val_loss: 2.4720486490597278 (48 / 100)\n",
            "train_acc: 0.9060568603213844, val_acc: 0.5665024630541872, train_loss: 0.2718780276507176, val_loss: 2.8482806999695125 (49 / 100)\n",
            "train_acc: 0.8912237330037083, val_acc: 0.5960591133004927, train_loss: 0.3131481999961938, val_loss: 2.305279546183318 (50 / 100)\n",
            "train_acc: 0.92336217552534, val_acc: 0.645320197044335, train_loss: 0.20832140778729, val_loss: 2.1001954361282547 (51 / 100)\n",
            "train_acc: 0.9147095179233622, val_acc: 0.6157635467980296, train_loss: 0.2299972784386577, val_loss: 1.965979175614606 (52 / 100)\n",
            "train_acc: 0.9171817058096415, val_acc: 0.5862068965517241, train_loss: 0.21900170636265476, val_loss: 2.1339386871882846 (53 / 100)\n",
            "train_acc: 0.907292954264524, val_acc: 0.6009852216748769, train_loss: 0.2554233395419693, val_loss: 1.6827529991788817 (54 / 100)\n",
            "train_acc: 0.930778739184178, val_acc: 0.5812807881773399, train_loss: 0.19063158825095122, val_loss: 2.614736650964897 (55 / 100)\n",
            "train_acc: 0.9097651421508035, val_acc: 0.5517241379310345, train_loss: 0.23956616154707258, val_loss: 1.4199787724781505 (56 / 100)\n",
            "train_acc: 0.9332509270704573, val_acc: 0.6206896551724138, train_loss: 0.18820206712291326, val_loss: 2.2667982255296755 (57 / 100)\n",
            "train_acc: 0.9493201483312732, val_acc: 0.6108374384236454, train_loss: 0.12814599194544354, val_loss: 2.58874107111851 (58 / 100)\n",
            "train_acc: 0.9505562422744128, val_acc: 0.6354679802955665, train_loss: 0.13289546819198853, val_loss: 2.9451583242181485 (59 / 100)\n",
            "train_acc: 0.9035846724351051, val_acc: 0.5467980295566502, train_loss: 0.2675964487497827, val_loss: 2.9340403796416785 (60 / 100)\n",
            "train_acc: 0.9666254635352287, val_acc: 0.5714285714285714, train_loss: 0.10274416525078056, val_loss: 2.52878909980135 (61 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.5812807881773399, train_loss: 0.07449627408875228, val_loss: 2.6770895631442517 (62 / 100)\n",
            "train_acc: 0.9802224969097652, val_acc: 0.6059113300492611, train_loss: 0.06196342587323654, val_loss: 2.625586562555999 (63 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.5960591133004927, train_loss: 0.04961812393037586, val_loss: 2.768040720465148 (64 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6059113300492611, train_loss: 0.06447267915468723, val_loss: 2.909442692554643 (65 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6009852216748769, train_loss: 0.04559037844536478, val_loss: 2.8837241374800358 (66 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.6059113300492611, train_loss: 0.06077460084474278, val_loss: 3.0868928902254904 (67 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.5960591133004927, train_loss: 0.049511893865351916, val_loss: 3.2912160164029727 (68 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.5960591133004927, train_loss: 0.032521270703621055, val_loss: 3.3938420347392264 (69 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6108374384236454, train_loss: 0.04378673835796833, val_loss: 3.5022786990762342 (70 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.625615763546798, train_loss: 0.041570118242789555, val_loss: 3.2908778120144246 (71 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.625615763546798, train_loss: 0.03754236966334698, val_loss: 3.3298100626527383 (72 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6157635467980296, train_loss: 0.03705735097562132, val_loss: 3.3281776470503783 (73 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6157635467980296, train_loss: 0.03665551605861178, val_loss: 3.3833632962457063 (74 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.6108374384236454, train_loss: 0.0281858637247451, val_loss: 3.5485973311175267 (75 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.6009852216748769, train_loss: 0.024211329494330143, val_loss: 3.577732299936229 (76 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6108374384236454, train_loss: 0.046799956647073676, val_loss: 3.3829457853815237 (77 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6206896551724138, train_loss: 0.05299854720626095, val_loss: 3.3995541633643542 (78 / 100)\n",
            "train_acc: 0.992583436341162, val_acc: 0.6305418719211823, train_loss: 0.026042063097723923, val_loss: 3.4984687913227552 (79 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.6403940886699507, train_loss: 0.02216991122779799, val_loss: 3.647458262044221 (80 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.6206896551724138, train_loss: 0.026764850975998546, val_loss: 3.5573671580535438 (81 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6206896551724138, train_loss: 0.03438425771385542, val_loss: 3.5805592959737544 (82 / 100)\n",
            "train_acc: 0.9876390605686032, val_acc: 0.6354679802955665, train_loss: 0.03429327582841456, val_loss: 3.6362740652901784 (83 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.625615763546798, train_loss: 0.022312848618062965, val_loss: 3.800560356948176 (84 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6354679802955665, train_loss: 0.0310688487384169, val_loss: 3.815177010785183 (85 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.625615763546798, train_loss: 0.0246221824688434, val_loss: 3.874892011651852 (86 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.625615763546798, train_loss: 0.024877407789525054, val_loss: 4.0222052306377245 (87 / 100)\n",
            "train_acc: 0.992583436341162, val_acc: 0.6502463054187192, train_loss: 0.023034415522081447, val_loss: 3.985223145320498 (88 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.625615763546798, train_loss: 0.019148220385255568, val_loss: 3.94203638913009 (89 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.625615763546798, train_loss: 0.029150261719825096, val_loss: 3.674558780463458 (90 / 100)\n",
            "train_acc: 0.992583436341162, val_acc: 0.6305418719211823, train_loss: 0.026465249739412323, val_loss: 3.7035218530100553 (91 / 100)\n",
            "train_acc: 0.9913473423980222, val_acc: 0.6206896551724138, train_loss: 0.027051641855605307, val_loss: 3.6889150459778133 (92 / 100)\n",
            "train_acc: 0.9962917181705809, val_acc: 0.6206896551724138, train_loss: 0.0176967706609568, val_loss: 3.967735866020466 (93 / 100)\n",
            "train_acc: 0.992583436341162, val_acc: 0.6403940886699507, train_loss: 0.027850197920545806, val_loss: 3.800979367617903 (94 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6305418719211823, train_loss: 0.02554599257423203, val_loss: 3.88763540718943 (95 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.6206896551724138, train_loss: 0.019502421244702615, val_loss: 3.964420283369243 (96 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6059113300492611, train_loss: 0.038039761803795584, val_loss: 3.926831036365678 (97 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.6206896551724138, train_loss: 0.018328631322080333, val_loss: 4.264985176142801 (98 / 100)\n",
            "train_acc: 0.9950556242274413, val_acc: 0.6157635467980296, train_loss: 0.01637260846066092, val_loss: 4.451314484544575 (99 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.5911330049261084, train_loss: 0.02360446064080237, val_loss: 4.2186449079090735 (100 / 100)\n",
            "({'lr': 0.001, 'batch_size': 8, 'weight_decay': 0.001, 'gamma': 0.1}), val accuracy 0.6502463054187192, val loss 3.985223145320498\n",
            "train_acc: 0.17552533992583436, val_acc: 0.28078817733990147, train_loss: 1.7902324289562381, val_loss: 1.7831851196993749 (1 / 100)\n",
            "train_acc: 0.2027194066749073, val_acc: 0.18226600985221675, train_loss: 1.7772075885305887, val_loss: 1.759437391910647 (2 / 100)\n",
            "train_acc: 0.17058096415327564, val_acc: 0.18226600985221675, train_loss: 1.7591117599838597, val_loss: 1.7458137373618892 (3 / 100)\n",
            "train_acc: 0.2138442521631644, val_acc: 0.1921182266009852, train_loss: 1.754427653160614, val_loss: 1.7250228967572667 (4 / 100)\n",
            "train_acc: 0.22126081582200247, val_acc: 0.2857142857142857, train_loss: 1.7434814913458523, val_loss: 1.7127279366178465 (5 / 100)\n",
            "train_acc: 0.2880098887515451, val_acc: 0.2857142857142857, train_loss: 1.6841005010274785, val_loss: 1.640125358045982 (6 / 100)\n",
            "train_acc: 0.3065512978986403, val_acc: 0.2955665024630542, train_loss: 1.6261284006540795, val_loss: 1.5399653441800272 (7 / 100)\n",
            "train_acc: 0.3547589616810878, val_acc: 0.31527093596059114, train_loss: 1.5798544748162457, val_loss: 1.578779594064346 (8 / 100)\n",
            "train_acc: 0.34610630407911, val_acc: 0.33497536945812806, train_loss: 1.5520578027508314, val_loss: 1.496783615920344 (9 / 100)\n",
            "train_acc: 0.37453646477132263, val_acc: 0.2955665024630542, train_loss: 1.5230262397393604, val_loss: 1.5087274900210903 (10 / 100)\n",
            "train_acc: 0.3584672435105068, val_acc: 0.3645320197044335, train_loss: 1.4859952343114375, val_loss: 1.3949919786359288 (11 / 100)\n",
            "train_acc: 0.37824474660074164, val_acc: 0.4088669950738916, train_loss: 1.4470586275730204, val_loss: 1.4021579737733738 (12 / 100)\n",
            "train_acc: 0.3868974042027194, val_acc: 0.4236453201970443, train_loss: 1.4130148150864874, val_loss: 1.4025394035677605 (13 / 100)\n",
            "train_acc: 0.40914709517923364, val_acc: 0.3694581280788177, train_loss: 1.389194562054977, val_loss: 1.5094088597837927 (14 / 100)\n",
            "train_acc: 0.3819530284301607, val_acc: 0.43349753694581283, train_loss: 1.4505159175587525, val_loss: 1.3496663646744977 (15 / 100)\n",
            "train_acc: 0.40914709517923364, val_acc: 0.43349753694581283, train_loss: 1.3785042798121279, val_loss: 1.3469225767210786 (16 / 100)\n",
            "train_acc: 0.415327564894932, val_acc: 0.33004926108374383, train_loss: 1.3846423692255292, val_loss: 1.5028875725609916 (17 / 100)\n",
            "train_acc: 0.411619283065513, val_acc: 0.43842364532019706, train_loss: 1.3581837383277338, val_loss: 1.3255954011907718 (18 / 100)\n",
            "train_acc: 0.3943139678615575, val_acc: 0.3842364532019704, train_loss: 1.395244651436069, val_loss: 1.3398964728040648 (19 / 100)\n",
            "train_acc: 0.4326328800988875, val_acc: 0.4630541871921182, train_loss: 1.333203825431937, val_loss: 1.2588482002906611 (20 / 100)\n",
            "train_acc: 0.4684796044499382, val_acc: 0.458128078817734, train_loss: 1.3331238566281916, val_loss: 1.2923288685934884 (21 / 100)\n",
            "train_acc: 0.4363411619283066, val_acc: 0.4630541871921182, train_loss: 1.3168329706298112, val_loss: 1.2877141254876048 (22 / 100)\n",
            "train_acc: 0.449938195302843, val_acc: 0.35467980295566504, train_loss: 1.317410268653898, val_loss: 1.3283290243501147 (23 / 100)\n",
            "train_acc: 0.4622991347342398, val_acc: 0.5172413793103449, train_loss: 1.2347385766921732, val_loss: 1.2616076393080462 (24 / 100)\n",
            "train_acc: 0.47713226205191595, val_acc: 0.4876847290640394, train_loss: 1.3227594840364492, val_loss: 1.2107269593647547 (25 / 100)\n",
            "train_acc: 0.4672435105067985, val_acc: 0.4827586206896552, train_loss: 1.2359931725359377, val_loss: 1.240966909624673 (26 / 100)\n",
            "train_acc: 0.4684796044499382, val_acc: 0.5123152709359606, train_loss: 1.2486102560394627, val_loss: 1.2105556550284324 (27 / 100)\n",
            "train_acc: 0.48331273176761436, val_acc: 0.5172413793103449, train_loss: 1.2115459941668327, val_loss: 1.1821187029918427 (28 / 100)\n",
            "train_acc: 0.5265760197775031, val_acc: 0.5123152709359606, train_loss: 1.1548319290241293, val_loss: 1.2409529286652363 (29 / 100)\n",
            "train_acc: 0.5129789864029666, val_acc: 0.5221674876847291, train_loss: 1.1636625743472238, val_loss: 1.1411777802288825 (30 / 100)\n",
            "train_acc: 0.5080346106304079, val_acc: 0.5566502463054187, train_loss: 1.1828400785165605, val_loss: 1.0955397119663033 (31 / 100)\n",
            "train_acc: 0.5253399258343634, val_acc: 0.5172413793103449, train_loss: 1.1318982034295393, val_loss: 1.1427238997567464 (32 / 100)\n",
            "train_acc: 0.5426452410383189, val_acc: 0.541871921182266, train_loss: 1.1339853628456813, val_loss: 1.138973525005021 (33 / 100)\n",
            "train_acc: 0.5426452410383189, val_acc: 0.541871921182266, train_loss: 1.0991657888344104, val_loss: 1.1206553158501686 (34 / 100)\n",
            "train_acc: 0.5698393077873919, val_acc: 0.5369458128078818, train_loss: 1.0430758932170232, val_loss: 1.0727338503146995 (35 / 100)\n",
            "train_acc: 0.5661310259579728, val_acc: 0.5517241379310345, train_loss: 1.012071308305738, val_loss: 1.0851863828198662 (36 / 100)\n",
            "train_acc: 0.5908529048207664, val_acc: 0.5763546798029556, train_loss: 1.02722768391608, val_loss: 1.0340461901256017 (37 / 100)\n",
            "train_acc: 0.6032138442521632, val_acc: 0.5615763546798029, train_loss: 0.981523906049976, val_loss: 1.09841973088645 (38 / 100)\n",
            "train_acc: 0.6242274412855378, val_acc: 0.541871921182266, train_loss: 0.9435644683201322, val_loss: 1.1124424247318887 (39 / 100)\n",
            "train_acc: 0.6526576019777504, val_acc: 0.5714285714285714, train_loss: 0.8663245971476928, val_loss: 1.2045295423474804 (40 / 100)\n",
            "train_acc: 0.6254635352286774, val_acc: 0.5911330049261084, train_loss: 0.8835400870468177, val_loss: 1.011004573312299 (41 / 100)\n",
            "train_acc: 0.6650185414091471, val_acc: 0.5812807881773399, train_loss: 0.8316270224243513, val_loss: 1.0143939274285227 (42 / 100)\n",
            "train_acc: 0.7033374536464772, val_acc: 0.5467980295566502, train_loss: 0.7598995717405831, val_loss: 1.076921932215761 (43 / 100)\n",
            "train_acc: 0.7008652657601978, val_acc: 0.5911330049261084, train_loss: 0.7674669623374939, val_loss: 1.340899308179987 (44 / 100)\n",
            "train_acc: 0.6909765142150803, val_acc: 0.5172413793103449, train_loss: 0.7766417101375547, val_loss: 1.2751825683809854 (45 / 100)\n",
            "train_acc: 0.695920889987639, val_acc: 0.5714285714285714, train_loss: 0.7859187650371099, val_loss: 1.2556720948571642 (46 / 100)\n",
            "train_acc: 0.715698393077874, val_acc: 0.6009852216748769, train_loss: 0.7276496212326258, val_loss: 0.9640660561951511 (47 / 100)\n",
            "train_acc: 0.7775030902348579, val_acc: 0.5714285714285714, train_loss: 0.5830795697140311, val_loss: 1.207246011847933 (48 / 100)\n",
            "train_acc: 0.8046971569839307, val_acc: 0.6206896551724138, train_loss: 0.5068560102298322, val_loss: 1.3167539174333582 (49 / 100)\n",
            "train_acc: 0.7515451174289246, val_acc: 0.5862068965517241, train_loss: 0.6694267193967539, val_loss: 1.2973083521932216 (50 / 100)\n",
            "train_acc: 0.7898640296662547, val_acc: 0.5566502463054187, train_loss: 0.5425842058452305, val_loss: 1.3382137304015935 (51 / 100)\n",
            "train_acc: 0.8380716934487021, val_acc: 0.5960591133004927, train_loss: 0.4685095314705475, val_loss: 1.2430525460266715 (52 / 100)\n",
            "train_acc: 0.823238566131026, val_acc: 0.5714285714285714, train_loss: 0.45926096597030225, val_loss: 1.3789362326044168 (53 / 100)\n",
            "train_acc: 0.8244746600741656, val_acc: 0.6354679802955665, train_loss: 0.47539113205943917, val_loss: 1.1855713208320693 (54 / 100)\n",
            "train_acc: 0.8491965389369592, val_acc: 0.645320197044335, train_loss: 0.4020744338171149, val_loss: 1.175062914200017 (55 / 100)\n",
            "train_acc: 0.8665018541409147, val_acc: 0.6502463054187192, train_loss: 0.39468755135577455, val_loss: 1.1186855846437915 (56 / 100)\n",
            "train_acc: 0.8566131025957973, val_acc: 0.5665024630541872, train_loss: 0.38590913160032925, val_loss: 1.371974571585068 (57 / 100)\n",
            "train_acc: 0.8640296662546354, val_acc: 0.625615763546798, train_loss: 0.35152187416668434, val_loss: 1.1723287046836515 (58 / 100)\n",
            "train_acc: 0.8949320148331273, val_acc: 0.6502463054187192, train_loss: 0.3168216425055184, val_loss: 1.2734666111434034 (59 / 100)\n",
            "train_acc: 0.9184177997527813, val_acc: 0.6502463054187192, train_loss: 0.22288896132384892, val_loss: 1.8503675325750717 (60 / 100)\n",
            "train_acc: 0.9283065512978986, val_acc: 0.6896551724137931, train_loss: 0.1981514898786144, val_loss: 1.506265538666636 (61 / 100)\n",
            "train_acc: 0.9530284301606922, val_acc: 0.6650246305418719, train_loss: 0.13958557311109324, val_loss: 1.5340897702231195 (62 / 100)\n",
            "train_acc: 0.9567367119901112, val_acc: 0.6650246305418719, train_loss: 0.12613498148069763, val_loss: 1.533096134956247 (63 / 100)\n",
            "train_acc: 0.9542645241038319, val_acc: 0.6551724137931034, train_loss: 0.12126362086613335, val_loss: 1.5495809525104578 (64 / 100)\n",
            "train_acc: 0.9505562422744128, val_acc: 0.6798029556650246, train_loss: 0.12289714401562517, val_loss: 1.6670926116370215 (65 / 100)\n",
            "train_acc: 0.9592088998763906, val_acc: 0.6650246305418719, train_loss: 0.11174500813734044, val_loss: 1.6495411390154233 (66 / 100)\n",
            "train_acc: 0.9616810877626699, val_acc: 0.6896551724137931, train_loss: 0.1052793494252458, val_loss: 1.6778288915239532 (67 / 100)\n",
            "train_acc: 0.9641532756489494, val_acc: 0.6650246305418719, train_loss: 0.11550404319288704, val_loss: 1.6620369668077366 (68 / 100)\n",
            "train_acc: 0.9604449938195303, val_acc: 0.6600985221674877, train_loss: 0.09871293425707352, val_loss: 1.7123375620160783 (69 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.6600985221674877, train_loss: 0.07544083132671034, val_loss: 1.7472752215239802 (70 / 100)\n",
            "train_acc: 0.9641532756489494, val_acc: 0.6600985221674877, train_loss: 0.11137404355907764, val_loss: 1.819397855274783 (71 / 100)\n",
            "train_acc: 0.9530284301606922, val_acc: 0.6600985221674877, train_loss: 0.11025902352840335, val_loss: 1.803329513871611 (72 / 100)\n",
            "train_acc: 0.9678615574783683, val_acc: 0.6650246305418719, train_loss: 0.10108322396929685, val_loss: 1.845137270213348 (73 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.6650246305418719, train_loss: 0.07553694775841477, val_loss: 1.855618241972524 (74 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.6650246305418719, train_loss: 0.08864656895656962, val_loss: 1.848500399753965 (75 / 100)\n",
            "train_acc: 0.9666254635352287, val_acc: 0.6699507389162561, train_loss: 0.0765363664742748, val_loss: 1.9200792054237403 (76 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.6650246305418719, train_loss: 0.07300834370631959, val_loss: 1.8740544677367938 (77 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6600985221674877, train_loss: 0.07807734088997538, val_loss: 1.871677679968585 (78 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.6600985221674877, train_loss: 0.08311606809377155, val_loss: 1.8779518786322307 (79 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6600985221674877, train_loss: 0.06732259736809654, val_loss: 1.9612147391136057 (80 / 100)\n",
            "train_acc: 0.9604449938195303, val_acc: 0.6600985221674877, train_loss: 0.11851052846414196, val_loss: 1.8628940623382042 (81 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6600985221674877, train_loss: 0.0680172351754198, val_loss: 1.9023638311865294 (82 / 100)\n",
            "train_acc: 0.9666254635352287, val_acc: 0.6551724137931034, train_loss: 0.08815738401284766, val_loss: 1.979225555076975 (83 / 100)\n",
            "train_acc: 0.9567367119901112, val_acc: 0.6650246305418719, train_loss: 0.10630664078355277, val_loss: 1.9896595795166316 (84 / 100)\n",
            "train_acc: 0.9666254635352287, val_acc: 0.6502463054187192, train_loss: 0.08407784732258954, val_loss: 1.9673650393932325 (85 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.6551724137931034, train_loss: 0.0801396508469754, val_loss: 2.0345205708677545 (86 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6650246305418719, train_loss: 0.0762269295034274, val_loss: 2.024752492387894 (87 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6650246305418719, train_loss: 0.06849909728975467, val_loss: 2.1314910849914175 (88 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.6551724137931034, train_loss: 0.07680436368796822, val_loss: 2.100995264029855 (89 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.6551724137931034, train_loss: 0.08435834618688219, val_loss: 2.1633840159242377 (90 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.6748768472906403, train_loss: 0.08045362527378604, val_loss: 2.192467528023743 (91 / 100)\n",
            "train_acc: 0.9678615574783683, val_acc: 0.6699507389162561, train_loss: 0.09462899536960011, val_loss: 2.146125654281654 (92 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6600985221674877, train_loss: 0.05340742346530202, val_loss: 2.2015877545173534 (93 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.6551724137931034, train_loss: 0.07039212650528502, val_loss: 2.171542665641296 (94 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6551724137931034, train_loss: 0.04517215259179786, val_loss: 2.190579146587203 (95 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.6650246305418719, train_loss: 0.06968326282967227, val_loss: 2.235206336810671 (96 / 100)\n",
            "train_acc: 0.9802224969097652, val_acc: 0.6600985221674877, train_loss: 0.058537418377322084, val_loss: 2.1540049020879963 (97 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6650246305418719, train_loss: 0.06912334386733612, val_loss: 2.232070584602544 (98 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6600985221674877, train_loss: 0.058878566171831195, val_loss: 2.2059399382821443 (99 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6650246305418719, train_loss: 0.05782336930483027, val_loss: 2.3262458194065565 (100 / 100)\n",
            "({'lr': 0.001, 'batch_size': 16, 'weight_decay': 1e-05, 'gamma': 0.05}), val accuracy 0.6896551724137931, val loss 1.506265538666636\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7841880709043392, val_loss: 1.7739881400404305 (1 / 100)\n",
            "train_acc: 0.17058096415327564, val_acc: 0.19704433497536947, train_loss: 1.7736082601606182, val_loss: 1.756652729851859 (2 / 100)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7633950275602683, val_loss: 1.7506777506156508 (3 / 100)\n",
            "train_acc: 0.20395550061804696, val_acc: 0.18226600985221675, train_loss: 1.7596479841451562, val_loss: 1.740795163685465 (4 / 100)\n",
            "train_acc: 0.24721878862793573, val_acc: 0.2315270935960591, train_loss: 1.7331343302768005, val_loss: 1.6949066276033524 (5 / 100)\n",
            "train_acc: 0.2880098887515451, val_acc: 0.2955665024630542, train_loss: 1.6916895708430684, val_loss: 1.6076822498161805 (6 / 100)\n",
            "train_acc: 0.29913473423980225, val_acc: 0.37438423645320196, train_loss: 1.6339118268492785, val_loss: 1.5572883800920008 (7 / 100)\n",
            "train_acc: 0.34363411619283063, val_acc: 0.28078817733990147, train_loss: 1.5678360724478628, val_loss: 1.599644261627949 (8 / 100)\n",
            "train_acc: 0.34363411619283063, val_acc: 0.30049261083743845, train_loss: 1.613428169481539, val_loss: 1.6070480158763567 (9 / 100)\n",
            "train_acc: 0.34363411619283063, val_acc: 0.23645320197044334, train_loss: 1.5675158259011022, val_loss: 1.5924095940120115 (10 / 100)\n",
            "train_acc: 0.34610630407911, val_acc: 0.35467980295566504, train_loss: 1.5258345200193237, val_loss: 1.4393238357722467 (11 / 100)\n",
            "train_acc: 0.3831891223733004, val_acc: 0.29064039408866993, train_loss: 1.4781518505589186, val_loss: 1.541627468733952 (12 / 100)\n",
            "train_acc: 0.3695920889987639, val_acc: 0.39408866995073893, train_loss: 1.4601331101949195, val_loss: 1.4462276826351148 (13 / 100)\n",
            "train_acc: 0.380716934487021, val_acc: 0.35467980295566504, train_loss: 1.4623422299975666, val_loss: 1.5141282522032413 (14 / 100)\n",
            "train_acc: 0.38442521631644005, val_acc: 0.4630541871921182, train_loss: 1.446262398815273, val_loss: 1.383483437775391 (15 / 100)\n",
            "train_acc: 0.3720642768850433, val_acc: 0.4039408866995074, train_loss: 1.464873066201935, val_loss: 1.402455912434996 (16 / 100)\n",
            "train_acc: 0.4042027194066749, val_acc: 0.39408866995073893, train_loss: 1.4074349540420457, val_loss: 1.373695257849294 (17 / 100)\n",
            "train_acc: 0.3868974042027194, val_acc: 0.37438423645320196, train_loss: 1.4251198454751957, val_loss: 1.4811132447472934 (18 / 100)\n",
            "train_acc: 0.41656365883807167, val_acc: 0.47783251231527096, train_loss: 1.3729226863281099, val_loss: 1.3504145503631366 (19 / 100)\n",
            "train_acc: 0.41656365883807167, val_acc: 0.45320197044334976, train_loss: 1.3526217221329622, val_loss: 1.313427634133494 (20 / 100)\n",
            "train_acc: 0.42027194066749074, val_acc: 0.4039408866995074, train_loss: 1.3936786775388588, val_loss: 1.3808027059573846 (21 / 100)\n",
            "train_acc: 0.43016069221260816, val_acc: 0.46798029556650245, train_loss: 1.317288386070242, val_loss: 1.3792884026842165 (22 / 100)\n",
            "train_acc: 0.45241038318912236, val_acc: 0.4236453201970443, train_loss: 1.3186112555349419, val_loss: 1.3770210860398016 (23 / 100)\n",
            "train_acc: 0.44252163164400493, val_acc: 0.43349753694581283, train_loss: 1.3496094463192783, val_loss: 1.4463787583881997 (24 / 100)\n",
            "train_acc: 0.44128553770086526, val_acc: 0.4482758620689655, train_loss: 1.3365887735034392, val_loss: 1.2739695240124105 (25 / 100)\n",
            "train_acc: 0.4610630407911001, val_acc: 0.47783251231527096, train_loss: 1.2728945858251621, val_loss: 1.2374595855844432 (26 / 100)\n",
            "train_acc: 0.44870210135970334, val_acc: 0.4088669950738916, train_loss: 1.2609790445700269, val_loss: 1.3180155067021035 (27 / 100)\n",
            "train_acc: 0.46600741656365885, val_acc: 0.4876847290640394, train_loss: 1.2278818646526455, val_loss: 1.2446118640194972 (28 / 100)\n",
            "train_acc: 0.5006180469715699, val_acc: 0.4039408866995074, train_loss: 1.2230288142474237, val_loss: 1.3107438337039479 (29 / 100)\n",
            "train_acc: 0.4857849196538937, val_acc: 0.43842364532019706, train_loss: 1.1892061575381512, val_loss: 1.3373286183831727 (30 / 100)\n",
            "train_acc: 0.5043263288009888, val_acc: 0.46798029556650245, train_loss: 1.1893803217944463, val_loss: 1.2888086030048689 (31 / 100)\n",
            "train_acc: 0.5166872682323856, val_acc: 0.46798029556650245, train_loss: 1.1722391684653586, val_loss: 1.2098256719523464 (32 / 100)\n",
            "train_acc: 0.5377008652657602, val_acc: 0.4975369458128079, train_loss: 1.1076346010301257, val_loss: 1.1810872105542074 (33 / 100)\n",
            "train_acc: 0.5364647713226205, val_acc: 0.5467980295566502, train_loss: 1.1119948279282956, val_loss: 1.1109554908545733 (34 / 100)\n",
            "train_acc: 0.5661310259579728, val_acc: 0.43842364532019706, train_loss: 1.063453614048669, val_loss: 1.340432355850201 (35 / 100)\n",
            "train_acc: 0.5636588380716935, val_acc: 0.5270935960591133, train_loss: 1.0799030912821312, val_loss: 1.1919463693801993 (36 / 100)\n",
            "train_acc: 0.5859085290482077, val_acc: 0.5615763546798029, train_loss: 1.0258674550115399, val_loss: 1.1223627240787 (37 / 100)\n",
            "train_acc: 0.6093943139678616, val_acc: 0.46798029556650245, train_loss: 0.9507650926764433, val_loss: 1.262927376578007 (38 / 100)\n",
            "train_acc: 0.584672435105068, val_acc: 0.4876847290640394, train_loss: 1.0171812665182522, val_loss: 1.1597252326645875 (39 / 100)\n",
            "train_acc: 0.6378244746600742, val_acc: 0.5566502463054187, train_loss: 0.9025413643002362, val_loss: 1.0797137494744926 (40 / 100)\n",
            "train_acc: 0.6501854140914709, val_acc: 0.5812807881773399, train_loss: 0.8778074462569982, val_loss: 1.1052001030574292 (41 / 100)\n",
            "train_acc: 0.6316440049443758, val_acc: 0.5320197044334976, train_loss: 0.9383847786042245, val_loss: 1.3139599039049572 (42 / 100)\n",
            "train_acc: 0.7021013597033374, val_acc: 0.4975369458128079, train_loss: 0.7787504076810349, val_loss: 1.2536556139368142 (43 / 100)\n",
            "train_acc: 0.6983930778739185, val_acc: 0.49261083743842365, train_loss: 0.7527857386726974, val_loss: 1.1727508782165978 (44 / 100)\n",
            "train_acc: 0.7107540173053152, val_acc: 0.4827586206896552, train_loss: 0.7493545905179235, val_loss: 1.2253646219305216 (45 / 100)\n",
            "train_acc: 0.7243510506798516, val_acc: 0.5862068965517241, train_loss: 0.6869368658519646, val_loss: 1.1435968658606994 (46 / 100)\n",
            "train_acc: 0.7713226205191595, val_acc: 0.5763546798029556, train_loss: 0.6376128515000691, val_loss: 1.1375616225115772 (47 / 100)\n",
            "train_acc: 0.7639060568603214, val_acc: 0.5517241379310345, train_loss: 0.5967512858665476, val_loss: 1.255990255642407 (48 / 100)\n",
            "train_acc: 0.788627935723115, val_acc: 0.5714285714285714, train_loss: 0.5349405740004387, val_loss: 1.3301133393066857 (49 / 100)\n",
            "train_acc: 0.7935723114956736, val_acc: 0.5615763546798029, train_loss: 0.5530817340714086, val_loss: 1.2229744213555247 (50 / 100)\n",
            "train_acc: 0.8046971569839307, val_acc: 0.49261083743842365, train_loss: 0.5109994351053415, val_loss: 1.1970506787593729 (51 / 100)\n",
            "train_acc: 0.8182941903584673, val_acc: 0.6009852216748769, train_loss: 0.460841009510757, val_loss: 1.5524489282093612 (52 / 100)\n",
            "train_acc: 0.8145859085290482, val_acc: 0.6009852216748769, train_loss: 0.4917443971183008, val_loss: 1.1387971903890224 (53 / 100)\n",
            "train_acc: 0.8368355995055624, val_acc: 0.6059113300492611, train_loss: 0.4308929453305762, val_loss: 1.509889430600434 (54 / 100)\n",
            "train_acc: 0.8244746600741656, val_acc: 0.6108374384236454, train_loss: 0.4727052369503804, val_loss: 1.4776286529790004 (55 / 100)\n",
            "train_acc: 0.8590852904820766, val_acc: 0.6108374384236454, train_loss: 0.3471510104549535, val_loss: 1.2665232555032364 (56 / 100)\n",
            "train_acc: 0.8949320148331273, val_acc: 0.6403940886699507, train_loss: 0.3106890932296498, val_loss: 1.0692152219452882 (57 / 100)\n",
            "train_acc: 0.8936959208899876, val_acc: 0.6108374384236454, train_loss: 0.2895925848092078, val_loss: 1.5593960696253284 (58 / 100)\n",
            "train_acc: 0.8467243510506799, val_acc: 0.6157635467980296, train_loss: 0.40870870471148024, val_loss: 1.155804151971939 (59 / 100)\n",
            "train_acc: 0.8862793572311496, val_acc: 0.6206896551724138, train_loss: 0.30334262673286044, val_loss: 1.6196881691223295 (60 / 100)\n",
            "train_acc: 0.930778739184178, val_acc: 0.6502463054187192, train_loss: 0.19895654908935131, val_loss: 1.466872737325471 (61 / 100)\n",
            "train_acc: 0.9555006180469716, val_acc: 0.6551724137931034, train_loss: 0.1284004113854488, val_loss: 1.4976085066208111 (62 / 100)\n",
            "train_acc: 0.9419035846724351, val_acc: 0.6600985221674877, train_loss: 0.1399857658796169, val_loss: 1.5069911397736648 (63 / 100)\n",
            "train_acc: 0.9629171817058096, val_acc: 0.6699507389162561, train_loss: 0.10702224197361171, val_loss: 1.6346570229882678 (64 / 100)\n",
            "train_acc: 0.9616810877626699, val_acc: 0.6650246305418719, train_loss: 0.10716731144502133, val_loss: 1.6504517118331834 (65 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.6748768472906403, train_loss: 0.07673982237966452, val_loss: 1.7572315531998433 (66 / 100)\n",
            "train_acc: 0.9616810877626699, val_acc: 0.6502463054187192, train_loss: 0.1100878747656702, val_loss: 1.7801022940668567 (67 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.6847290640394089, train_loss: 0.08533622188164365, val_loss: 1.722552746387538 (68 / 100)\n",
            "train_acc: 0.9604449938195303, val_acc: 0.6650246305418719, train_loss: 0.09462484045987961, val_loss: 1.7178171180151953 (69 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.6896551724137931, train_loss: 0.09042448991603404, val_loss: 1.7598839463858769 (70 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6847290640394089, train_loss: 0.08594987728047025, val_loss: 1.7460551784543568 (71 / 100)\n",
            "train_acc: 0.965389369592089, val_acc: 0.6945812807881774, train_loss: 0.10766244354589907, val_loss: 1.8404957754858609 (72 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.6847290640394089, train_loss: 0.08691272835767562, val_loss: 1.7615350211782408 (73 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6699507389162561, train_loss: 0.06774866566055067, val_loss: 1.9096000329614273 (74 / 100)\n",
            "train_acc: 0.965389369592089, val_acc: 0.6748768472906403, train_loss: 0.08643725333932202, val_loss: 1.9587731966244175 (75 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.6798029556650246, train_loss: 0.0828707570276869, val_loss: 1.9759152558049544 (76 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.6748768472906403, train_loss: 0.07905176747935665, val_loss: 1.9695899938714916 (77 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.6847290640394089, train_loss: 0.09022805614141363, val_loss: 1.9204748739749926 (78 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6945812807881774, train_loss: 0.06665858409133808, val_loss: 1.823059764988904 (79 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6847290640394089, train_loss: 0.07082848232446701, val_loss: 1.862157896821722 (80 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6995073891625616, train_loss: 0.05167390980811879, val_loss: 1.9537365324978757 (81 / 100)\n",
            "train_acc: 0.9604449938195303, val_acc: 0.6847290640394089, train_loss: 0.0981361656386419, val_loss: 1.9441161478681517 (82 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6748768472906403, train_loss: 0.06426758553927168, val_loss: 2.0166720286965956 (83 / 100)\n",
            "train_acc: 0.9678615574783683, val_acc: 0.6798029556650246, train_loss: 0.0886814321479927, val_loss: 1.9821416585903449 (84 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6748768472906403, train_loss: 0.051826592426567494, val_loss: 1.9942022291897552 (85 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.6896551724137931, train_loss: 0.07439307509262427, val_loss: 1.9492201993030867 (86 / 100)\n",
            "train_acc: 0.9802224969097652, val_acc: 0.6847290640394089, train_loss: 0.06431830149461666, val_loss: 1.9841942652105697 (87 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6748768472906403, train_loss: 0.06190505531721268, val_loss: 1.9705117494601923 (88 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6699507389162561, train_loss: 0.05959133829631381, val_loss: 1.9507587566751565 (89 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.6748768472906403, train_loss: 0.08889030115345058, val_loss: 1.9196075623845819 (90 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6798029556650246, train_loss: 0.041463987788845115, val_loss: 1.9813429093713244 (91 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6650246305418719, train_loss: 0.0638764781373172, val_loss: 1.9432598287836085 (92 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6748768472906403, train_loss: 0.051735701451563865, val_loss: 2.051260568238244 (93 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6699507389162561, train_loss: 0.06016335281807059, val_loss: 2.0592323341980356 (94 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.6847290640394089, train_loss: 0.06435864968559209, val_loss: 2.1125612059250254 (95 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6748768472906403, train_loss: 0.06391498700419743, val_loss: 2.094337347693044 (96 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6798029556650246, train_loss: 0.06963940648123094, val_loss: 2.1031951176121906 (97 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.6699507389162561, train_loss: 0.06590437152416058, val_loss: 2.0948653632196885 (98 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.6847290640394089, train_loss: 0.06180818576011139, val_loss: 2.1176699811014634 (99 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6551724137931034, train_loss: 0.049955400017496515, val_loss: 2.172750768403114 (100 / 100)\n",
            "({'lr': 0.001, 'batch_size': 16, 'weight_decay': 1e-05, 'gamma': 0.1}), val accuracy 0.6995073891625616, val loss 1.9537365324978757\n",
            "train_acc: 0.2088998763906057, val_acc: 0.18226600985221675, train_loss: 1.7831404161983722, val_loss: 1.7705621983617397 (1 / 100)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7682728026205, val_loss: 1.7516744189661713 (2 / 100)\n",
            "train_acc: 0.2126081582200247, val_acc: 0.2561576354679803, train_loss: 1.7600891570962405, val_loss: 1.754685591594339 (3 / 100)\n",
            "train_acc: 0.23485784919653893, val_acc: 0.2512315270935961, train_loss: 1.7531537106364266, val_loss: 1.7364868553988453 (4 / 100)\n",
            "train_acc: 0.27935723114956734, val_acc: 0.3103448275862069, train_loss: 1.7047248926976113, val_loss: 1.6237360855628704 (5 / 100)\n",
            "train_acc: 0.3164400494437577, val_acc: 0.2413793103448276, train_loss: 1.6816647225168933, val_loss: 1.5830282589484905 (6 / 100)\n",
            "train_acc: 0.29295426452410384, val_acc: 0.2413793103448276, train_loss: 1.6278915875332318, val_loss: 1.6416205044450431 (7 / 100)\n",
            "train_acc: 0.34239802224969096, val_acc: 0.30049261083743845, train_loss: 1.5837083584888019, val_loss: 1.6597708363838384 (8 / 100)\n",
            "train_acc: 0.3362175525339926, val_acc: 0.35960591133004927, train_loss: 1.5676575442327114, val_loss: 1.4988961337235174 (9 / 100)\n",
            "train_acc: 0.35970333745364647, val_acc: 0.3054187192118227, train_loss: 1.5137566120279735, val_loss: 1.648766269824775 (10 / 100)\n",
            "train_acc: 0.37330037082818296, val_acc: 0.458128078817734, train_loss: 1.4773683570076714, val_loss: 1.4263457142073532 (11 / 100)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.3694581280788177, train_loss: 1.459747477721225, val_loss: 1.4248015099558338 (12 / 100)\n",
            "train_acc: 0.3683559950556242, val_acc: 0.39901477832512317, train_loss: 1.429874653279855, val_loss: 1.3713982939132916 (13 / 100)\n",
            "train_acc: 0.4079110012360939, val_acc: 0.3891625615763547, train_loss: 1.4384183039329255, val_loss: 1.362556317756916 (14 / 100)\n",
            "train_acc: 0.36093943139678614, val_acc: 0.37438423645320196, train_loss: 1.437684490889937, val_loss: 1.4632418044094968 (15 / 100)\n",
            "train_acc: 0.39184177997527814, val_acc: 0.3793103448275862, train_loss: 1.3964696018893286, val_loss: 1.3803424735374639 (16 / 100)\n",
            "train_acc: 0.3992583436341162, val_acc: 0.4187192118226601, train_loss: 1.3885958875507596, val_loss: 1.3453176506047178 (17 / 100)\n",
            "train_acc: 0.3943139678615575, val_acc: 0.4482758620689655, train_loss: 1.3882736554104553, val_loss: 1.425066293166776 (18 / 100)\n",
            "train_acc: 0.40296662546353523, val_acc: 0.4827586206896552, train_loss: 1.3873857616346759, val_loss: 1.4194481930709237 (19 / 100)\n",
            "train_acc: 0.3967861557478368, val_acc: 0.4187192118226601, train_loss: 1.3880404516851945, val_loss: 1.340086808345588 (20 / 100)\n",
            "train_acc: 0.45982694684796044, val_acc: 0.4827586206896552, train_loss: 1.30467900151228, val_loss: 1.2806376905864096 (21 / 100)\n",
            "train_acc: 0.446229913473424, val_acc: 0.458128078817734, train_loss: 1.3113855348971335, val_loss: 1.3635989345353225 (22 / 100)\n",
            "train_acc: 0.4573547589616811, val_acc: 0.35960591133004927, train_loss: 1.2859186077000038, val_loss: 1.4230306524361296 (23 / 100)\n",
            "train_acc: 0.4721878862793572, val_acc: 0.49261083743842365, train_loss: 1.2690815710461478, val_loss: 1.195765158812988 (24 / 100)\n",
            "train_acc: 0.4721878862793572, val_acc: 0.49261083743842365, train_loss: 1.2575254791009853, val_loss: 1.2158611014558764 (25 / 100)\n",
            "train_acc: 0.4956736711990111, val_acc: 0.49261083743842365, train_loss: 1.1937289716876187, val_loss: 1.170752451337617 (26 / 100)\n",
            "train_acc: 0.4796044499381953, val_acc: 0.4433497536945813, train_loss: 1.2377553055401342, val_loss: 1.2278347097594162 (27 / 100)\n",
            "train_acc: 0.515451174289246, val_acc: 0.5172413793103449, train_loss: 1.1923300873658567, val_loss: 1.1495513187840654 (28 / 100)\n",
            "train_acc: 0.5241038318912238, val_acc: 0.5024630541871922, train_loss: 1.171164071604112, val_loss: 1.1429874098359658 (29 / 100)\n",
            "train_acc: 0.519159456118665, val_acc: 0.4975369458128079, train_loss: 1.1592178751423274, val_loss: 1.2097807459056085 (30 / 100)\n",
            "train_acc: 0.5784919653893696, val_acc: 0.5467980295566502, train_loss: 1.065443887698783, val_loss: 1.1093724919070165 (31 / 100)\n",
            "train_acc: 0.5624227441285538, val_acc: 0.5270935960591133, train_loss: 1.0834863123993939, val_loss: 1.1602535362314121 (32 / 100)\n",
            "train_acc: 0.5747836835599506, val_acc: 0.5615763546798029, train_loss: 1.04491612024154, val_loss: 1.1155296490697437 (33 / 100)\n",
            "train_acc: 0.5859085290482077, val_acc: 0.5270935960591133, train_loss: 1.0381728197794466, val_loss: 1.1273985243783209 (34 / 100)\n",
            "train_acc: 0.5760197775030902, val_acc: 0.5369458128078818, train_loss: 0.9932363501143249, val_loss: 1.1510787336109893 (35 / 100)\n",
            "train_acc: 0.6328800988875154, val_acc: 0.5172413793103449, train_loss: 0.9202169386948584, val_loss: 1.1662526433103777 (36 / 100)\n",
            "train_acc: 0.6316440049443758, val_acc: 0.5172413793103449, train_loss: 0.9177990037814354, val_loss: 1.2102517338809122 (37 / 100)\n",
            "train_acc: 0.6056860321384425, val_acc: 0.5714285714285714, train_loss: 0.9593724945124943, val_loss: 1.071578031103012 (38 / 100)\n",
            "train_acc: 0.69221260815822, val_acc: 0.5320197044334976, train_loss: 0.7982118671992506, val_loss: 1.3423616342943878 (39 / 100)\n",
            "train_acc: 0.6390605686032138, val_acc: 0.5517241379310345, train_loss: 0.8982314202929899, val_loss: 1.1026394123514298 (40 / 100)\n",
            "train_acc: 0.7058096415327565, val_acc: 0.5073891625615764, train_loss: 0.7491900935695405, val_loss: 1.4454392047938456 (41 / 100)\n",
            "train_acc: 0.7218788627935723, val_acc: 0.5862068965517241, train_loss: 0.7026038631078486, val_loss: 1.0651068106073465 (42 / 100)\n",
            "train_acc: 0.7404202719406675, val_acc: 0.5911330049261084, train_loss: 0.6838866758110791, val_loss: 1.0567239734339597 (43 / 100)\n",
            "train_acc: 0.7218788627935723, val_acc: 0.6059113300492611, train_loss: 0.7358938476579593, val_loss: 1.1037945671034564 (44 / 100)\n",
            "train_acc: 0.7688504326328801, val_acc: 0.5517241379310345, train_loss: 0.5923467042714321, val_loss: 1.101405455561107 (45 / 100)\n",
            "train_acc: 0.7799752781211372, val_acc: 0.6009852216748769, train_loss: 0.5879717210240948, val_loss: 1.270216011648695 (46 / 100)\n",
            "train_acc: 0.7948084054388134, val_acc: 0.5467980295566502, train_loss: 0.5521385159863824, val_loss: 1.3849222721724674 (47 / 100)\n",
            "train_acc: 0.7750309023485785, val_acc: 0.5665024630541872, train_loss: 0.5560365664796865, val_loss: 1.2456552629987594 (48 / 100)\n",
            "train_acc: 0.8590852904820766, val_acc: 0.5270935960591133, train_loss: 0.4281946413670246, val_loss: 1.873525101563026 (49 / 100)\n",
            "train_acc: 0.8158220024721878, val_acc: 0.5714285714285714, train_loss: 0.4810503103824422, val_loss: 1.6627089965519646 (50 / 100)\n",
            "train_acc: 0.8207663782447466, val_acc: 0.5615763546798029, train_loss: 0.4710262078326771, val_loss: 1.4578336829622391 (51 / 100)\n",
            "train_acc: 0.8899876390605687, val_acc: 0.5665024630541872, train_loss: 0.3027664215772497, val_loss: 1.661786505741439 (52 / 100)\n",
            "train_acc: 0.8739184177997528, val_acc: 0.6059113300492611, train_loss: 0.35234901192972184, val_loss: 1.5895998125593063 (53 / 100)\n",
            "train_acc: 0.8491965389369592, val_acc: 0.5911330049261084, train_loss: 0.38813326458995806, val_loss: 1.5604634032460856 (54 / 100)\n",
            "train_acc: 0.8949320148331273, val_acc: 0.6206896551724138, train_loss: 0.28840299803482733, val_loss: 1.683601600195974 (55 / 100)\n",
            "train_acc: 0.8726823238566132, val_acc: 0.5615763546798029, train_loss: 0.3344923246776218, val_loss: 2.0200357031939653 (56 / 100)\n",
            "train_acc: 0.9097651421508035, val_acc: 0.5763546798029556, train_loss: 0.26048933144552894, val_loss: 1.6870829304450838 (57 / 100)\n",
            "train_acc: 0.9048207663782447, val_acc: 0.6157635467980296, train_loss: 0.2677128919122835, val_loss: 1.3865154917016993 (58 / 100)\n",
            "train_acc: 0.9184177997527813, val_acc: 0.625615763546798, train_loss: 0.264824509031251, val_loss: 1.6024512551688208 (59 / 100)\n",
            "train_acc: 0.9110012360939431, val_acc: 0.6009852216748769, train_loss: 0.24082680269431125, val_loss: 1.6917385632181403 (60 / 100)\n",
            "train_acc: 0.9592088998763906, val_acc: 0.6650246305418719, train_loss: 0.1183023128948636, val_loss: 1.6425495687963927 (61 / 100)\n",
            "train_acc: 0.9616810877626699, val_acc: 0.6600985221674877, train_loss: 0.10498065118331402, val_loss: 1.7021234114769057 (62 / 100)\n",
            "train_acc: 0.965389369592089, val_acc: 0.6600985221674877, train_loss: 0.10194993034760354, val_loss: 1.7629617528962385 (63 / 100)\n",
            "train_acc: 0.9616810877626699, val_acc: 0.6600985221674877, train_loss: 0.08574113447785525, val_loss: 1.730633448497415 (64 / 100)\n",
            "train_acc: 0.9678615574783683, val_acc: 0.645320197044335, train_loss: 0.08136918708884672, val_loss: 1.8116523908276863 (65 / 100)\n",
            "train_acc: 0.9604449938195303, val_acc: 0.645320197044335, train_loss: 0.11271703272265911, val_loss: 1.7596244773841256 (66 / 100)\n",
            "train_acc: 0.9616810877626699, val_acc: 0.6699507389162561, train_loss: 0.09386901266421611, val_loss: 1.7874652355762537 (67 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6699507389162561, train_loss: 0.08477819584855725, val_loss: 1.7938506409452466 (68 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6699507389162561, train_loss: 0.08552226899440685, val_loss: 1.8427916219081786 (69 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.6748768472906403, train_loss: 0.07851963608936673, val_loss: 1.899099320613692 (70 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6650246305418719, train_loss: 0.06482917490577594, val_loss: 1.9425382549539576 (71 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6650246305418719, train_loss: 0.064441224110741, val_loss: 1.9299999264073489 (72 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6650246305418719, train_loss: 0.05831695561457255, val_loss: 1.9769619779633771 (73 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.6699507389162561, train_loss: 0.08032967902637088, val_loss: 1.968437091176733 (74 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6699507389162561, train_loss: 0.06273104175027316, val_loss: 1.9807367982535526 (75 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.6699507389162561, train_loss: 0.07784015389230696, val_loss: 1.9982910696508849 (76 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.6650246305418719, train_loss: 0.0619279576536862, val_loss: 2.0838503038941933 (77 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6650246305418719, train_loss: 0.045443727258404044, val_loss: 2.118516559084061 (78 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.6551724137931034, train_loss: 0.06448804595385654, val_loss: 2.0884812688592618 (79 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6600985221674877, train_loss: 0.04103385370825642, val_loss: 2.121659063940565 (80 / 100)\n",
            "train_acc: 0.9901112484548825, val_acc: 0.6650246305418719, train_loss: 0.04260682882882116, val_loss: 2.180244135151943 (81 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6650246305418719, train_loss: 0.046264333210710544, val_loss: 2.2209863051992333 (82 / 100)\n",
            "train_acc: 0.9802224969097652, val_acc: 0.645320197044335, train_loss: 0.06153793483788357, val_loss: 2.1850925478441963 (83 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6699507389162561, train_loss: 0.0582988749894835, val_loss: 2.257726914776957 (84 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6551724137931034, train_loss: 0.05725004926452825, val_loss: 2.287060201461679 (85 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6600985221674877, train_loss: 0.05188360202445088, val_loss: 2.2703502078361697 (86 / 100)\n",
            "train_acc: 0.9938195302843016, val_acc: 0.6600985221674877, train_loss: 0.026964445638931408, val_loss: 2.30942202965027 (87 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6551724137931034, train_loss: 0.055833553648997, val_loss: 2.3568775301496383 (88 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6551724137931034, train_loss: 0.040698854384005036, val_loss: 2.3593940617415705 (89 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6600985221674877, train_loss: 0.04915179796644247, val_loss: 2.370829969204118 (90 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6502463054187192, train_loss: 0.059410169776051326, val_loss: 2.399216923220404 (91 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.645320197044335, train_loss: 0.04649731314140147, val_loss: 2.4706736527053006 (92 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6551724137931034, train_loss: 0.0356550428858637, val_loss: 2.467560160923474 (93 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6798029556650246, train_loss: 0.07053460830037467, val_loss: 2.406967087332251 (94 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6650246305418719, train_loss: 0.0501769649966278, val_loss: 2.340714565638838 (95 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.6650246305418719, train_loss: 0.06778342228000626, val_loss: 2.3905783979763537 (96 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6502463054187192, train_loss: 0.03994192771512292, val_loss: 2.3585257817958962 (97 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.645320197044335, train_loss: 0.0445087459882346, val_loss: 2.395776771559504 (98 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.6650246305418719, train_loss: 0.07648252999716253, val_loss: 2.363981010878615 (99 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6600985221674877, train_loss: 0.03679075347644142, val_loss: 2.427178770450536 (100 / 100)\n",
            "({'lr': 0.001, 'batch_size': 16, 'weight_decay': 0.001, 'gamma': 0.05}), val accuracy 0.6798029556650246, val loss 2.406967087332251\n",
            "train_acc: 0.19530284301606923, val_acc: 0.22660098522167488, train_loss: 1.7855405201870667, val_loss: 1.7686893393840697 (1 / 100)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7655226863064195, val_loss: 1.750889197946182 (2 / 100)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.32019704433497537, train_loss: 1.75630616699662, val_loss: 1.741050796555768 (3 / 100)\n",
            "train_acc: 0.19901112484548825, val_acc: 0.2413793103448276, train_loss: 1.7517467278927308, val_loss: 1.7161011895522695 (4 / 100)\n",
            "train_acc: 0.23856613102595797, val_acc: 0.2512315270935961, train_loss: 1.7116753813362828, val_loss: 1.6764515849757078 (5 / 100)\n",
            "train_acc: 0.2904820766378245, val_acc: 0.3103448275862069, train_loss: 1.6592213456209275, val_loss: 1.5889328271884637 (6 / 100)\n",
            "train_acc: 0.28059332509270707, val_acc: 0.28078817733990147, train_loss: 1.687496948301129, val_loss: 1.5733275460492213 (7 / 100)\n",
            "train_acc: 0.3300370828182942, val_acc: 0.3251231527093596, train_loss: 1.5912676573683806, val_loss: 1.5713328786671454 (8 / 100)\n",
            "train_acc: 0.33498145859085293, val_acc: 0.45320197044334976, train_loss: 1.5371168873955499, val_loss: 1.476784875240232 (9 / 100)\n",
            "train_acc: 0.3362175525339926, val_acc: 0.27586206896551724, train_loss: 1.5416137299814685, val_loss: 1.5711522607380533 (10 / 100)\n",
            "train_acc: 0.3572311495673671, val_acc: 0.3891625615763547, train_loss: 1.4981150721442713, val_loss: 1.3992841924939836 (11 / 100)\n",
            "train_acc: 0.34487021013597036, val_acc: 0.3842364532019704, train_loss: 1.4859829656273238, val_loss: 1.464575834755827 (12 / 100)\n",
            "train_acc: 0.3547589616810878, val_acc: 0.3399014778325123, train_loss: 1.4810713270243963, val_loss: 1.3994745870529137 (13 / 100)\n",
            "train_acc: 0.377008652657602, val_acc: 0.3891625615763547, train_loss: 1.4494740572494393, val_loss: 1.4333257387424339 (14 / 100)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.4236453201970443, train_loss: 1.4222113981529867, val_loss: 1.3236381843172271 (15 / 100)\n",
            "train_acc: 0.39184177997527814, val_acc: 0.43349753694581283, train_loss: 1.4214078448022105, val_loss: 1.3051769140318696 (16 / 100)\n",
            "train_acc: 0.41409147095179233, val_acc: 0.3251231527093596, train_loss: 1.3640280506666864, val_loss: 1.4269531941766223 (17 / 100)\n",
            "train_acc: 0.41409147095179233, val_acc: 0.4433497536945813, train_loss: 1.3994272461043595, val_loss: 1.2608409620858179 (18 / 100)\n",
            "train_acc: 0.4276885043263288, val_acc: 0.4433497536945813, train_loss: 1.3398996529679363, val_loss: 1.2888420083252667 (19 / 100)\n",
            "train_acc: 0.44746600741656367, val_acc: 0.4433497536945813, train_loss: 1.3268917190424443, val_loss: 1.2409324381739049 (20 / 100)\n",
            "train_acc: 0.45859085290482077, val_acc: 0.49261083743842365, train_loss: 1.3083020579829647, val_loss: 1.3015951834288724 (21 / 100)\n",
            "train_acc: 0.4746600741656366, val_acc: 0.4827586206896552, train_loss: 1.287365431691277, val_loss: 1.2136395059782883 (22 / 100)\n",
            "train_acc: 0.4684796044499382, val_acc: 0.4482758620689655, train_loss: 1.2943029898058793, val_loss: 1.2534116283426144 (23 / 100)\n",
            "train_acc: 0.4511742892459827, val_acc: 0.4975369458128079, train_loss: 1.2814833367858152, val_loss: 1.1769659536812693 (24 / 100)\n",
            "train_acc: 0.48702101359703337, val_acc: 0.5221674876847291, train_loss: 1.1976273223407778, val_loss: 1.2088796511072244 (25 / 100)\n",
            "train_acc: 0.5129789864029666, val_acc: 0.4876847290640394, train_loss: 1.1675606992542376, val_loss: 1.317612458332419 (26 / 100)\n",
            "train_acc: 0.522867737948084, val_acc: 0.5221674876847291, train_loss: 1.1773173698536104, val_loss: 1.186978963501935 (27 / 100)\n",
            "train_acc: 0.5030902348578492, val_acc: 0.4088669950738916, train_loss: 1.1849176768761482, val_loss: 1.370314096582347 (28 / 100)\n",
            "train_acc: 0.5216316440049443, val_acc: 0.4827586206896552, train_loss: 1.1449236223665245, val_loss: 1.1714074482471484 (29 / 100)\n",
            "train_acc: 0.5438813349814586, val_acc: 0.5024630541871922, train_loss: 1.0941994082647732, val_loss: 1.1656810846822014 (30 / 100)\n",
            "train_acc: 0.5673671199011124, val_acc: 0.5615763546798029, train_loss: 1.0305360725105766, val_loss: 1.1333928924475984 (31 / 100)\n",
            "train_acc: 0.5834363411619283, val_acc: 0.5123152709359606, train_loss: 0.9945916865753155, val_loss: 1.2679693607861184 (32 / 100)\n",
            "train_acc: 0.5624227441285538, val_acc: 0.5320197044334976, train_loss: 1.0854253076357658, val_loss: 1.0924386637551444 (33 / 100)\n",
            "train_acc: 0.622991347342398, val_acc: 0.5270935960591133, train_loss: 0.9368780404881877, val_loss: 1.2761254140308924 (34 / 100)\n",
            "train_acc: 0.6180469715698393, val_acc: 0.5763546798029556, train_loss: 0.9583623260443821, val_loss: 1.09542571265122 (35 / 100)\n",
            "train_acc: 0.6316440049443758, val_acc: 0.43349753694581283, train_loss: 0.9018494822038856, val_loss: 1.5097757624875148 (36 / 100)\n",
            "train_acc: 0.6402966625463535, val_acc: 0.5566502463054187, train_loss: 0.9349391710920298, val_loss: 1.1145448038730714 (37 / 100)\n",
            "train_acc: 0.6613102595797281, val_acc: 0.5123152709359606, train_loss: 0.8330687910723598, val_loss: 1.2098380896845475 (38 / 100)\n",
            "train_acc: 0.6749072929542645, val_acc: 0.5665024630541872, train_loss: 0.7994468487679443, val_loss: 1.1908471293637317 (39 / 100)\n",
            "train_acc: 0.7021013597033374, val_acc: 0.5763546798029556, train_loss: 0.7308433553933803, val_loss: 1.247300153295395 (40 / 100)\n",
            "train_acc: 0.723114956736712, val_acc: 0.5960591133004927, train_loss: 0.726198678110969, val_loss: 1.336607545467433 (41 / 100)\n",
            "train_acc: 0.7243510506798516, val_acc: 0.5172413793103449, train_loss: 0.720011999639504, val_loss: 1.1705042896012368 (42 / 100)\n",
            "train_acc: 0.7416563658838071, val_acc: 0.5812807881773399, train_loss: 0.649172165190481, val_loss: 1.2904558299210271 (43 / 100)\n",
            "train_acc: 0.7750309023485785, val_acc: 0.5517241379310345, train_loss: 0.6106315297897725, val_loss: 1.456777771705477 (44 / 100)\n",
            "train_acc: 0.7812113720642769, val_acc: 0.5763546798029556, train_loss: 0.6290738063925424, val_loss: 1.2661514892953958 (45 / 100)\n",
            "train_acc: 0.7750309023485785, val_acc: 0.49261083743842365, train_loss: 0.5995451040630435, val_loss: 1.4171956752615023 (46 / 100)\n",
            "train_acc: 0.8343634116192831, val_acc: 0.6108374384236454, train_loss: 0.46275841158311654, val_loss: 1.3333669272549633 (47 / 100)\n",
            "train_acc: 0.8158220024721878, val_acc: 0.5566502463054187, train_loss: 0.49458448347556133, val_loss: 1.43542285505774 (48 / 100)\n",
            "train_acc: 0.8133498145859085, val_acc: 0.5665024630541872, train_loss: 0.48703586694189294, val_loss: 1.0378757591905265 (49 / 100)\n",
            "train_acc: 0.8467243510506799, val_acc: 0.5960591133004927, train_loss: 0.40137341185685876, val_loss: 1.3847280934526416 (50 / 100)\n",
            "train_acc: 0.8714462299134734, val_acc: 0.6305418719211823, train_loss: 0.38729592719390454, val_loss: 1.536653264402756 (51 / 100)\n",
            "train_acc: 0.8417799752781211, val_acc: 0.6059113300492611, train_loss: 0.3887831102342924, val_loss: 1.5809046693623359 (52 / 100)\n",
            "train_acc: 0.8751545117428925, val_acc: 0.6009852216748769, train_loss: 0.3418375129163339, val_loss: 1.3487470044291079 (53 / 100)\n",
            "train_acc: 0.8936959208899876, val_acc: 0.5714285714285714, train_loss: 0.33722624374261156, val_loss: 1.6024963164270805 (54 / 100)\n",
            "train_acc: 0.8751545117428925, val_acc: 0.5911330049261084, train_loss: 0.35095195066096313, val_loss: 1.6853223369626575 (55 / 100)\n",
            "train_acc: 0.8751545117428925, val_acc: 0.5270935960591133, train_loss: 0.3639157072206952, val_loss: 1.904931391401244 (56 / 100)\n",
            "train_acc: 0.8875154511742892, val_acc: 0.4975369458128079, train_loss: 0.3296270721922404, val_loss: 1.6219433205468314 (57 / 100)\n",
            "train_acc: 0.8887515451174289, val_acc: 0.6354679802955665, train_loss: 0.33583577903740486, val_loss: 1.651530242905828 (58 / 100)\n",
            "train_acc: 0.896168108776267, val_acc: 0.5960591133004927, train_loss: 0.2898525775436713, val_loss: 1.7380783014696808 (59 / 100)\n",
            "train_acc: 0.896168108776267, val_acc: 0.5615763546798029, train_loss: 0.3047105884651466, val_loss: 1.9847855098141824 (60 / 100)\n",
            "train_acc: 0.9555006180469716, val_acc: 0.645320197044335, train_loss: 0.1407252059436699, val_loss: 1.693721218062152 (61 / 100)\n",
            "train_acc: 0.9629171817058096, val_acc: 0.6157635467980296, train_loss: 0.11601406564229204, val_loss: 1.8737029254142874 (62 / 100)\n",
            "train_acc: 0.9629171817058096, val_acc: 0.6305418719211823, train_loss: 0.10627642992253061, val_loss: 1.8853151187520896 (63 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.625615763546798, train_loss: 0.09605264014878732, val_loss: 2.0235647932062006 (64 / 100)\n",
            "train_acc: 0.9616810877626699, val_acc: 0.6305418719211823, train_loss: 0.10041153431053841, val_loss: 2.0979122704473037 (65 / 100)\n",
            "train_acc: 0.9728059332509271, val_acc: 0.6551724137931034, train_loss: 0.07685511338965431, val_loss: 2.118677528033703 (66 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.6551724137931034, train_loss: 0.08336904272897576, val_loss: 2.104407903596098 (67 / 100)\n",
            "train_acc: 0.965389369592089, val_acc: 0.6403940886699507, train_loss: 0.08602097357896114, val_loss: 2.057441581058972 (68 / 100)\n",
            "train_acc: 0.9678615574783683, val_acc: 0.6600985221674877, train_loss: 0.08502200226517041, val_loss: 2.0422752537750846 (69 / 100)\n",
            "train_acc: 0.9641532756489494, val_acc: 0.6600985221674877, train_loss: 0.0882945800452218, val_loss: 2.0453447602652566 (70 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.6551724137931034, train_loss: 0.08711730593951288, val_loss: 2.2291803066366414 (71 / 100)\n",
            "train_acc: 0.9802224969097652, val_acc: 0.6551724137931034, train_loss: 0.06572199324364641, val_loss: 2.239701152435077 (72 / 100)\n",
            "train_acc: 0.9703337453646477, val_acc: 0.6600985221674877, train_loss: 0.07292569037273582, val_loss: 2.2240016037607426 (73 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6551724137931034, train_loss: 0.05419270821319771, val_loss: 2.376876365962287 (74 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6502463054187192, train_loss: 0.06421944771620487, val_loss: 2.423926644724578 (75 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.6551724137931034, train_loss: 0.06284186704491802, val_loss: 2.301781446475701 (76 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6650246305418719, train_loss: 0.07452228352685351, val_loss: 2.4512285892599324 (77 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6600985221674877, train_loss: 0.06406678698671468, val_loss: 2.4466741554842795 (78 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.6502463054187192, train_loss: 0.07632313626277294, val_loss: 2.4347962041206546 (79 / 100)\n",
            "train_acc: 0.9752781211372065, val_acc: 0.6551724137931034, train_loss: 0.06768154125771174, val_loss: 2.485454417214605 (80 / 100)\n",
            "train_acc: 0.9814585908529048, val_acc: 0.6650246305418719, train_loss: 0.05634338525728275, val_loss: 2.4742644737506736 (81 / 100)\n",
            "train_acc: 0.9777503090234858, val_acc: 0.6600985221674877, train_loss: 0.060442436611028307, val_loss: 2.5553863624046587 (82 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6600985221674877, train_loss: 0.04272156062378227, val_loss: 2.6642406691471345 (83 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.6551724137931034, train_loss: 0.05700274671765208, val_loss: 2.678679021121246 (84 / 100)\n",
            "train_acc: 0.9826946847960445, val_acc: 0.6650246305418719, train_loss: 0.05848134654410481, val_loss: 2.728360578931611 (85 / 100)\n",
            "train_acc: 0.969097651421508, val_acc: 0.645320197044335, train_loss: 0.07279764161513122, val_loss: 2.4878352874605527 (86 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6699507389162561, train_loss: 0.06204288218133566, val_loss: 2.3763108300458033 (87 / 100)\n",
            "train_acc: 0.9765142150803461, val_acc: 0.6403940886699507, train_loss: 0.07321777669033692, val_loss: 2.468250327509612 (88 / 100)\n",
            "train_acc: 0.9789864029666254, val_acc: 0.6354679802955665, train_loss: 0.0627627234597436, val_loss: 2.5111611288756572 (89 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6403940886699507, train_loss: 0.03419416325514484, val_loss: 2.6224009168559106 (90 / 100)\n",
            "train_acc: 0.9715698393077874, val_acc: 0.6305418719211823, train_loss: 0.0579918220418198, val_loss: 2.6117232256922227 (91 / 100)\n",
            "train_acc: 0.9802224969097652, val_acc: 0.6403940886699507, train_loss: 0.05301909570579909, val_loss: 2.6734777777065784 (92 / 100)\n",
            "train_acc: 0.992583436341162, val_acc: 0.6650246305418719, train_loss: 0.029834985438900067, val_loss: 2.521628756828496 (93 / 100)\n",
            "train_acc: 0.9864029666254636, val_acc: 0.6551724137931034, train_loss: 0.044227909246026174, val_loss: 2.6390926098001413 (94 / 100)\n",
            "train_acc: 0.9740420271940667, val_acc: 0.6551724137931034, train_loss: 0.06593744565441523, val_loss: 2.714348930443449 (95 / 100)\n",
            "train_acc: 0.9802224969097652, val_acc: 0.6502463054187192, train_loss: 0.053520994369174145, val_loss: 2.576184107165031 (96 / 100)\n",
            "train_acc: 0.9851668726823238, val_acc: 0.645320197044335, train_loss: 0.04101192038299053, val_loss: 2.6387774227875207 (97 / 100)\n",
            "train_acc: 0.9888751545117429, val_acc: 0.6502463054187192, train_loss: 0.03380941680652102, val_loss: 2.905695945758538 (98 / 100)\n",
            "train_acc: 0.9839307787391842, val_acc: 0.645320197044335, train_loss: 0.04463958650901085, val_loss: 2.827765424850539 (99 / 100)\n",
            "train_acc: 0.9802224969097652, val_acc: 0.6502463054187192, train_loss: 0.04744047360962121, val_loss: 2.639707820168857 (100 / 100)\n",
            "({'lr': 0.001, 'batch_size': 16, 'weight_decay': 0.001, 'gamma': 0.1}), val accuracy 0.6699507389162561, val loss 2.3763108300458033\n",
            "\n",
            "({'lr': 0.0005, 'batch_size': 8, 'weight_decay': 1e-05, 'gamma': 0.05}), best val accuracy 0.7635467980295566, best val loss 1.4869063125161701\n",
            "\n",
            "val_accuracies\n",
            "[0.7635467980295566, 0.6945812807881774, 0.6798029556650246, 0.6995073891625616, 0.6354679802955665, 0.6354679802955665, 0.645320197044335, 0.6502463054187192, 0.6502463054187192, 0.6305418719211823, 0.6551724137931034, 0.6502463054187192, 0.6896551724137931, 0.6995073891625616, 0.6798029556650246, 0.6699507389162561]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S1laZWm8Q0tm"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TKl555WRQ1AF",
        "trusted": false,
        "colab": {}
      },
      "source": [
        "# todo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jJGI06ylKePa"
      },
      "source": [
        "**Mean / std computation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YDJptx12L1OL",
        "trusted": false,
        "colab": {}
      },
      "source": [
        "TRAIN_DATA_DIR = 'AIML_project/ravdess-emotional-song-mel'\n",
        "pixel_mean = np.zeros(3)\n",
        "pixel_std = np.zeros(3)\n",
        "k = 1\n",
        "dataset, _ = get_datasets(TRAIN_DATA_DIR, TRAIN_DATA_DIR, [])\n",
        "for image, _ in tqdm(dataset, \"Computing mean/std\", len(dataset), unit=\"samples\"):\n",
        "    image = np.array(image)\n",
        "    pixels = image.reshape((-1, image.shape[2]))\n",
        "\n",
        "    for pixel in pixels:\n",
        "        diff = pixel - pixel_mean\n",
        "        pixel_mean += diff / k\n",
        "        pixel_std += diff * (pixel - pixel_mean)\n",
        "        k += 1\n",
        "\n",
        "pixel_std = np.sqrt(pixel_std / (k - 2))\n",
        "print(pixel_mean)\n",
        "print(pixel_std)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}