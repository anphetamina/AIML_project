training set 809
validation set 203
-------------------------------------
lr 0.000396845229881056, batch 9, decay 2.6808725159406163e-07, gamma 0.9871566226587425
train_acc: 0.20024721878862795, val_acc: 0.18226600985221675, train_loss: 1.7848302131824942, val_loss: 1.7766147810837318 (1 / 15)
train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.766595770755716, val_loss: 1.7492677214110426 (2 / 15)
train_acc: 0.18912237330037082, val_acc: 0.21674876847290642, train_loss: 1.7529719969250805, val_loss: 1.7355747446050784 (3 / 15)
train_acc: 0.22126081582200247, val_acc: 0.19704433497536947, train_loss: 1.7261965563624397, val_loss: 1.702792909932254 (4 / 15)
train_acc: 0.242274412855377, val_acc: 0.33004926108374383, train_loss: 1.6897596358074098, val_loss: 1.6553639813596979 (5 / 15)
train_acc: 0.31025957972805934, val_acc: 0.3793103448275862, train_loss: 1.642398045443192, val_loss: 1.5483448038547498 (6 / 15)
train_acc: 0.29295426452410384, val_acc: 0.3793103448275862, train_loss: 1.6473808130905567, val_loss: 1.5723920810986034 (7 / 15)
train_acc: 0.3547589616810878, val_acc: 0.3399014778325123, train_loss: 1.534724946369789, val_loss: 1.5307881056968802 (8 / 15)
train_acc: 0.3658838071693449, val_acc: 0.35467980295566504, train_loss: 1.514555627837022, val_loss: 1.4957832227199537 (9 / 15)
train_acc: 0.36711990111248455, val_acc: 0.3645320197044335, train_loss: 1.4743111276508705, val_loss: 1.4308567795847438 (10 / 15)
train_acc: 0.3621755253399258, val_acc: 0.41379310344827586, train_loss: 1.4220087260044696, val_loss: 1.4029420314751235 (11 / 15)
train_acc: 0.3831891223733004, val_acc: 0.4482758620689655, train_loss: 1.4373014170690284, val_loss: 1.3468167291486204 (12 / 15)
train_acc: 0.4276885043263288, val_acc: 0.4236453201970443, train_loss: 1.3735396489224712, val_loss: 1.3642887141316982 (13 / 15)
train_acc: 0.40667490729295425, val_acc: 0.35960591133004927, train_loss: 1.3866343801042205, val_loss: 1.380812403456918 (14 / 15)
train_acc: 0.3980222496909765, val_acc: 0.39901477832512317, train_loss: 1.3794479499051833, val_loss: 1.3488022567603388 (15 / 15)



val accuracy 0.4482758620689655
val loss 1.3468167291486204
-------------------------------------
lr 0.00012162182276317409, batch 11, decay 7.878901145200322e-07, gamma 0.054280651290473374
train_acc: 0.18046971569839307, val_acc: 0.22660098522167488, train_loss: 1.7924848674106952, val_loss: 1.7890762037831573 (1 / 15)
train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7876108977644347, val_loss: 1.785244296924234 (2 / 15)
train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7844557685521978, val_loss: 1.7809127440006274 (3 / 15)
train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.779856942198038, val_loss: 1.7748696304894434 (4 / 15)
train_acc: 0.20395550061804696, val_acc: 0.18226600985221675, train_loss: 1.770158713768675, val_loss: 1.7658044675300861 (5 / 15)
train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.7617518845830475, val_loss: 1.7558694884107617 (6 / 15)
train_acc: 0.173053152039555, val_acc: 0.18226600985221675, train_loss: 1.7583676908160613, val_loss: 1.755373258308824 (7 / 15)
train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.755191636910692, val_loss: 1.7547893553531815 (8 / 15)
train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7545599194775405, val_loss: 1.7543077392531146 (9 / 15)
train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.755799512780641, val_loss: 1.7537206622767332 (10 / 15)
train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.754692523676916, val_loss: 1.75322440046395 (11 / 15)
train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7545254205449108, val_loss: 1.7527315851502818 (12 / 15)
train_acc: 0.17058096415327564, val_acc: 0.18226600985221675, train_loss: 1.7573026045880595, val_loss: 1.7527074026944014 (13 / 15)
train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7553925366867311, val_loss: 1.7526868264663396 (14 / 15)
train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7518993346446523, val_loss: 1.7526643029574691 (15 / 15)



val accuracy 0.22660098522167488
val loss 1.7890762037831573
-------------------------------------
lr 0.0004828117784026051, batch 9, decay 1.201423540206584e-07, gamma 0.5318142372644433
train_acc: 0.17428924598269468, val_acc: 0.18226600985221675, train_loss: 1.7865929964593668, val_loss: 1.7750052448563975 (1 / 15)
train_acc: 0.17428924598269468, val_acc: 0.1921182266009852, train_loss: 1.7681530255764466, val_loss: 1.7493618944008362 (2 / 15)
train_acc: 0.18541409147095178, val_acc: 0.22660098522167488, train_loss: 1.7527998709413413, val_loss: 1.728518964621821 (3 / 15)
train_acc: 0.23485784919653893, val_acc: 0.2019704433497537, train_loss: 1.7252467416862327, val_loss: 1.7116355449695306 (4 / 15)
train_acc: 0.2954264524103832, val_acc: 0.3103448275862069, train_loss: 1.6640131797719797, val_loss: 1.6224137644462397 (5 / 15)
train_acc: 0.3176761433868974, val_acc: 0.33004926108374383, train_loss: 1.5991232597046936, val_loss: 1.610436108899234 (6 / 15)
train_acc: 0.377008652657602, val_acc: 0.32019704433497537, train_loss: 1.5338265892896428, val_loss: 1.4995223418832413 (7 / 15)
train_acc: 0.3819530284301607, val_acc: 0.3842364532019704, train_loss: 1.4678179802941747, val_loss: 1.477359423496453 (8 / 15)
train_acc: 0.36341161928306553, val_acc: 0.35467980295566504, train_loss: 1.4601140932924814, val_loss: 1.4504487203259773 (9 / 15)
train_acc: 0.39184177997527814, val_acc: 0.37438423645320196, train_loss: 1.4251445521530322, val_loss: 1.4116344440159538 (10 / 15)
train_acc: 0.3683559950556242, val_acc: 0.3891625615763547, train_loss: 1.4189053255194344, val_loss: 1.378293768232092 (11 / 15)
train_acc: 0.3980222496909765, val_acc: 0.37438423645320196, train_loss: 1.3829145378471157, val_loss: 1.4074647802437468 (12 / 15)
train_acc: 0.40173053152039556, val_acc: 0.3891625615763547, train_loss: 1.3752073166691623, val_loss: 1.3577896773521536 (13 / 15)
train_acc: 0.4103831891223733, val_acc: 0.4039408866995074, train_loss: 1.3594067137969292, val_loss: 1.349713334455866 (14 / 15)
train_acc: 0.3980222496909765, val_acc: 0.41379310344827586, train_loss: 1.3588216570016038, val_loss: 1.3463674865919968 (15 / 15)



val accuracy 0.41379310344827586
val loss 1.3463674865919968
-------------------------------------
lr 0.0002043332169114638, batch 15, decay 2.7804163514612535e-08, gamma 0.25867685801184426
train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7910368068256544, val_loss: 1.7874364847032895 (1 / 15)
train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.785053313883627, val_loss: 1.7821736582394303 (2 / 15)
train_acc: 0.19406674907292953, val_acc: 0.1921182266009852, train_loss: 1.77896735768383, val_loss: 1.774569630035626 (3 / 15)
train_acc: 0.20519159456118666, val_acc: 0.22167487684729065, train_loss: 1.7715490845726212, val_loss: 1.7652860316149708 (4 / 15)
train_acc: 0.21013597033374537, val_acc: 0.18226600985221675, train_loss: 1.759694210236388, val_loss: 1.7551869764703836 (5 / 15)
train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7573068127201867, val_loss: 1.748938647397046 (6 / 15)
train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.751741527774278, val_loss: 1.7472092747101056 (7 / 15)
train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.752324692693128, val_loss: 1.74550252125181 (8 / 15)
train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7492427248300522, val_loss: 1.7440883637649085 (9 / 15)
train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.748071237164463, val_loss: 1.7422701455102179 (10 / 15)
train_acc: 0.2138442521631644, val_acc: 0.18226600985221675, train_loss: 1.7447104832740001, val_loss: 1.7405474573520605 (11 / 15)
train_acc: 0.2027194066749073, val_acc: 0.18226600985221675, train_loss: 1.7453577727705645, val_loss: 1.7385093267328047 (12 / 15)
train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7385701593863507, val_loss: 1.7378926470949145 (13 / 15)
train_acc: 0.2138442521631644, val_acc: 0.18226600985221675, train_loss: 1.739126086677108, val_loss: 1.7372303719591038 (14 / 15)
train_acc: 0.2200247218788628, val_acc: 0.18226600985221675, train_loss: 1.7398522930180629, val_loss: 1.73668993811302 (15 / 15)



val accuracy 0.22167487684729065
val loss 1.7652860316149708
-------------------------------------
lr 0.00017186388216725213, batch 13, decay 1.7574926043544498e-07, gamma 0.03471151349800979
train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7903164494022892, val_loss: 1.7880271620351105 (1 / 15)
train_acc: 0.19777503090234858, val_acc: 0.2561576354679803, train_loss: 1.7859291720891324, val_loss: 1.78380451472522 (2 / 15)
train_acc: 0.2027194066749073, val_acc: 0.26108374384236455, train_loss: 1.7822695107041537, val_loss: 1.7781740033567832 (3 / 15)
train_acc: 0.20395550061804696, val_acc: 0.18226600985221675, train_loss: 1.7752138469068908, val_loss: 1.770595074287189 (4 / 15)
train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.766083412618366, val_loss: 1.7592461238353712 (5 / 15)
train_acc: 0.17428924598269468, val_acc: 0.18226600985221675, train_loss: 1.7645381217539238, val_loss: 1.75340206399927 (6 / 15)
train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7559431542277484, val_loss: 1.7530856813703264 (7 / 15)
train_acc: 0.2088998763906057, val_acc: 0.18226600985221675, train_loss: 1.7556082704305944, val_loss: 1.752810006658432 (8 / 15)
train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7578241929725016, val_loss: 1.752483747275592 (9 / 15)
train_acc: 0.19777503090234858, val_acc: 0.18226600985221675, train_loss: 1.7553314344844653, val_loss: 1.7521869220169894 (10 / 15)
train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.7578053850178665, val_loss: 1.7519225193361931 (11 / 15)
train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.75752007195033, val_loss: 1.7516106367111206 (12 / 15)
train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.756177163831383, val_loss: 1.751603316790952 (13 / 15)
train_acc: 0.19901112484548825, val_acc: 0.18226600985221675, train_loss: 1.7532517898215352, val_loss: 1.7515950214686653 (14 / 15)
train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.7549702424201448, val_loss: 1.7515892624267804 (15 / 15)



val accuracy 0.26108374384236455
val loss 1.7781740033567832
-------------------------------------
lr 0.00035053622394581847, batch 14, decay 9.603037128193257e-07, gamma 0.257590473612614
train_acc: 0.1841779975278121, val_acc: 0.22660098522167488, train_loss: 1.7901896904366568, val_loss: 1.7869497044333096 (1 / 15)
train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7853187024961739, val_loss: 1.780898957416929 (2 / 15)
train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7769872104871112, val_loss: 1.7693783044815063 (3 / 15)
train_acc: 0.16934487021013597, val_acc: 0.18226600985221675, train_loss: 1.7664172054073866, val_loss: 1.755848062449488 (4 / 15)
train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7528122312795689, val_loss: 1.7435951520656716 (5 / 15)
train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.740729303383562, val_loss: 1.728534513506396 (6 / 15)
train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.72374359579994, val_loss: 1.7225392070309868 (7 / 15)
train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7182625182921571, val_loss: 1.716058899616373 (8 / 15)
train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7132867374290495, val_loss: 1.709807091745837 (9 / 15)
train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7045973641026595, val_loss: 1.7020157986673816 (10 / 15)
train_acc: 0.19777503090234858, val_acc: 0.18226600985221675, train_loss: 1.6968091472854425, val_loss: 1.694020809798405 (11 / 15)
train_acc: 0.18912237330037082, val_acc: 0.18719211822660098, train_loss: 1.6893870208702513, val_loss: 1.6857586605795498 (12 / 15)
train_acc: 0.19283065512978986, val_acc: 0.1921182266009852, train_loss: 1.6853110206731319, val_loss: 1.6835282917680412 (13 / 15)
train_acc: 0.20148331273176762, val_acc: 0.1921182266009852, train_loss: 1.6824468365411087, val_loss: 1.681578697829411 (14 / 15)
train_acc: 0.20519159456118666, val_acc: 0.1921182266009852, train_loss: 1.6787893587932894, val_loss: 1.679009770524913 (15 / 15)



val accuracy 0.22660098522167488
val loss 1.7869497044333096
-------------------------------------
lr 0.0006265409718207409, batch 13, decay 3.937859977121997e-06, gamma 0.20220797100321766
train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7913132693770495, val_loss: 1.7866097612334002 (1 / 15)
train_acc: 0.20519159456118666, val_acc: 0.33497536945812806, train_loss: 1.7830580611458815, val_loss: 1.7745234719638168 (2 / 15)
train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7687981343829293, val_loss: 1.755541849018905 (3 / 15)
train_acc: 0.18912237330037082, val_acc: 0.2019704433497537, train_loss: 1.7631613335886462, val_loss: 1.744195390804648 (4 / 15)
train_acc: 0.23362175525339926, val_acc: 0.27586206896551724, train_loss: 1.7299087084415374, val_loss: 1.7090840421873947 (5 / 15)
train_acc: 0.2867737948084054, val_acc: 0.30049261083743845, train_loss: 1.682973950401372, val_loss: 1.6019994073313446 (6 / 15)
train_acc: 0.3176761433868974, val_acc: 0.30049261083743845, train_loss: 1.5953631119616098, val_loss: 1.5705093979248272 (7 / 15)
train_acc: 0.35970333745364647, val_acc: 0.3645320197044335, train_loss: 1.5372443720790452, val_loss: 1.504821533052792 (8 / 15)
train_acc: 0.3399258343634116, val_acc: 0.3251231527093596, train_loss: 1.4953623517924097, val_loss: 1.4993905243027974 (9 / 15)
train_acc: 0.3658838071693449, val_acc: 0.33004926108374383, train_loss: 1.4744374816880386, val_loss: 1.5260129434428191 (10 / 15)
train_acc: 0.3819530284301607, val_acc: 0.35960591133004927, train_loss: 1.4666210249416318, val_loss: 1.5095633054014497 (11 / 15)
train_acc: 0.3695920889987639, val_acc: 0.39408866995073893, train_loss: 1.4534478715087928, val_loss: 1.4322497765418931 (12 / 15)
train_acc: 0.3868974042027194, val_acc: 0.39901477832512317, train_loss: 1.4141314896134423, val_loss: 1.4213746665733789 (13 / 15)
train_acc: 0.380716934487021, val_acc: 0.4088669950738916, train_loss: 1.4303607516294652, val_loss: 1.4173615075684534 (14 / 15)
train_acc: 0.37824474660074164, val_acc: 0.39408866995073893, train_loss: 1.4132197118365426, val_loss: 1.4141263251234157 (15 / 15)



val accuracy 0.4088669950738916
val loss 1.4173615075684534
-------------------------------------
lr 0.0005500352008587738, batch 7, decay 1.177132250588604e-06, gamma 0.7637578715881801
train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.784018263681268, val_loss: 1.773295706716077 (1 / 15)
train_acc: 0.18541409147095178, val_acc: 0.20689655172413793, train_loss: 1.7643736611190626, val_loss: 1.7447288611839558 (2 / 15)
train_acc: 0.18046971569839307, val_acc: 0.30049261083743845, train_loss: 1.7357754997623571, val_loss: 1.7013937317091843 (3 / 15)
train_acc: 0.25092707045735474, val_acc: 0.2512315270935961, train_loss: 1.6809835877495143, val_loss: 1.6254099936320865 (4 / 15)
train_acc: 0.2719406674907293, val_acc: 0.35960591133004927, train_loss: 1.634666715770481, val_loss: 1.5979176718613197 (5 / 15)
train_acc: 0.3584672435105068, val_acc: 0.35960591133004927, train_loss: 1.5913601656926724, val_loss: 1.5677629298177258 (6 / 15)
train_acc: 0.34857849196538937, val_acc: 0.2955665024630542, train_loss: 1.550400878354557, val_loss: 1.6170150781499928 (7 / 15)
train_acc: 0.3695920889987639, val_acc: 0.3694581280788177, train_loss: 1.5458435532631185, val_loss: 1.5696321263395507 (8 / 15)
train_acc: 0.35599505562422745, val_acc: 0.3497536945812808, train_loss: 1.5388126556746597, val_loss: 1.5559531789401482 (9 / 15)
train_acc: 0.3621755253399258, val_acc: 0.28078817733990147, train_loss: 1.5307392391050407, val_loss: 1.6268035625589305 (10 / 15)
train_acc: 0.35970333745364647, val_acc: 0.3497536945812808, train_loss: 1.5548641011210984, val_loss: 1.5516418937979073 (11 / 15)
train_acc: 0.40173053152039556, val_acc: 0.39408866995073893, train_loss: 1.5105821684794902, val_loss: 1.5122802648051032 (12 / 15)
train_acc: 0.3943139678615575, val_acc: 0.35960591133004927, train_loss: 1.481925735441215, val_loss: 1.5048816738457516 (13 / 15)
train_acc: 0.38813349814585907, val_acc: 0.3793103448275862, train_loss: 1.4972109131524236, val_loss: 1.526846621570916 (14 / 15)
train_acc: 0.4042027194066749, val_acc: 0.3694581280788177, train_loss: 1.4660417217259354, val_loss: 1.6041237788981404 (15 / 15)



val accuracy 0.39408866995073893
val loss 1.5122802648051032
-------------------------------------
lr 0.000418587975512554, batch 15, decay 1.5875393272343253e-07, gamma 0.061631462519194144
train_acc: 0.1681087762669963, val_acc: 0.17733990147783252, train_loss: 1.7901616261532929, val_loss: 1.7863649065271388 (1 / 15)
train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7830312627797074, val_loss: 1.777766276462912 (2 / 15)
train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7717427175920295, val_loss: 1.761397215533139 (3 / 15)
train_acc: 0.1681087762669963, val_acc: 0.18226600985221675, train_loss: 1.7643528758227014, val_loss: 1.7519322410592892 (4 / 15)
train_acc: 0.2138442521631644, val_acc: 0.18226600985221675, train_loss: 1.7535967688330614, val_loss: 1.745137520024342 (5 / 15)
train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7452601839791713, val_loss: 1.7332050853174896 (6 / 15)
train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7367753649524176, val_loss: 1.7321024381468448 (7 / 15)
train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7309967294169593, val_loss: 1.7309619258777262 (8 / 15)
train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7297018371494797, val_loss: 1.729689081314162 (9 / 15)
train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.728100798038676, val_loss: 1.7285573605833382 (10 / 15)
train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7259692366249924, val_loss: 1.7273520307587873 (11 / 15)
train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7257400749640353, val_loss: 1.7261331275178882 (12 / 15)
train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7281923571092679, val_loss: 1.7260645810019206 (13 / 15)
train_acc: 0.20148331273176762, val_acc: 0.18226600985221675, train_loss: 1.725598096994888, val_loss: 1.7259891984497973 (14 / 15)
train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7260142273012875, val_loss: 1.7259117734843288 (15 / 15)



val accuracy 0.18226600985221675
val loss 1.777766276462912
-------------------------------------
lr 0.000261284437403571, batch 14, decay 9.632847620035092e-06, gamma 0.6325169196741609
train_acc: 0.1619283065512979, val_acc: 0.2955665024630542, train_loss: 1.7910351247811052, val_loss: 1.788206988367541 (1 / 15)
train_acc: 0.21137206427688504, val_acc: 0.18226600985221675, train_loss: 1.7855382037840608, val_loss: 1.7833294005229556 (2 / 15)
train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7801150591912758, val_loss: 1.7758505138857612 (3 / 15)
train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.7741605344013172, val_loss: 1.7651274533107364 (4 / 15)
train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.7624896408158857, val_loss: 1.7534105202247356 (5 / 15)
train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7536874828291469, val_loss: 1.7448112471350308 (6 / 15)
train_acc: 0.19901112484548825, val_acc: 0.22167487684729065, train_loss: 1.7472429206256372, val_loss: 1.7395548286109135 (7 / 15)
train_acc: 0.21631644004944375, val_acc: 0.2561576354679803, train_loss: 1.7389656617703337, val_loss: 1.7303337516455815 (8 / 15)
train_acc: 0.21508034610630408, val_acc: 0.2857142857142857, train_loss: 1.7391548236309378, val_loss: 1.7225404648945248 (9 / 15)
train_acc: 0.2583436341161928, val_acc: 0.32019704433497537, train_loss: 1.7143965732623974, val_loss: 1.7011792289799657 (10 / 15)
train_acc: 0.27564894932014833, val_acc: 0.3054187192118227, train_loss: 1.7034315696016082, val_loss: 1.6685913307913418 (11 / 15)
train_acc: 0.31025957972805934, val_acc: 0.2512315270935961, train_loss: 1.6505796720719308, val_loss: 1.6163105430274174 (12 / 15)
train_acc: 0.3288009888751545, val_acc: 0.3054187192118227, train_loss: 1.5893286379659721, val_loss: 1.5700170829378326 (13 / 15)
train_acc: 0.33127317676143386, val_acc: 0.3448275862068966, train_loss: 1.5781935922000259, val_loss: 1.5366655094870205 (14 / 15)
train_acc: 0.33498145859085293, val_acc: 0.33004926108374383, train_loss: 1.5579381472395435, val_loss: 1.5382931026919135 (15 / 15)



val accuracy 0.3448275862068966
val loss 1.5366655094870205
-------------------------------------
lr 0.00011176020363985656, batch 7, decay 3.2518348583908426e-06, gamma 0.16374695403639838
train_acc: 0.17058096415327564, val_acc: 0.18226600985221675, train_loss: 1.7921533581647648, val_loss: 1.7900923819377506 (1 / 15)
train_acc: 0.19777503090234858, val_acc: 0.18226600985221675, train_loss: 1.7887661199192477, val_loss: 1.786939312671793 (2 / 15)
train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.786010635945352, val_loss: 1.7831371775988876 (3 / 15)
train_acc: 0.2088998763906057, val_acc: 0.2413793103448276, train_loss: 1.7818155195863343, val_loss: 1.7786324599693561 (4 / 15)
train_acc: 0.16934487021013597, val_acc: 0.18226600985221675, train_loss: 1.7780621134306502, val_loss: 1.7727925818541954 (5 / 15)
train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7710669603571756, val_loss: 1.7642126412227237 (6 / 15)
train_acc: 0.2088998763906057, val_acc: 0.18226600985221675, train_loss: 1.7644411652580327, val_loss: 1.7627988724872983 (7 / 15)
train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7638674855084886, val_loss: 1.7613970164594979 (8 / 15)
train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7621706150960275, val_loss: 1.759925517542609 (9 / 15)
train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7636252516427058, val_loss: 1.758804539154316 (10 / 15)
train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7612466350916143, val_loss: 1.7577998185979908 (11 / 15)
train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7590797052100504, val_loss: 1.756651237093169 (12 / 15)
train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7580525788152763, val_loss: 1.7564641031725654 (13 / 15)
train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7573145853426901, val_loss: 1.7563060850932681 (14 / 15)
train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.763214194141006, val_loss: 1.7561618130782555 (15 / 15)



val accuracy 0.2413793103448276
val loss 1.7786324599693561
-------------------------------------
lr 0.00021435250426491992, batch 15, decay 5.420970747054209e-06, gamma 0.14947642219867446
train_acc: 0.18046971569839307, val_acc: 0.1921182266009852, train_loss: 1.7908798503345258, val_loss: 1.788701837873224 (1 / 15)
train_acc: 0.19283065512978986, val_acc: 0.20689655172413793, train_loss: 1.7866990364378845, val_loss: 1.7841441361187713 (2 / 15)
train_acc: 0.19777503090234858, val_acc: 0.18226600985221675, train_loss: 1.7816208496376669, val_loss: 1.7778270978645738 (3 / 15)
train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7757238123119246, val_loss: 1.767739528505673 (4 / 15)
train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.762336168801976, val_loss: 1.754509907637911 (5 / 15)
train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.752678276876584, val_loss: 1.744321214154436 (6 / 15)
train_acc: 0.19901112484548825, val_acc: 0.18226600985221675, train_loss: 1.746591516124598, val_loss: 1.7428597898906089 (7 / 15)
train_acc: 0.17428924598269468, val_acc: 0.18226600985221675, train_loss: 1.7466442892961065, val_loss: 1.7414366147788287 (8 / 15)
train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7412579139171926, val_loss: 1.7398121157303232 (9 / 15)
train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7404841069680062, val_loss: 1.7383589632992673 (10 / 15)
train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7381266788734935, val_loss: 1.7368437374753904 (11 / 15)
train_acc: 0.20519159456118666, val_acc: 0.18226600985221675, train_loss: 1.741010499825731, val_loss: 1.7353942135871925 (12 / 15)
train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7341811088165924, val_loss: 1.7351538723912732 (13 / 15)
train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7354475810884398, val_loss: 1.7349053920783433 (14 / 15)
train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7377619786079794, val_loss: 1.7346764638506134 (15 / 15)



val accuracy 0.20689655172413793
val loss 1.7841441361187713
-------------------------------------
lr 0.00039409387506816445, batch 7, decay 1.4206786907925918e-07, gamma 0.01522737004507927
train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.7859199172339422, val_loss: 1.7776660261483028 (1 / 15)
train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.766338638823171, val_loss: 1.7529426895338913 (2 / 15)
train_acc: 0.18788627935723115, val_acc: 0.3054187192118227, train_loss: 1.7558778032236841, val_loss: 1.7394525429298138 (3 / 15)
train_acc: 0.2138442521631644, val_acc: 0.2561576354679803, train_loss: 1.7298389396207736, val_loss: 1.7045391880232712 (4 / 15)
train_acc: 0.23980222496909764, val_acc: 0.3103448275862069, train_loss: 1.6861201660300067, val_loss: 1.6533702570816566 (5 / 15)
train_acc: 0.29913473423980225, val_acc: 0.35467980295566504, train_loss: 1.631881959211399, val_loss: 1.6102208269053493 (6 / 15)
train_acc: 0.315203955500618, val_acc: 0.35467980295566504, train_loss: 1.5931862277359838, val_loss: 1.6048961240669777 (7 / 15)
train_acc: 0.3263288009888752, val_acc: 0.35467980295566504, train_loss: 1.5898867318008385, val_loss: 1.6011599610591758 (8 / 15)
train_acc: 0.32756489493201485, val_acc: 0.35467980295566504, train_loss: 1.5841556948106574, val_loss: 1.5977035822539494 (9 / 15)
train_acc: 0.3362175525339926, val_acc: 0.35467980295566504, train_loss: 1.5859111727241828, val_loss: 1.5949467009511487 (10 / 15)
train_acc: 0.34363411619283063, val_acc: 0.3399014778325123, train_loss: 1.5748637135155563, val_loss: 1.5915842529000908 (11 / 15)
train_acc: 0.3288009888751545, val_acc: 0.3497536945812808, train_loss: 1.5766514346685043, val_loss: 1.5891368265809684 (12 / 15)
train_acc: 0.3399258343634116, val_acc: 0.3497536945812808, train_loss: 1.5753314291738314, val_loss: 1.5891068392786487 (13 / 15)
train_acc: 0.33127317676143386, val_acc: 0.3497536945812808, train_loss: 1.5759816844619543, val_loss: 1.5890765251784489 (14 / 15)
train_acc: 0.33127317676143386, val_acc: 0.3497536945812808, train_loss: 1.5761904657550148, val_loss: 1.5890468400100182 (15 / 15)



val accuracy 0.35467980295566504
val loss 1.6102208269053493
-------------------------------------
lr 0.00033035078143576314, batch 11, decay 3.0930874663132856e-09, gamma 0.20989790942848344
train_acc: 0.1619283065512979, val_acc: 0.18226600985221675, train_loss: 1.7824693686882556, val_loss: 1.7644618156508272 (1 / 15)
train_acc: 0.18912237330037082, val_acc: 0.2315270935960591, train_loss: 1.760966505638896, val_loss: 1.7462930843747895 (2 / 15)
train_acc: 0.18665018541409148, val_acc: 0.1921182266009852, train_loss: 1.7539151425119972, val_loss: 1.7336927875509403 (3 / 15)
train_acc: 0.2249690976514215, val_acc: 0.3054187192118227, train_loss: 1.7337785571703068, val_loss: 1.705697043775925 (4 / 15)
train_acc: 0.26452410383189123, val_acc: 0.35960591133004927, train_loss: 1.6933209753154381, val_loss: 1.6213775714629977 (5 / 15)
train_acc: 0.32014833127317677, val_acc: 0.2955665024630542, train_loss: 1.6248975747595316, val_loss: 1.597509065285105 (6 / 15)
train_acc: 0.34857849196538937, val_acc: 0.31527093596059114, train_loss: 1.5375714007355226, val_loss: 1.5185995207631529 (7 / 15)
train_acc: 0.34239802224969096, val_acc: 0.3645320197044335, train_loss: 1.5238594361966562, val_loss: 1.5184680625722913 (8 / 15)
train_acc: 0.3473423980222497, val_acc: 0.32019704433497537, train_loss: 1.511504783323875, val_loss: 1.4872984542635275 (9 / 15)
train_acc: 0.35599505562422745, val_acc: 0.35960591133004927, train_loss: 1.486658769426004, val_loss: 1.5053262766358888 (10 / 15)
train_acc: 0.36711990111248455, val_acc: 0.3399014778325123, train_loss: 1.446733987965012, val_loss: 1.4667552727196604 (11 / 15)
train_acc: 0.38442521631644005, val_acc: 0.3448275862068966, train_loss: 1.4692335106681098, val_loss: 1.4653106331825256 (12 / 15)
train_acc: 0.3621755253399258, val_acc: 0.3694581280788177, train_loss: 1.4468085342932986, val_loss: 1.464742455282822 (13 / 15)
train_acc: 0.39060568603213847, val_acc: 0.3497536945812808, train_loss: 1.440756988466449, val_loss: 1.4534432829307218 (14 / 15)
train_acc: 0.380716934487021, val_acc: 0.3645320197044335, train_loss: 1.4422354365161383, val_loss: 1.4510125103842448 (15 / 15)



val accuracy 0.3694581280788177
val loss 1.464742455282822
-------------------------------------
lr 0.0006352431618853048, batch 10, decay 4.301879159178508e-06, gamma 0.0838355103347808
train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7821286459935757, val_loss: 1.7615327012949977 (1 / 15)
train_acc: 0.20024721878862795, val_acc: 0.18226600985221675, train_loss: 1.7544236751952484, val_loss: 1.7440841791077788 (2 / 15)
train_acc: 0.2027194066749073, val_acc: 0.21182266009852216, train_loss: 1.7348204095225104, val_loss: 1.7105496980873822 (3 / 15)
train_acc: 0.24474660074165636, val_acc: 0.28078817733990147, train_loss: 1.7040456167994382, val_loss: 1.657204031944275 (4 / 15)
train_acc: 0.30778739184178, val_acc: 0.3448275862068966, train_loss: 1.6536804733818629, val_loss: 1.6209409700825883 (5 / 15)
train_acc: 0.29295426452410384, val_acc: 0.32019704433497537, train_loss: 1.6361920419523241, val_loss: 1.5621476649063561 (6 / 15)
train_acc: 0.3683559950556242, val_acc: 0.3497536945812808, train_loss: 1.5418722829948397, val_loss: 1.5334030166635373 (7 / 15)
train_acc: 0.3757725587144623, val_acc: 0.3448275862068966, train_loss: 1.5163703162236915, val_loss: 1.5250700641735433 (8 / 15)
train_acc: 0.36711990111248455, val_acc: 0.3645320197044335, train_loss: 1.5086396108893736, val_loss: 1.5164428544161943 (9 / 15)
train_acc: 0.3683559950556242, val_acc: 0.3251231527093596, train_loss: 1.5091798111004353, val_loss: 1.5026835925473367 (10 / 15)
train_acc: 0.40667490729295425, val_acc: 0.3694581280788177, train_loss: 1.4924416253239616, val_loss: 1.4971451248441423 (11 / 15)
train_acc: 0.3930778739184178, val_acc: 0.37438423645320196, train_loss: 1.4709384417799112, val_loss: 1.5010737468456399 (12 / 15)
train_acc: 0.3720642768850433, val_acc: 0.37438423645320196, train_loss: 1.4723620327795097, val_loss: 1.4888261955947124 (13 / 15)
train_acc: 0.3967861557478368, val_acc: 0.3694581280788177, train_loss: 1.4650953925584245, val_loss: 1.4862674433609535 (14 / 15)
train_acc: 0.3819530284301607, val_acc: 0.3694581280788177, train_loss: 1.4701072014453826, val_loss: 1.4847515093281938 (15 / 15)



val accuracy 0.37438423645320196
val loss 1.5010737468456399
-------------------------------------
lr 0.00042575868391387766, batch 13, decay 2.2757326883026235e-07, gamma 0.039059332187255144
train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.786509048364071, val_loss: 1.778097322421708 (1 / 15)
train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7687608759541742, val_loss: 1.7538399009281778 (2 / 15)
train_acc: 0.1681087762669963, val_acc: 0.18226600985221675, train_loss: 1.758787471962211, val_loss: 1.7400234021576755 (3 / 15)
train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7408022329450685, val_loss: 1.7230157828683337 (4 / 15)
train_acc: 0.21137206427688504, val_acc: 0.22660098522167488, train_loss: 1.7194661264513862, val_loss: 1.6998425222969995 (5 / 15)
train_acc: 0.2521631644004944, val_acc: 0.3103448275862069, train_loss: 1.689331822253865, val_loss: 1.6727303519037557 (6 / 15)
train_acc: 0.29666254635352285, val_acc: 0.29064039408866993, train_loss: 1.665688154576588, val_loss: 1.6601730890461963 (7 / 15)
train_acc: 0.30778739184178, val_acc: 0.2857142857142857, train_loss: 1.6569618141400653, val_loss: 1.6514657894378812 (8 / 15)
train_acc: 0.31396786155747836, val_acc: 0.3054187192118227, train_loss: 1.6449197261088857, val_loss: 1.6450939618895206 (9 / 15)
train_acc: 0.3374536464771323, val_acc: 0.31527093596059114, train_loss: 1.6383871288028429, val_loss: 1.6389514601289346 (10 / 15)
train_acc: 0.3473423980222497, val_acc: 0.31527093596059114, train_loss: 1.6407536044256354, val_loss: 1.6333206163838578 (11 / 15)
train_acc: 0.315203955500618, val_acc: 0.32019704433497537, train_loss: 1.6317952532408706, val_loss: 1.6288228719105273 (12 / 15)
train_acc: 0.3003708281829419, val_acc: 0.31527093596059114, train_loss: 1.6208482518036964, val_loss: 1.6285545714383054 (13 / 15)
train_acc: 0.315203955500618, val_acc: 0.31527093596059114, train_loss: 1.6309471985023603, val_loss: 1.628327312727867 (14 / 15)
train_acc: 0.3127317676143387, val_acc: 0.3251231527093596, train_loss: 1.6327475212118387, val_loss: 1.6280867257728953 (15 / 15)



val accuracy 0.3251231527093596
val loss 1.6280867257728953
-------------------------------------
lr 0.0002211346551293342, batch 15, decay 2.5577105310422304e-07, gamma 0.016191525265173642
train_acc: 0.18046971569839307, val_acc: 0.2955665024630542, train_loss: 1.7897221420250364, val_loss: 1.7877693728273139 (1 / 15)
train_acc: 0.1915945611866502, val_acc: 0.2019704433497537, train_loss: 1.7852027593053905, val_loss: 1.7818944894621525 (2 / 15)
train_acc: 0.20024721878862795, val_acc: 0.18226600985221675, train_loss: 1.77918569605489, val_loss: 1.7744544662278274 (3 / 15)
train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7722921294835943, val_loss: 1.7642953842144293 (4 / 15)
train_acc: 0.19901112484548825, val_acc: 0.18226600985221675, train_loss: 1.762718433797433, val_loss: 1.7541425909314836 (5 / 15)
train_acc: 0.207663782447466, val_acc: 0.18719211822660098, train_loss: 1.7558348856691377, val_loss: 1.7463688727082878 (6 / 15)
train_acc: 0.19283065512978986, val_acc: 0.18719211822660098, train_loss: 1.7536595290611936, val_loss: 1.7462522167290373 (7 / 15)
train_acc: 0.19406674907292953, val_acc: 0.18719211822660098, train_loss: 1.7530376716656209, val_loss: 1.746135351105864 (8 / 15)
train_acc: 0.2126081582200247, val_acc: 0.18719211822660098, train_loss: 1.7460718262475559, val_loss: 1.7459577280899574 (9 / 15)
train_acc: 0.20024721878862795, val_acc: 0.18719211822660098, train_loss: 1.749279437165325, val_loss: 1.7458100536186707 (10 / 15)
train_acc: 0.1965389369592089, val_acc: 0.18719211822660098, train_loss: 1.7530497862175751, val_loss: 1.745687913424863 (11 / 15)
train_acc: 0.20148331273176762, val_acc: 0.18719211822660098, train_loss: 1.7507840729467063, val_loss: 1.7455412566368216 (12 / 15)
train_acc: 0.20642768850432633, val_acc: 0.18719211822660098, train_loss: 1.7521940942747778, val_loss: 1.74554066998618 (13 / 15)
train_acc: 0.20395550061804696, val_acc: 0.18719211822660098, train_loss: 1.7509916468220676, val_loss: 1.7455395683279178 (14 / 15)
train_acc: 0.21631644004944375, val_acc: 0.18719211822660098, train_loss: 1.7531905685573337, val_loss: 1.7455387661609743 (15 / 15)



val accuracy 0.2955665024630542
val loss 1.7877693728273139
-------------------------------------
lr 0.00021732212768881969, batch 12, decay 1.4223581566368751e-06, gamma 0.010647767270529278
train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.789909594727977, val_loss: 1.7873665082630852 (1 / 15)
train_acc: 0.20642768850432633, val_acc: 0.18226600985221675, train_loss: 1.784297939873449, val_loss: 1.7812751913305573 (2 / 15)
train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7761411398686053, val_loss: 1.770813861503977 (3 / 15)
train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.76825248782508, val_loss: 1.7588929372468018 (4 / 15)
train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7592005486541389, val_loss: 1.7494858808705371 (5 / 15)
train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7510477198363823, val_loss: 1.7409133159468326 (6 / 15)
train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7399287810284363, val_loss: 1.7407915639172633 (7 / 15)
train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7425900181969547, val_loss: 1.7406396736652392 (8 / 15)
train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7389310623423573, val_loss: 1.7404957187586818 (9 / 15)
train_acc: 0.19777503090234858, val_acc: 0.18226600985221675, train_loss: 1.7421638888982671, val_loss: 1.7403517479967014 (10 / 15)
train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7462417949704805, val_loss: 1.7402136795626486 (11 / 15)
train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7446691183872953, val_loss: 1.7400692895128222 (12 / 15)
train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.741444896119192, val_loss: 1.7400687322240744 (13 / 15)
train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.7439164888431469, val_loss: 1.7400681021178297 (14 / 15)
train_acc: 0.2088998763906057, val_acc: 0.18226600985221675, train_loss: 1.737037895047031, val_loss: 1.7400673768790484 (15 / 15)



val accuracy 0.18226600985221675
val loss 1.7873665082630852
-------------------------------------
lr 0.000324849436178335, batch 13, decay 1.0909178174295089e-08, gamma 0.22856182017762505
train_acc: 0.18912237330037082, val_acc: 0.1625615763546798, train_loss: 1.7897356897408352, val_loss: 1.7858177758202765 (1 / 15)
train_acc: 0.1668726823238566, val_acc: 0.18226600985221675, train_loss: 1.7809114339767191, val_loss: 1.7723330605793468 (2 / 15)
train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.7663773229007227, val_loss: 1.753998041152954 (3 / 15)
train_acc: 0.20148331273176762, val_acc: 0.21182266009852216, train_loss: 1.7536273702555445, val_loss: 1.7472255370887042 (4 / 15)
train_acc: 0.21508034610630408, val_acc: 0.17733990147783252, train_loss: 1.7484442776596296, val_loss: 1.7325474975144335 (5 / 15)
train_acc: 0.22991347342398022, val_acc: 0.2413793103448276, train_loss: 1.738476151588968, val_loss: 1.7137203938855325 (6 / 15)
train_acc: 0.242274412855377, val_acc: 0.29064039408866993, train_loss: 1.7097948682028226, val_loss: 1.6985718287857883 (7 / 15)
train_acc: 0.2904820766378245, val_acc: 0.2512315270935961, train_loss: 1.6898559896259284, val_loss: 1.6797494876560906 (8 / 15)
train_acc: 0.29171817058096416, val_acc: 0.2955665024630542, train_loss: 1.6807636446946925, val_loss: 1.6524538477066115 (9 / 15)
train_acc: 0.3238566131025958, val_acc: 0.31527093596059114, train_loss: 1.6434736922291213, val_loss: 1.6099387665687523 (10 / 15)
train_acc: 0.3522867737948084, val_acc: 0.31527093596059114, train_loss: 1.6080091832152699, val_loss: 1.573384975564891 (11 / 15)
train_acc: 0.34981458590852904, val_acc: 0.2857142857142857, train_loss: 1.5602765373600134, val_loss: 1.5601466811936477 (12 / 15)
train_acc: 0.34981458590852904, val_acc: 0.3054187192118227, train_loss: 1.528689749161304, val_loss: 1.5300814641520308 (13 / 15)
train_acc: 0.3510506798516687, val_acc: 0.3399014778325123, train_loss: 1.5364951207699675, val_loss: 1.5209543728476087 (14 / 15)
train_acc: 0.3658838071693449, val_acc: 0.32019704433497537, train_loss: 1.5215316335998743, val_loss: 1.515241032163498 (15 / 15)



val accuracy 0.3399014778325123
val loss 1.5209543728476087
-------------------------------------
lr 0.0003253864620810195, batch 4, decay 4.847897488015508e-09, gamma 0.5973417918757363
train_acc: 0.17428924598269468, val_acc: 0.18226600985221675, train_loss: 1.781717401647155, val_loss: 1.7576534037519558 (1 / 15)
train_acc: 0.1841779975278121, val_acc: 0.19704433497536947, train_loss: 1.7519389763750752, val_loss: 1.7179744143791387 (2 / 15)
train_acc: 0.22744128553770088, val_acc: 0.3399014778325123, train_loss: 1.7086508424379327, val_loss: 1.6874039208360494 (3 / 15)
train_acc: 0.2620519159456119, val_acc: 0.39901477832512317, train_loss: 1.6487483885438539, val_loss: 1.6236702127409686 (4 / 15)
train_acc: 0.32138442521631644, val_acc: 0.3645320197044335, train_loss: 1.6089138010671025, val_loss: 1.5703981022529414 (5 / 15)
train_acc: 0.33498145859085293, val_acc: 0.3497536945812808, train_loss: 1.5838563308138194, val_loss: 1.6057580914990655 (6 / 15)
train_acc: 0.37082818294190356, val_acc: 0.35960591133004927, train_loss: 1.531624100264277, val_loss: 1.56990065187069 (7 / 15)
train_acc: 0.3646477132262052, val_acc: 0.3842364532019704, train_loss: 1.5290702168814774, val_loss: 1.56137807087358 (8 / 15)
train_acc: 0.3621755253399258, val_acc: 0.35960591133004927, train_loss: 1.5221965389581782, val_loss: 1.5552917595567375 (9 / 15)
train_acc: 0.3794808405438813, val_acc: 0.3842364532019704, train_loss: 1.5150588556332112, val_loss: 1.5406522351532734 (10 / 15)
train_acc: 0.39184177997527814, val_acc: 0.3645320197044335, train_loss: 1.4892421424462563, val_loss: 1.5288101763560855 (11 / 15)
train_acc: 0.3831891223733004, val_acc: 0.3054187192118227, train_loss: 1.4887008756878055, val_loss: 1.563875651124663 (12 / 15)
train_acc: 0.41903584672435107, val_acc: 0.35467980295566504, train_loss: 1.4544262499980196, val_loss: 1.5178721661638157 (13 / 15)
train_acc: 0.4079110012360939, val_acc: 0.3645320197044335, train_loss: 1.476865128474713, val_loss: 1.487903868036317 (14 / 15)
train_acc: 0.4079110012360939, val_acc: 0.39408866995073893, train_loss: 1.438475600868574, val_loss: 1.483409738892992 (15 / 15)



val accuracy 0.39901477832512317
val loss 1.6236702127409686
--------------------------------------------

{'lr': 0.000396845229881056, 'batch_size': 9, 'weight_decay': 2.6808725159406163e-07, 'gamma': 0.9871566226587425}, best val accuracy 0.4482758620689655, best val loss 1.3468167291486204
val accuracies
[0.4482758620689655, 0.22660098522167488, 0.41379310344827586, 0.22167487684729065, 0.26108374384236455, 0.22660098522167488, 0.4088669950738916, 0.39408866995073893, 0.18226600985221675, 0.3448275862068966, 0.2413793103448276, 0.20689655172413793, 0.35467980295566504, 0.3694581280788177, 0.37438423645320196, 0.3251231527093596, 0.2955665024630542, 0.18226600985221675, 0.3399014778325123, 0.39901477832512317]
val losses
[1.3468167291486204, 1.7890762037831573, 1.3463674865919968, 1.7652860316149708, 1.7781740033567832, 1.7869497044333096, 1.4173615075684534, 1.5122802648051032, 1.777766276462912, 1.5366655094870205, 1.7786324599693561, 1.7841441361187713, 1.6102208269053493, 1.464742455282822, 1.5010737468456399, 1.6280867257728953, 1.7877693728273139, 1.7873665082630852, 1.5209543728476087, 1.6236702127409686]