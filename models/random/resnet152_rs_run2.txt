

training set 809
validation set 203
---------------------------------------------
train_acc: 0.27070457354758964, val_acc: 0.20689655172413793, train_loss: 2.6246349375386466, val_loss: 1.9200861882693663 (1 / 15)
train_acc: 0.2719406674907293, val_acc: 0.35467980295566504, train_loss: 2.0816243363841354, val_loss: 1.673038169080988 (2 / 15)
train_acc: 0.34239802224969096, val_acc: 0.33004926108374383, train_loss: 1.7482172750277336, val_loss: 1.6274720677014054 (3 / 15)
train_acc: 0.34857849196538937, val_acc: 0.32019704433497537, train_loss: 1.6270234764580085, val_loss: 2.0659679051103264 (4 / 15)
train_acc: 0.34610630407911, val_acc: 0.3448275862068966, train_loss: 1.6142005554972532, val_loss: 1.4292492426087704 (5 / 15)
train_acc: 0.35599505562422745, val_acc: 0.3891625615763547, train_loss: 1.6140193844902502, val_loss: 1.5080209715025765 (6 / 15)
train_acc: 0.37082818294190356, val_acc: 0.39901477832512317, train_loss: 1.5423026350136888, val_loss: 1.5524500350059547 (7 / 15)
train_acc: 0.34857849196538937, val_acc: 0.33004926108374383, train_loss: 1.4742200737093818, val_loss: 1.4540104754452634 (8 / 15)
train_acc: 0.4103831891223733, val_acc: 0.3842364532019704, train_loss: 1.4653056070153292, val_loss: 1.6045973259827186 (9 / 15)
train_acc: 0.41409147095179233, val_acc: 0.43349753694581283, train_loss: 1.3467680072902306, val_loss: 1.3691707596990275 (10 / 15)
train_acc: 0.4437577255871446, val_acc: 0.4482758620689655, train_loss: 1.3266194255743982, val_loss: 1.3292738105275947 (11 / 15)
train_acc: 0.5030902348578492, val_acc: 0.3694581280788177, train_loss: 1.3034157956339079, val_loss: 1.551365589567006 (12 / 15)
train_acc: 0.4622991347342398, val_acc: 0.5369458128078818, train_loss: 1.3394362107195577, val_loss: 1.2700795463740533 (13 / 15)
train_acc: 0.4956736711990111, val_acc: 0.45320197044334976, train_loss: 1.2625677231952492, val_loss: 1.3155696762019191 (14 / 15)
train_acc: 0.5735475896168108, val_acc: 0.39408866995073893, train_loss: 1.0646440499497876, val_loss: 1.720111467568158 (15 / 15)

lr 0.003144561579141248, batch 8, decay 0, gamma 1, val accuracy 0.5369458128078818, val loss 1.2700795463740533 [1 / 20]
---------------------------------------------
train_acc: 0.242274412855377, val_acc: 0.2660098522167488, train_loss: 2.088896861919219, val_loss: 3.439218692591625 (1 / 15)
train_acc: 0.3176761433868974, val_acc: 0.2561576354679803, train_loss: 1.9503299149653526, val_loss: 2.4816665396901776 (2 / 15)
train_acc: 0.32138442521631644, val_acc: 0.26108374384236455, train_loss: 1.8990409138470852, val_loss: 2.0915278073014885 (3 / 15)
train_acc: 0.30284301606922126, val_acc: 0.3793103448275862, train_loss: 1.8220802653115817, val_loss: 1.6313348339108997 (4 / 15)
train_acc: 0.3374536464771323, val_acc: 0.3793103448275862, train_loss: 1.6867941021182482, val_loss: 1.4863293564378335 (5 / 15)
train_acc: 0.3868974042027194, val_acc: 0.3694581280788177, train_loss: 1.5982171951324595, val_loss: 1.4080784831728255 (6 / 15)
train_acc: 0.4042027194066749, val_acc: 0.35960591133004927, train_loss: 1.510193411293077, val_loss: 1.4440467401678339 (7 / 15)
train_acc: 0.3856613102595797, val_acc: 0.3448275862068966, train_loss: 1.4828538134307296, val_loss: 1.5480637576779708 (8 / 15)
train_acc: 0.41656365883807167, val_acc: 0.41379310344827586, train_loss: 1.489592752144275, val_loss: 1.5851523424017018 (9 / 15)
train_acc: 0.4289245982694685, val_acc: 0.4187192118226601, train_loss: 1.4179495654088459, val_loss: 1.7197160104225422 (10 / 15)
train_acc: 0.43757725587144625, val_acc: 0.35467980295566504, train_loss: 1.3171290390571058, val_loss: 1.8391087853849815 (11 / 15)
train_acc: 0.45488257107540175, val_acc: 0.3399014778325123, train_loss: 1.356784562982057, val_loss: 1.8277871503031313 (12 / 15)
train_acc: 0.48825710754017304, val_acc: 0.3694581280788177, train_loss: 1.3027997794935229, val_loss: 2.0896940031662363 (13 / 15)
train_acc: 0.4363411619283066, val_acc: 0.37438423645320196, train_loss: 1.3622028149839385, val_loss: 1.5828855877439376 (14 / 15)
train_acc: 0.5179233621755254, val_acc: 0.37438423645320196, train_loss: 1.2223203608073174, val_loss: 1.9008326909224975 (15 / 15)

lr 0.0013347088491115402, batch 8, decay 0, gamma 1, val accuracy 0.4187192118226601, val loss 1.7197160104225422 [2 / 20]
---------------------------------------------
train_acc: 0.25092707045735474, val_acc: 0.18226600985221675, train_loss: 3.652868189534681, val_loss: 2.2026322351887897 (1 / 15)
train_acc: 0.2867737948084054, val_acc: 0.31527093596059114, train_loss: 1.9138938462631074, val_loss: 1.630749791713771 (2 / 15)
train_acc: 0.30778739184178, val_acc: 0.3103448275862069, train_loss: 1.628581307285059, val_loss: 6.139042132300109 (3 / 15)
train_acc: 0.311495673671199, val_acc: 0.35467980295566504, train_loss: 1.7137505539562852, val_loss: 1.8902001797859305 (4 / 15)
train_acc: 0.3399258343634116, val_acc: 0.35467980295566504, train_loss: 1.595126521602107, val_loss: 1.6197947231419567 (5 / 15)
train_acc: 0.3572311495673671, val_acc: 0.3891625615763547, train_loss: 1.4617051359160131, val_loss: 1.7648464993303046 (6 / 15)
train_acc: 0.377008652657602, val_acc: 0.35960591133004927, train_loss: 1.45916335957012, val_loss: 1.5162181877737562 (7 / 15)
train_acc: 0.37824474660074164, val_acc: 0.3497536945812808, train_loss: 1.4644832784961563, val_loss: 1.6185160106038812 (8 / 15)
train_acc: 0.3621755253399258, val_acc: 0.37438423645320196, train_loss: 1.442002846372437, val_loss: 2.5276239440946155 (9 / 15)
train_acc: 0.34487021013597036, val_acc: 0.4039408866995074, train_loss: 1.5520566937655247, val_loss: 1.4400399309661 (10 / 15)
train_acc: 0.38813349814585907, val_acc: 0.3694581280788177, train_loss: 1.3820810933932386, val_loss: 1.4417135439482816 (11 / 15)
train_acc: 0.39184177997527814, val_acc: 0.3645320197044335, train_loss: 1.368093237151911, val_loss: 1.814490068722241 (12 / 15)
train_acc: 0.380716934487021, val_acc: 0.39901477832512317, train_loss: 1.3903315476935048, val_loss: 1.5612668703342307 (13 / 15)
train_acc: 0.38442521631644005, val_acc: 0.3448275862068966, train_loss: 1.4823025157041987, val_loss: 1.3195255272494162 (14 / 15)
train_acc: 0.37824474660074164, val_acc: 0.3891625615763547, train_loss: 1.3869765314684515, val_loss: 1.3469504405712258 (15 / 15)

lr 0.008344516267296516, batch 8, decay 0, gamma 1, val accuracy 0.4039408866995074, val loss 1.4400399309661 [3 / 20]
---------------------------------------------
train_acc: 0.22991347342398022, val_acc: 0.2512315270935961, train_loss: 2.906363638723442, val_loss: 2.3298580587791107 (1 / 15)
train_acc: 0.2583436341161928, val_acc: 0.30049261083743845, train_loss: 2.192244395926797, val_loss: 2.3628424752521986 (2 / 15)
train_acc: 0.2843016069221261, val_acc: 0.29064039408866993, train_loss: 1.8720558022686518, val_loss: 1.819396646151989 (3 / 15)
train_acc: 0.33250927070457353, val_acc: 0.3103448275862069, train_loss: 1.654956345652473, val_loss: 1.9401477921772472 (4 / 15)
train_acc: 0.34857849196538937, val_acc: 0.35467980295566504, train_loss: 1.5969774967660422, val_loss: 1.6896629867882564 (5 / 15)
train_acc: 0.3547589616810878, val_acc: 0.39408866995073893, train_loss: 1.6160659106317055, val_loss: 1.389127902796703 (6 / 15)
train_acc: 0.3757725587144623, val_acc: 0.3054187192118227, train_loss: 1.476309231686209, val_loss: 2.433867326510951 (7 / 15)
train_acc: 0.35599505562422745, val_acc: 0.4187192118226601, train_loss: 1.453188481230671, val_loss: 2.3001119990654177 (8 / 15)
train_acc: 0.41903584672435107, val_acc: 0.4039408866995074, train_loss: 1.3910128849251162, val_loss: 2.353429455768886 (9 / 15)
train_acc: 0.377008652657602, val_acc: 0.3793103448275862, train_loss: 1.4178783878849817, val_loss: 1.8663866132350977 (10 / 15)
train_acc: 0.38813349814585907, val_acc: 0.3054187192118227, train_loss: 1.3444005921390945, val_loss: 2.666084224367377 (11 / 15)
train_acc: 0.3992583436341162, val_acc: 0.4039408866995074, train_loss: 1.423340325891898, val_loss: 1.6355611173977405 (12 / 15)
train_acc: 0.3720642768850433, val_acc: 0.41379310344827586, train_loss: 1.4116498592904825, val_loss: 1.3857753317931603 (13 / 15)
train_acc: 0.44252163164400493, val_acc: 0.3891625615763547, train_loss: 1.336513475080945, val_loss: 1.6794178362550407 (14 / 15)
train_acc: 0.4326328800988875, val_acc: 0.3448275862068966, train_loss: 1.3662996421785674, val_loss: 1.5776312506844845 (15 / 15)

lr 0.004695569268423923, batch 8, decay 0, gamma 1, val accuracy 0.4187192118226601, val loss 2.3001119990654177 [4 / 20]
---------------------------------------------
train_acc: 0.2311495673671199, val_acc: 0.2315270935960591, train_loss: 2.9568434339223892, val_loss: 2.4303612039594227 (1 / 15)
train_acc: 0.276885043263288, val_acc: 0.29064039408866993, train_loss: 1.8633579099723523, val_loss: 1.790909654401206 (2 / 15)
train_acc: 0.32509270704573545, val_acc: 0.39408866995073893, train_loss: 1.7649681070089636, val_loss: 1.5344034850303763 (3 / 15)
train_acc: 0.3510506798516687, val_acc: 0.37438423645320196, train_loss: 1.5875772920026179, val_loss: 1.9722316875833596 (4 / 15)
train_acc: 0.36093943139678614, val_acc: 0.41379310344827586, train_loss: 1.5282670817946915, val_loss: 1.4408667997773645 (5 / 15)
train_acc: 0.3522867737948084, val_acc: 0.4039408866995074, train_loss: 1.5721517393114541, val_loss: 2.6227773774433607 (6 / 15)
train_acc: 0.3856613102595797, val_acc: 0.3645320197044335, train_loss: 1.518471586394811, val_loss: 2.311651944526898 (7 / 15)
train_acc: 0.4289245982694685, val_acc: 0.3645320197044335, train_loss: 1.4098858924672395, val_loss: 4.0536310202969705 (8 / 15)
train_acc: 0.3572311495673671, val_acc: 0.4187192118226601, train_loss: 1.5716202232540022, val_loss: 1.3519367696029212 (9 / 15)
train_acc: 0.3646477132262052, val_acc: 0.31527093596059114, train_loss: 1.4421101359118638, val_loss: 2.404779876100606 (10 / 15)
train_acc: 0.4177997527812114, val_acc: 0.42857142857142855, train_loss: 1.3631220836427214, val_loss: 1.341339082553469 (11 / 15)
train_acc: 0.4326328800988875, val_acc: 0.4630541871921182, train_loss: 1.3602246330459273, val_loss: 2.344619427995729 (12 / 15)
train_acc: 0.40173053152039556, val_acc: 0.3891625615763547, train_loss: 1.402616522073451, val_loss: 1.5086134836591523 (13 / 15)
train_acc: 0.45241038318912236, val_acc: 0.4482758620689655, train_loss: 1.2982510478593805, val_loss: 1.569345859764832 (14 / 15)
train_acc: 0.484548825710754, val_acc: 0.4088669950738916, train_loss: 1.237742036470524, val_loss: 2.5734252876836092 (15 / 15)

lr 0.006877668418173347, batch 8, decay 0, gamma 1, val accuracy 0.4630541871921182, val loss 2.344619427995729 [5 / 20]
---------------------------------------------
train_acc: 0.25092707045735474, val_acc: 0.2660098522167488, train_loss: 2.558036805967465, val_loss: 2.2191439947764864 (1 / 15)
train_acc: 0.27070457354758964, val_acc: 0.2413793103448276, train_loss: 1.9102641663828357, val_loss: 2.0952870528686223 (2 / 15)
train_acc: 0.26452410383189123, val_acc: 0.2660098522167488, train_loss: 1.9823153033686804, val_loss: 2.390700537289305 (3 / 15)
train_acc: 0.3621755253399258, val_acc: 0.33497536945812806, train_loss: 1.699685150672243, val_loss: 1.7986673780262763 (4 / 15)
train_acc: 0.35970333745364647, val_acc: 0.2857142857142857, train_loss: 1.5575638483422354, val_loss: 2.1191373617191034 (5 / 15)
train_acc: 0.3856613102595797, val_acc: 0.3842364532019704, train_loss: 1.5525252565612604, val_loss: 1.4561225463604104 (6 / 15)
train_acc: 0.3930778739184178, val_acc: 0.3694581280788177, train_loss: 1.5631677815587028, val_loss: 1.6913710292336976 (7 / 15)
train_acc: 0.3720642768850433, val_acc: 0.41379310344827586, train_loss: 1.5332052368758193, val_loss: 1.34392883507489 (8 / 15)
train_acc: 0.3980222496909765, val_acc: 0.3251231527093596, train_loss: 1.4378888548083595, val_loss: 2.0550832219898996 (9 / 15)
train_acc: 0.3930778739184178, val_acc: 0.3793103448275862, train_loss: 1.417513604217171, val_loss: 1.5142288296093493 (10 / 15)
train_acc: 0.41285537700865266, val_acc: 0.3891625615763547, train_loss: 1.3859882519772675, val_loss: 1.6120963020277728 (11 / 15)
train_acc: 0.40296662546353523, val_acc: 0.4039408866995074, train_loss: 1.4137871215311058, val_loss: 1.557383958929278 (12 / 15)
train_acc: 0.4289245982694685, val_acc: 0.41379310344827586, train_loss: 1.3252577917242816, val_loss: 1.3997849689915849 (13 / 15)
train_acc: 0.4721878862793572, val_acc: 0.4827586206896552, train_loss: 1.3444471998768772, val_loss: 1.7886598591734035 (14 / 15)
train_acc: 0.5105067985166872, val_acc: 0.4630541871921182, train_loss: 1.1943380865089972, val_loss: 1.4002988303236186 (15 / 15)

lr 0.0021239933194290893, batch 8, decay 0, gamma 1, val accuracy 0.4827586206896552, val loss 1.7886598591734035 [6 / 20]
---------------------------------------------
train_acc: 0.2138442521631644, val_acc: 0.22660098522167488, train_loss: 3.2073553318735697, val_loss: 3.8796188161878162 (1 / 15)
train_acc: 0.26081582200247216, val_acc: 0.23645320197044334, train_loss: 2.134310932772416, val_loss: 3.4571612650537724 (2 / 15)
train_acc: 0.2867737948084054, val_acc: 0.2512315270935961, train_loss: 1.734003448368445, val_loss: 2.0015924114899093 (3 / 15)
train_acc: 0.3300370828182942, val_acc: 0.3054187192118227, train_loss: 1.6770480978768894, val_loss: 1.920299655698203 (4 / 15)
train_acc: 0.33127317676143386, val_acc: 0.2857142857142857, train_loss: 1.5597728521773784, val_loss: 8.122476036912701 (5 / 15)
train_acc: 0.34981458590852904, val_acc: 0.37438423645320196, train_loss: 1.5806646349992977, val_loss: 1.5740695909913538 (6 / 15)
train_acc: 0.3288009888751545, val_acc: 0.35960591133004927, train_loss: 1.5445839318415733, val_loss: 2.0580867963471436 (7 / 15)
train_acc: 0.34487021013597036, val_acc: 0.4039408866995074, train_loss: 1.4818287118256313, val_loss: 1.4544435663176287 (8 / 15)
train_acc: 0.3720642768850433, val_acc: 0.37438423645320196, train_loss: 1.4191400575991466, val_loss: 1.5000362886583865 (9 / 15)
train_acc: 0.3868974042027194, val_acc: 0.32019704433497537, train_loss: 1.4248195608113103, val_loss: 1.5425310225909568 (10 / 15)
train_acc: 0.3831891223733004, val_acc: 0.3793103448275862, train_loss: 1.4234416263801943, val_loss: 1.5109472903124805 (11 / 15)
train_acc: 0.40914709517923364, val_acc: 0.35960591133004927, train_loss: 1.3864386697634778, val_loss: 1.662492460805207 (12 / 15)
train_acc: 0.37453646477132263, val_acc: 0.37438423645320196, train_loss: 1.433321649123476, val_loss: 1.4937987794429797 (13 / 15)
train_acc: 0.4042027194066749, val_acc: 0.3842364532019704, train_loss: 1.328434942974001, val_loss: 1.3550343766001058 (14 / 15)
train_acc: 0.43016069221260816, val_acc: 0.39901477832512317, train_loss: 1.3094186278886641, val_loss: 1.964410921035729 (15 / 15)

lr 0.008700778956298756, batch 8, decay 0, gamma 1, val accuracy 0.4039408866995074, val loss 1.4544435663176287 [7 / 20]
---------------------------------------------
train_acc: 0.2200247218788628, val_acc: 0.20689655172413793, train_loss: 2.541854657407744, val_loss: 1.9493606642549262 (1 / 15)
train_acc: 0.2620519159456119, val_acc: 0.2561576354679803, train_loss: 2.362105495555439, val_loss: 2.7133760176268704 (2 / 15)
train_acc: 0.3065512978986403, val_acc: 0.28078817733990147, train_loss: 1.9021032189556635, val_loss: 1.9662555967058455 (3 / 15)
train_acc: 0.3127317676143387, val_acc: 0.2660098522167488, train_loss: 2.0110205792084024, val_loss: 1.5853834809928105 (4 / 15)
train_acc: 0.3510506798516687, val_acc: 0.3399014778325123, train_loss: 1.7759982033624637, val_loss: 2.229248933604198 (5 / 15)
train_acc: 0.3522867737948084, val_acc: 0.3251231527093596, train_loss: 1.7486557615702172, val_loss: 1.9161262882166896 (6 / 15)
train_acc: 0.3683559950556242, val_acc: 0.2561576354679803, train_loss: 1.6058432786220083, val_loss: 2.252476394470102 (7 / 15)
train_acc: 0.3967861557478368, val_acc: 0.42857142857142855, train_loss: 1.495499027379511, val_loss: 1.6797993494372063 (8 / 15)
train_acc: 0.41656365883807167, val_acc: 0.33004926108374383, train_loss: 1.4272771966177396, val_loss: 1.9028998707315605 (9 / 15)
train_acc: 0.4400494437577256, val_acc: 0.3793103448275862, train_loss: 1.4242285622654502, val_loss: 1.4928373985102612 (10 / 15)
train_acc: 0.4400494437577256, val_acc: 0.3891625615763547, train_loss: 1.3643577717732736, val_loss: 1.8039042597333785 (11 / 15)
train_acc: 0.48702101359703337, val_acc: 0.4039408866995074, train_loss: 1.2438502022892937, val_loss: 1.6074547265550774 (12 / 15)
train_acc: 0.48084054388133496, val_acc: 0.3448275862068966, train_loss: 1.352782281279122, val_loss: 1.6263452674367744 (13 / 15)
train_acc: 0.511742892459827, val_acc: 0.458128078817734, train_loss: 1.2767123265378408, val_loss: 1.839869562628234 (14 / 15)
train_acc: 0.5525339925834364, val_acc: 0.3645320197044335, train_loss: 1.1438505170666537, val_loss: 2.171671674462962 (15 / 15)

lr 0.00177137687837888, batch 8, decay 0, gamma 1, val accuracy 0.458128078817734, val loss 1.839869562628234 [8 / 20]
---------------------------------------------
train_acc: 0.23980222496909764, val_acc: 0.3054187192118227, train_loss: 3.226726918933713, val_loss: 3.6494167320834006 (1 / 15)
train_acc: 0.2546353522867738, val_acc: 0.29064039408866993, train_loss: 2.034773399125513, val_loss: 1.7912229158608197 (2 / 15)
train_acc: 0.31025957972805934, val_acc: 0.3054187192118227, train_loss: 1.7314365437652626, val_loss: 1.6658425466180435 (3 / 15)
train_acc: 0.3065512978986403, val_acc: 0.30049261083743845, train_loss: 1.6794124197753604, val_loss: 4.150977324969663 (4 / 15)
train_acc: 0.3584672435105068, val_acc: 0.33497536945812806, train_loss: 1.5601264838087838, val_loss: 1.7906181712455937 (5 / 15)
train_acc: 0.35970333745364647, val_acc: 0.33004926108374383, train_loss: 1.5295779570955577, val_loss: 2.1345027508994043 (6 / 15)
train_acc: 0.3300370828182942, val_acc: 0.33497536945812806, train_loss: 1.51048277261378, val_loss: 1.4715416102573788 (7 / 15)
train_acc: 0.3572311495673671, val_acc: 0.33004926108374383, train_loss: 1.4598872794209068, val_loss: 2.9878534607112113 (8 / 15)
train_acc: 0.3621755253399258, val_acc: 0.35467980295566504, train_loss: 1.5488811665029254, val_loss: 2.3461396564991017 (9 / 15)
train_acc: 0.39184177997527814, val_acc: 0.3497536945812808, train_loss: 1.389329599066924, val_loss: 1.9323995876782045 (10 / 15)
train_acc: 0.40296662546353523, val_acc: 0.3793103448275862, train_loss: 1.3911895747532508, val_loss: 1.5578518553907648 (11 / 15)
train_acc: 0.3720642768850433, val_acc: 0.3645320197044335, train_loss: 1.3818126249372296, val_loss: 1.7777553855491977 (12 / 15)
train_acc: 0.4103831891223733, val_acc: 0.3103448275862069, train_loss: 1.3499942894182335, val_loss: 1.8433193203263682 (13 / 15)
train_acc: 0.3967861557478368, val_acc: 0.3891625615763547, train_loss: 1.3463825831749237, val_loss: 1.6255233505089295 (14 / 15)
train_acc: 0.4177997527812114, val_acc: 0.3497536945812808, train_loss: 1.3454815180251567, val_loss: 1.84149364884851 (15 / 15)

lr 0.008195524051216232, batch 8, decay 0, gamma 1, val accuracy 0.3891625615763547, val loss 1.6255233505089295 [9 / 20]
---------------------------------------------
train_acc: 0.22867737948084055, val_acc: 0.22660098522167488, train_loss: 3.1608307370444018, val_loss: 3.228670559493192 (1 / 15)
train_acc: 0.29913473423980225, val_acc: 0.3103448275862069, train_loss: 1.8013597981744114, val_loss: 1.6566755871467402 (2 / 15)
train_acc: 0.3300370828182942, val_acc: 0.3497536945812808, train_loss: 1.6972758711047462, val_loss: 1.696726128385572 (3 / 15)
train_acc: 0.33250927070457353, val_acc: 0.35467980295566504, train_loss: 1.5839383216075167, val_loss: 1.8368307927559162 (4 / 15)
train_acc: 0.33127317676143386, val_acc: 0.3645320197044335, train_loss: 1.585421735482988, val_loss: 1.757491043048539 (5 / 15)
train_acc: 0.33498145859085293, val_acc: 0.3891625615763547, train_loss: 1.5283104374323258, val_loss: 1.394014825374622 (6 / 15)
train_acc: 0.3522867737948084, val_acc: 0.37438423645320196, train_loss: 1.456294494742074, val_loss: 1.5828468036181822 (7 / 15)
train_acc: 0.3794808405438813, val_acc: 0.4039408866995074, train_loss: 1.4835655005517494, val_loss: 1.3841331345694405 (8 / 15)
train_acc: 0.3930778739184178, val_acc: 0.4039408866995074, train_loss: 1.406089175616855, val_loss: 1.4232882507916154 (9 / 15)
train_acc: 0.3943139678615575, val_acc: 0.3694581280788177, train_loss: 1.4189396941912336, val_loss: 1.4308961359738128 (10 / 15)
train_acc: 0.42027194066749074, val_acc: 0.32019704433497537, train_loss: 1.3261052807712437, val_loss: 2.024680662037704 (11 / 15)
train_acc: 0.44499381953028433, val_acc: 0.4482758620689655, train_loss: 1.3043539332518324, val_loss: 1.4332317736348494 (12 / 15)
train_acc: 0.4561186650185414, val_acc: 0.37438423645320196, train_loss: 1.2824720481712237, val_loss: 1.6166584303813616 (13 / 15)
train_acc: 0.4227441285537701, val_acc: 0.4236453201970443, train_loss: 1.4233299801759283, val_loss: 1.3579418089589461 (14 / 15)
train_acc: 0.4721878862793572, val_acc: 0.4876847290640394, train_loss: 1.2801324552010251, val_loss: 1.2610897051876988 (15 / 15)

lr 0.005374785246930523, batch 8, decay 0, gamma 1, val accuracy 0.4876847290640394, val loss 1.2610897051876988 [10 / 20]
---------------------------------------------
train_acc: 0.2620519159456119, val_acc: 0.18226600985221675, train_loss: 3.296462902916671, val_loss: 2.553881504265546 (1 / 15)
train_acc: 0.27564894932014833, val_acc: 0.20689655172413793, train_loss: 1.9242288055467076, val_loss: 2.983689127003618 (2 / 15)
train_acc: 0.29913473423980225, val_acc: 0.35960591133004927, train_loss: 1.81211354146045, val_loss: 1.7412197261021054 (3 / 15)
train_acc: 0.3263288009888752, val_acc: 0.3645320197044335, train_loss: 1.5903690766050436, val_loss: 1.678243139107239 (4 / 15)
train_acc: 0.3362175525339926, val_acc: 0.3103448275862069, train_loss: 1.5654383489611123, val_loss: 1.693117858153846 (5 / 15)
train_acc: 0.3226205191594561, val_acc: 0.35467980295566504, train_loss: 1.536474174268461, val_loss: 1.5314349558553084 (6 / 15)
train_acc: 0.37824474660074164, val_acc: 0.3891625615763547, train_loss: 1.4603631505270676, val_loss: 2.0340477468932203 (7 / 15)
train_acc: 0.3300370828182942, val_acc: 0.3448275862068966, train_loss: 1.5014861764660283, val_loss: 1.4556853477590777 (8 / 15)
train_acc: 0.37330037082818296, val_acc: 0.39901477832512317, train_loss: 1.418617057269819, val_loss: 1.3619242701037177 (9 / 15)
train_acc: 0.377008652657602, val_acc: 0.4236453201970443, train_loss: 1.4166235534752845, val_loss: 1.3248863954262193 (10 / 15)
train_acc: 0.380716934487021, val_acc: 0.35467980295566504, train_loss: 1.3806343396897665, val_loss: 1.5624534238148204 (11 / 15)
train_acc: 0.3856613102595797, val_acc: 0.3645320197044335, train_loss: 1.394257363047087, val_loss: 1.5823950937816076 (12 / 15)
train_acc: 0.3695920889987639, val_acc: 0.3645320197044335, train_loss: 1.4569011559739837, val_loss: 1.4352656362091967 (13 / 15)
train_acc: 0.411619283065513, val_acc: 0.39901477832512317, train_loss: 1.331222978451638, val_loss: 1.2899792740497682 (14 / 15)

---------------------------------------------------------------------------

KeyboardInterrupt                         Traceback (most recent call last)

<ipython-input-4-9f57ffdd1918> in <module>()
     39 
     40   net = resnet152(num_classes=NUM_CLASSES)
---> 41   current_net, val_accuracy, val_loss = train_network(net, net.parameters(), LR, NUM_EPOCHS, BATCH_SIZE, WEIGHT_DECAY, STEP_SIZE, GAMMA, train_dataset, val_dataset=val_dataset, verbosity=True, plot=True)
     42   val_accuracies.append(val_accuracy)
     43   val_losses.append(val_loss)

2 frames

/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     97     Variable._execution_engine.run_backward(
     98         tensors, grad_tensors, retain_graph, create_graph,
---> 99         allow_unreachable=True)  # allow_unreachable flag
    100 
    101 

KeyboardInterrupt: 

