training set 809
validation set 203
-------------------------------------
{'lr': 0.0009605105020413945, 'batch_size': 8, 'weight_decay': 1.1923305553979827e-06, 'gamma': 0.00433118420135304}
train_acc: 0.17799752781211373, val_acc: 0.19704433497536947, train_loss: 1.7849447890766177, val_loss: 1.769988350680309 (1 / 100)
train_acc: 0.17181705809641531, val_acc: 0.28078817733990147, train_loss: 1.7643280991517716, val_loss: 1.7478274565024916 (2 / 100)
train_acc: 0.20395550061804696, val_acc: 0.18226600985221675, train_loss: 1.763094927088734, val_loss: 1.7348777736936296 (3 / 100)
train_acc: 0.23485784919653893, val_acc: 0.26108374384236455, train_loss: 1.729027126569241, val_loss: 1.692833564551593 (4 / 100)
train_acc: 0.27935723114956734, val_acc: 0.2857142857142857, train_loss: 1.6654108475401022, val_loss: 1.7143301658442456 (5 / 100)
train_acc: 0.30284301606922126, val_acc: 0.270935960591133, train_loss: 1.643770363628496, val_loss: 1.5803389625596296 (6 / 100)
train_acc: 0.33250927070457353, val_acc: 0.31527093596059114, train_loss: 1.5700676868518881, val_loss: 1.518441369380857 (7 / 100)
train_acc: 0.34239802224969096, val_acc: 0.39901477832512317, train_loss: 1.5188140768939986, val_loss: 1.487303146294185 (8 / 100)
train_acc: 0.34363411619283063, val_acc: 0.3399014778325123, train_loss: 1.5358246021129291, val_loss: 1.5066844066375582 (9 / 100)
train_acc: 0.3683559950556242, val_acc: 0.43349753694581283, train_loss: 1.4367126021603276, val_loss: 1.436339361327035 (10 / 100)
train_acc: 0.35970333745364647, val_acc: 0.3694581280788177, train_loss: 1.4342868634590555, val_loss: 1.388116239914166 (11 / 100)
train_acc: 0.3695920889987639, val_acc: 0.39901477832512317, train_loss: 1.4370142302495441, val_loss: 1.370665864991437 (12 / 100)
train_acc: 0.37330037082818296, val_acc: 0.4088669950738916, train_loss: 1.4723256963439866, val_loss: 1.3920586919549651 (13 / 100)
train_acc: 0.3943139678615575, val_acc: 0.4482758620689655, train_loss: 1.369249689122213, val_loss: 1.2830601918873528 (14 / 100)
train_acc: 0.4264524103831891, val_acc: 0.43349753694581283, train_loss: 1.356589071830212, val_loss: 1.334406545009519 (15 / 100)
train_acc: 0.4215080346106304, val_acc: 0.3694581280788177, train_loss: 1.385472560842488, val_loss: 1.434971092369756 (16 / 100)
train_acc: 0.4326328800988875, val_acc: 0.42857142857142855, train_loss: 1.3245325265325634, val_loss: 1.2682313138041004 (17 / 100)
train_acc: 0.44128553770086526, val_acc: 0.4187192118226601, train_loss: 1.300168034762181, val_loss: 1.413006793689258 (18 / 100)
train_acc: 0.45488257107540175, val_acc: 0.4876847290640394, train_loss: 1.282852697726086, val_loss: 1.2275782482964652 (19 / 100)
train_acc: 0.45982694684796044, val_acc: 0.4827586206896552, train_loss: 1.2469256740565353, val_loss: 1.1865446617450621 (20 / 100)
train_acc: 0.48825710754017304, val_acc: 0.4876847290640394, train_loss: 1.2302058671401959, val_loss: 1.1883703857807104 (21 / 100)
train_acc: 0.4932014833127318, val_acc: 0.45320197044334976, train_loss: 1.2204586769948047, val_loss: 1.2340848287338106 (22 / 100)
train_acc: 0.4783683559950556, val_acc: 0.4482758620689655, train_loss: 1.203567565887319, val_loss: 1.1807303945419236 (23 / 100)
train_acc: 0.4956736711990111, val_acc: 0.47783251231527096, train_loss: 1.1895751826400662, val_loss: 1.1595719817823964 (24 / 100)
train_acc: 0.4932014833127318, val_acc: 0.47783251231527096, train_loss: 1.1427832439597074, val_loss: 1.2359732134001595 (25 / 100)
train_acc: 0.5327564894932015, val_acc: 0.5172413793103449, train_loss: 1.1318439040402104, val_loss: 1.1284613168885556 (26 / 100)
train_acc: 0.5426452410383189, val_acc: 0.5172413793103449, train_loss: 1.0866840003594775, val_loss: 1.0970356972346753 (27 / 100)
train_acc: 0.5451174289245982, val_acc: 0.5073891625615764, train_loss: 1.0928326787701055, val_loss: 1.124043857522786 (28 / 100)
train_acc: 0.5414091470951793, val_acc: 0.4975369458128079, train_loss: 1.050088636512662, val_loss: 1.1774577836097755 (29 / 100)
train_acc: 0.5797280593325093, val_acc: 0.5024630541871922, train_loss: 0.9811397793855892, val_loss: 1.0980136928887203 (30 / 100)
train_acc: 0.6291718170580964, val_acc: 0.5024630541871922, train_loss: 0.9474433887432179, val_loss: 1.1333736853646528 (31 / 100)
train_acc: 0.6056860321384425, val_acc: 0.4827586206896552, train_loss: 0.9685947523718269, val_loss: 1.3225777795162108 (32 / 100)
train_acc: 0.6526576019777504, val_acc: 0.5812807881773399, train_loss: 0.8886057813323767, val_loss: 1.0619566146963335 (33 / 100)
train_acc: 0.6674907292954264, val_acc: 0.5812807881773399, train_loss: 0.831836785904704, val_loss: 1.1526525723816725 (34 / 100)
train_acc: 0.6909765142150803, val_acc: 0.541871921182266, train_loss: 0.8064870698785016, val_loss: 1.0724302048753636 (35 / 100)
train_acc: 0.7280593325092707, val_acc: 0.5221674876847291, train_loss: 0.7386576226084725, val_loss: 1.3857591762918557 (36 / 100)
train_acc: 0.6971569839307787, val_acc: 0.6354679802955665, train_loss: 0.7499120842541104, val_loss: 0.899597328871929 (37 / 100)
train_acc: 0.7873918417799752, val_acc: 0.6009852216748769, train_loss: 0.5736295836522936, val_loss: 1.1240809951803368 (38 / 100)
train_acc: 0.7911001236093943, val_acc: 0.5566502463054187, train_loss: 0.5680641220290817, val_loss: 1.157919485580745 (39 / 100)
train_acc: 0.8046971569839307, val_acc: 0.5615763546798029, train_loss: 0.5236230208935637, val_loss: 1.4095153585076332 (40 / 100)
train_acc: 0.8405438813349815, val_acc: 0.6354679802955665, train_loss: 0.43812267535696514, val_loss: 1.0710394370732048 (41 / 100)
overfit -> train_accuracy 0.8281829419035847, val_accuracy 0.5517241379310345
lr 0.0009605105020413945, batch 8, decay 1.1923305553979827e-06, gamma 0.00433118420135304, val accuracy 0.6354679802955665, val loss 0.899597328871929 [1 / 50]
-------------------------------------
{'lr': 0.0013817157823343665, 'batch_size': 8, 'weight_decay': 1.698514373794663e-05, 'gamma': 0.0019419152163696607}
train_acc: 0.17428924598269468, val_acc: 0.22167487684729065, train_loss: 1.7791680631001003, val_loss: 1.7557941452035764 (1 / 100)
train_acc: 0.20024721878862795, val_acc: 0.21182266009852216, train_loss: 1.7589330964978458, val_loss: 1.739792711629069 (2 / 100)
train_acc: 0.23485784919653893, val_acc: 0.2660098522167488, train_loss: 1.7397506098517381, val_loss: 1.6936947577105368 (3 / 100)
train_acc: 0.26823238566131025, val_acc: 0.270935960591133, train_loss: 1.680294215458138, val_loss: 1.6531045378135343 (4 / 100)
train_acc: 0.311495673671199, val_acc: 0.2561576354679803, train_loss: 1.6017039184664619, val_loss: 1.5781576151918308 (5 / 100)
train_acc: 0.3362175525339926, val_acc: 0.3793103448275862, train_loss: 1.5857719899108, val_loss: 1.4404211637421782 (6 / 100)
train_acc: 0.34981458590852904, val_acc: 0.2561576354679803, train_loss: 1.5531566334595934, val_loss: 1.5937065197329217 (7 / 100)
train_acc: 0.3510506798516687, val_acc: 0.3645320197044335, train_loss: 1.5050489917231726, val_loss: 1.4029756520182042 (8 / 100)
train_acc: 0.380716934487021, val_acc: 0.4187192118226601, train_loss: 1.4738783600598537, val_loss: 1.3110084457350482 (9 / 100)
train_acc: 0.411619283065513, val_acc: 0.4088669950738916, train_loss: 1.4356740850158616, val_loss: 1.4784014806371604 (10 / 100)
train_acc: 0.37824474660074164, val_acc: 0.39408866995073893, train_loss: 1.384679002726476, val_loss: 1.3318745778699226 (11 / 100)
train_acc: 0.38813349814585907, val_acc: 0.41379310344827586, train_loss: 1.35723456965094, val_loss: 1.2594102914697431 (12 / 100)
train_acc: 0.40296662546353523, val_acc: 0.39901477832512317, train_loss: 1.3432273528779246, val_loss: 1.2815693243971011 (13 / 100)
train_acc: 0.4338689740420272, val_acc: 0.43842364532019706, train_loss: 1.3431276498530498, val_loss: 1.2495643117744935 (14 / 100)
train_acc: 0.40667490729295425, val_acc: 0.46798029556650245, train_loss: 1.3124889833818112, val_loss: 1.233721019599238 (15 / 100)
train_acc: 0.4363411619283066, val_acc: 0.4729064039408867, train_loss: 1.2926990425925615, val_loss: 1.3085452588320954 (16 / 100)
train_acc: 0.4783683559950556, val_acc: 0.5073891625615764, train_loss: 1.2358071612486585, val_loss: 1.1824292238122724 (17 / 100)
train_acc: 0.46971569839307786, val_acc: 0.5024630541871922, train_loss: 1.225754331157293, val_loss: 1.1712763224329268 (18 / 100)
train_acc: 0.49938195302843014, val_acc: 0.42857142857142855, train_loss: 1.1857919507327275, val_loss: 1.2162915970304329 (19 / 100)
train_acc: 0.484548825710754, val_acc: 0.5024630541871922, train_loss: 1.2324461830855888, val_loss: 1.1310436543572713 (20 / 100)
train_acc: 0.5203955500618047, val_acc: 0.5221674876847291, train_loss: 1.1562782055957355, val_loss: 1.1453456074146215 (21 / 100)
train_acc: 0.5241038318912238, val_acc: 0.5024630541871922, train_loss: 1.1309735319670404, val_loss: 1.217001794594262 (22 / 100)
train_acc: 0.5352286773794809, val_acc: 0.49261083743842365, train_loss: 1.105895516604222, val_loss: 1.1501539552153037 (23 / 100)
train_acc: 0.5488257107540173, val_acc: 0.5123152709359606, train_loss: 1.0601023398754181, val_loss: 1.1120960060598815 (24 / 100)
train_acc: 0.5747836835599506, val_acc: 0.4975369458128079, train_loss: 0.9874418162002846, val_loss: 1.2499565984227974 (25 / 100)
train_acc: 0.5920889987639061, val_acc: 0.5320197044334976, train_loss: 0.9692528289092338, val_loss: 1.1197419471928638 (26 / 100)
train_acc: 0.6402966625463535, val_acc: 0.5369458128078818, train_loss: 0.9074448702214527, val_loss: 1.2420181094719271 (27 / 100)
train_acc: 0.6600741656365884, val_acc: 0.5467980295566502, train_loss: 0.8759222861715537, val_loss: 1.1032942827111982 (28 / 100)
train_acc: 0.6687268232385661, val_acc: 0.45320197044334976, train_loss: 0.800086731963753, val_loss: 1.469937592304399 (29 / 100)
train_acc: 0.6934487021013597, val_acc: 0.5517241379310345, train_loss: 0.790132066228039, val_loss: 1.276987349458516 (30 / 100)
train_acc: 0.7058096415327565, val_acc: 0.5714285714285714, train_loss: 0.7473612515387046, val_loss: 1.1136927055607875 (31 / 100)
train_acc: 0.7330037082818294, val_acc: 0.5763546798029556, train_loss: 0.7370121540923054, val_loss: 1.2088975944542533 (32 / 100)
train_acc: 0.7453646477132262, val_acc: 0.5714285714285714, train_loss: 0.6374611940018473, val_loss: 1.1418928750630082 (33 / 100)
train_acc: 0.7935723114956736, val_acc: 0.5566502463054187, train_loss: 0.5324884692876979, val_loss: 1.3563485218973583 (34 / 100)
overfit -> train_accuracy 0.8380716934487021, val_accuracy 0.5714285714285714
lr 0.0013817157823343665, batch 8, decay 1.698514373794663e-05, gamma 0.0019419152163696607, val accuracy 0.5763546798029556, val loss 1.2088975944542533 [2 / 50]
-------------------------------------
{'lr': 0.002162837005669799, 'batch_size': 8, 'weight_decay': 8.610650708706833e-06, 'gamma': 0.009945362963723885}
train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.778599164541926, val_loss: 1.7499181484353954 (1 / 100)
train_acc: 0.23362175525339926, val_acc: 0.19704433497536947, train_loss: 1.750899739996317, val_loss: 1.7124772664948638 (2 / 100)
train_acc: 0.24474660074165636, val_acc: 0.33497536945812806, train_loss: 1.7289045920920165, val_loss: 1.6463088290444736 (3 / 100)
train_acc: 0.2954264524103832, val_acc: 0.4039408866995074, train_loss: 1.674826889603925, val_loss: 1.4935673915693912 (4 / 100)
train_acc: 0.32014833127317677, val_acc: 0.33004926108374383, train_loss: 1.5795201990012038, val_loss: 1.4825338683104867 (5 / 100)
train_acc: 0.33868974042027195, val_acc: 0.3645320197044335, train_loss: 1.5523371333981622, val_loss: 1.5082765700194636 (6 / 100)
train_acc: 0.3695920889987639, val_acc: 0.35467980295566504, train_loss: 1.4752792892114193, val_loss: 1.381637068804849 (7 / 100)
train_acc: 0.38813349814585907, val_acc: 0.3054187192118227, train_loss: 1.4431360053780347, val_loss: 1.503635989034117 (8 / 100)
train_acc: 0.37330037082818296, val_acc: 0.4088669950738916, train_loss: 1.4050886032755208, val_loss: 1.2745367491186546 (9 / 100)
train_acc: 0.4103831891223733, val_acc: 0.39408866995073893, train_loss: 1.3579347960291746, val_loss: 1.3468265674384357 (10 / 100)
train_acc: 0.41409147095179233, val_acc: 0.43842364532019706, train_loss: 1.331786031628716, val_loss: 1.2738967347027632 (11 / 100)
train_acc: 0.41285537700865266, val_acc: 0.3891625615763547, train_loss: 1.338402155155894, val_loss: 1.3134176857365762 (12 / 100)
train_acc: 0.44499381953028433, val_acc: 0.43842364532019706, train_loss: 1.2797964165915074, val_loss: 1.258602036631166 (13 / 100)
train_acc: 0.43016069221260816, val_acc: 0.46798029556650245, train_loss: 1.2851382707635906, val_loss: 1.228052625515191 (14 / 100)
train_acc: 0.45488257107540175, val_acc: 0.4876847290640394, train_loss: 1.2779807304717112, val_loss: 1.2079308391204608 (15 / 100)
train_acc: 0.47342398022249693, val_acc: 0.4482758620689655, train_loss: 1.2280055946119046, val_loss: 1.2345971573749785 (16 / 100)
train_acc: 0.4857849196538937, val_acc: 0.46798029556650245, train_loss: 1.2170675337240928, val_loss: 1.1570631048362243 (17 / 100)
train_acc: 0.48825710754017304, val_acc: 0.5320197044334976, train_loss: 1.1723508211531952, val_loss: 1.1157246557949798 (18 / 100)
train_acc: 0.5315203955500618, val_acc: 0.5172413793103449, train_loss: 1.0886207216307613, val_loss: 1.2271407067482107 (19 / 100)
train_acc: 0.5550061804697157, val_acc: 0.5024630541871922, train_loss: 1.0582644043511011, val_loss: 1.2864529394310684 (20 / 100)
train_acc: 0.5599505562422744, val_acc: 0.47783251231527096, train_loss: 1.0199660671361446, val_loss: 1.340315883382788 (21 / 100)
train_acc: 0.5822002472187886, val_acc: 0.46798029556650245, train_loss: 0.9884949417137835, val_loss: 1.215118344781434 (22 / 100)
train_acc: 0.6044499381953028, val_acc: 0.4827586206896552, train_loss: 0.9648060383107075, val_loss: 1.2459801039085012 (23 / 100)
train_acc: 0.6328800988875154, val_acc: 0.5024630541871922, train_loss: 0.9334317243880188, val_loss: 1.1446115453842238 (24 / 100)
train_acc: 0.6316440049443758, val_acc: 0.5615763546798029, train_loss: 0.8949682759708173, val_loss: 1.1985353472197584 (25 / 100)
train_acc: 0.6860321384425216, val_acc: 0.541871921182266, train_loss: 0.7720820072702188, val_loss: 1.383161436748035 (26 / 100)
train_acc: 0.6773794808405439, val_acc: 0.6157635467980296, train_loss: 0.8058929844161931, val_loss: 1.0175374276532327 (27 / 100)
train_acc: 0.7033374536464772, val_acc: 0.5960591133004927, train_loss: 0.7416868777151309, val_loss: 1.1717309655227097 (28 / 100)
train_acc: 0.7515451174289246, val_acc: 0.5812807881773399, train_loss: 0.6217221577176352, val_loss: 1.0732411874338912 (29 / 100)
train_acc: 0.7737948084054388, val_acc: 0.6502463054187192, train_loss: 0.5807704613146882, val_loss: 0.9713151466670294 (30 / 100)
train_acc: 0.7948084054388134, val_acc: 0.6157635467980296, train_loss: 0.5268103659374015, val_loss: 1.486501742172711 (31 / 100)
overfit -> train_accuracy 0.8318912237330037, val_accuracy 0.5665024630541872
lr 0.002162837005669799, batch 8, decay 8.610650708706833e-06, gamma 0.009945362963723885, val accuracy 0.6502463054187192, val loss 0.9713151466670294 [3 / 50]
-------------------------------------
{'lr': 0.00397383675065819, 'batch_size': 8, 'weight_decay': 8.752657758177125e-06, 'gamma': 0.00380237959809614}
train_acc: 0.18541409147095178, val_acc: 0.2413793103448276, train_loss: 1.7728960387344266, val_loss: 1.7582977511025415 (1 / 100)
train_acc: 0.2249690976514215, val_acc: 0.18226600985221675, train_loss: 1.753346635473084, val_loss: 1.6921392972833418 (2 / 100)
train_acc: 0.2126081582200247, val_acc: 0.35467980295566504, train_loss: 1.765896622713181, val_loss: 1.7349786294504927 (3 / 100)
train_acc: 0.2546353522867738, val_acc: 0.3694581280788177, train_loss: 1.6703990341264325, val_loss: 1.5409934068548268 (4 / 100)
train_acc: 0.311495673671199, val_acc: 0.35467980295566504, train_loss: 1.5460450737673803, val_loss: 1.4666089779637717 (5 / 100)
train_acc: 0.31025957972805934, val_acc: 0.3251231527093596, train_loss: 1.5820540886726897, val_loss: 1.4924095305315968 (6 / 100)
train_acc: 0.32138442521631644, val_acc: 0.35960591133004927, train_loss: 1.5192144279574287, val_loss: 1.7228469073478812 (7 / 100)
train_acc: 0.3621755253399258, val_acc: 0.3694581280788177, train_loss: 1.4628707637598253, val_loss: 1.4079956556188649 (8 / 100)
train_acc: 0.34981458590852904, val_acc: 0.3842364532019704, train_loss: 1.431523182041418, val_loss: 1.3290827315429161 (9 / 100)
train_acc: 0.38813349814585907, val_acc: 0.3251231527093596, train_loss: 1.3881949056653657, val_loss: 2.066059814885332 (10 / 100)
train_acc: 0.3683559950556242, val_acc: 0.3891625615763547, train_loss: 1.403730854263117, val_loss: 1.519066489095171 (11 / 100)
train_acc: 0.3868974042027194, val_acc: 0.4187192118226601, train_loss: 1.3627226712235119, val_loss: 1.4441266882008519 (12 / 100)
train_acc: 0.4103831891223733, val_acc: 0.4187192118226601, train_loss: 1.305231957854093, val_loss: 1.3392265353884016 (13 / 100)
train_acc: 0.4437577255871446, val_acc: 0.4876847290640394, train_loss: 1.2628397296179357, val_loss: 1.2725550312126799 (14 / 100)
train_acc: 0.4647713226205192, val_acc: 0.5123152709359606, train_loss: 1.2225133045052716, val_loss: 1.2481721839294058 (15 / 100)
train_acc: 0.4857849196538937, val_acc: 0.43349753694581283, train_loss: 1.2066399231829366, val_loss: 1.2606578959620058 (16 / 100)
train_acc: 0.48331273176761436, val_acc: 0.4975369458128079, train_loss: 1.1688187732390036, val_loss: 1.2970925052764968 (17 / 100)
train_acc: 0.4956736711990111, val_acc: 0.4187192118226601, train_loss: 1.194718748883647, val_loss: 1.4821533563689058 (18 / 100)
train_acc: 0.5129789864029666, val_acc: 0.4187192118226601, train_loss: 1.1508927893432315, val_loss: 1.323996145443376 (19 / 100)
train_acc: 0.5772558714462299, val_acc: 0.4827586206896552, train_loss: 1.0374732419793185, val_loss: 1.2037781813168174 (20 / 100)
train_acc: 0.5920889987639061, val_acc: 0.5221674876847291, train_loss: 0.9409440238631994, val_loss: 1.0776758528695318 (21 / 100)
train_acc: 0.6217552533992583, val_acc: 0.5517241379310345, train_loss: 0.8871218200372383, val_loss: 1.2481364005892148 (22 / 100)
train_acc: 0.65389369592089, val_acc: 0.5221674876847291, train_loss: 0.8589609856953285, val_loss: 1.2869883487230451 (23 / 100)
train_acc: 0.6650185414091471, val_acc: 0.5024630541871922, train_loss: 0.7855653989742359, val_loss: 1.2283797369801939 (24 / 100)
train_acc: 0.6749072929542645, val_acc: 0.5615763546798029, train_loss: 0.8456093644624293, val_loss: 1.248339953680931 (25 / 100)
train_acc: 0.754017305315204, val_acc: 0.5123152709359606, train_loss: 0.6612746577033006, val_loss: 1.4858179477048037 (26 / 100)
train_acc: 0.7095179233621756, val_acc: 0.5517241379310345, train_loss: 0.7467667296142012, val_loss: 1.4710016955295806 (27 / 100)
train_acc: 0.7639060568603214, val_acc: 0.5960591133004927, train_loss: 0.6264889707376695, val_loss: 1.2198903813150717 (28 / 100)
train_acc: 0.7972805933250927, val_acc: 0.5714285714285714, train_loss: 0.5673656087870651, val_loss: 1.3089380786923939 (29 / 100)
train_acc: 0.7849196538936959, val_acc: 0.5467980295566502, train_loss: 0.564706563949585, val_loss: 1.3411614313501443 (30 / 100)
train_acc: 0.8244746600741656, val_acc: 0.5960591133004927, train_loss: 0.46270517336276024, val_loss: 1.3349904056840343 (31 / 100)
overfit -> train_accuracy 0.7972805933250927, val_accuracy 0.5270935960591133
lr 0.00397383675065819, batch 8, decay 8.752657758177125e-06, gamma 0.00380237959809614, val accuracy 0.5960591133004927, val loss 1.2198903813150717 [4 / 50]
-------------------------------------
{'lr': 0.0009581247587444257, 'batch_size': 8, 'weight_decay': 4.2121734739193815e-05, 'gamma': 0.09029836928703246}
train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.7816233680628433, val_loss: 1.763189015717342 (1 / 100)
train_acc: 0.18912237330037082, val_acc: 0.27586206896551724, train_loss: 1.763621202358062, val_loss: 1.7442332394604612 (2 / 100)
train_acc: 0.21631644004944375, val_acc: 0.19704433497536947, train_loss: 1.7494022218494392, val_loss: 1.7174247620728216 (3 / 100)
train_acc: 0.26823238566131025, val_acc: 0.26108374384236455, train_loss: 1.7162661926708056, val_loss: 1.6612656451211187 (4 / 100)
train_acc: 0.273176761433869, val_acc: 0.3054187192118227, train_loss: 1.6784494863305015, val_loss: 1.608897767043466 (5 / 100)
train_acc: 0.3176761433868974, val_acc: 0.2660098522167488, train_loss: 1.6165214458708415, val_loss: 1.781298714318299 (6 / 100)
train_acc: 0.34981458590852904, val_acc: 0.33004926108374383, train_loss: 1.5651655177987551, val_loss: 1.6529823201043266 (7 / 100)
train_acc: 0.3535228677379481, val_acc: 0.33497536945812806, train_loss: 1.524613542079336, val_loss: 1.5392848000737833 (8 / 100)
train_acc: 0.3658838071693449, val_acc: 0.37438423645320196, train_loss: 1.5148478193837132, val_loss: 1.3909470864704676 (9 / 100)
train_acc: 0.3683559950556242, val_acc: 0.3497536945812808, train_loss: 1.4466434367949648, val_loss: 1.3432026555385497 (10 / 100)
train_acc: 0.377008652657602, val_acc: 0.32019704433497537, train_loss: 1.4288372993469238, val_loss: 1.4225828001651857 (11 / 100)
train_acc: 0.3868974042027194, val_acc: 0.4236453201970443, train_loss: 1.4052453142750838, val_loss: 1.3485883268816719 (12 / 100)
train_acc: 0.38936959208899874, val_acc: 0.41379310344827586, train_loss: 1.4082907924546004, val_loss: 1.2692552053282413 (13 / 100)
train_acc: 0.411619283065513, val_acc: 0.42857142857142855, train_loss: 1.3533162401692092, val_loss: 1.2390418164248538 (14 / 100)
train_acc: 0.4103831891223733, val_acc: 0.4433497536945813, train_loss: 1.3343707852074773, val_loss: 1.2401952899148312 (15 / 100)
train_acc: 0.415327564894932, val_acc: 0.45320197044334976, train_loss: 1.3196533386433227, val_loss: 1.2344765445868957 (16 / 100)
train_acc: 0.45241038318912236, val_acc: 0.47783251231527096, train_loss: 1.2967411711425216, val_loss: 1.2557896425571349 (17 / 100)
train_acc: 0.4289245982694685, val_acc: 0.46798029556650245, train_loss: 1.2800610389638742, val_loss: 1.2108606840002125 (18 / 100)
train_acc: 0.43510506798516685, val_acc: 0.45320197044334976, train_loss: 1.2829332049609115, val_loss: 1.225726882225187 (19 / 100)
train_acc: 0.4647713226205192, val_acc: 0.4482758620689655, train_loss: 1.2295547601466421, val_loss: 1.2177591502960092 (20 / 100)
train_acc: 0.47713226205191595, val_acc: 0.45320197044334976, train_loss: 1.2179310379275874, val_loss: 1.225509432149051 (21 / 100)
train_acc: 0.4969097651421508, val_acc: 0.4630541871921182, train_loss: 1.180070999820978, val_loss: 1.2201068533465194 (22 / 100)
train_acc: 0.48825710754017304, val_acc: 0.42857142857142855, train_loss: 1.1788570870575121, val_loss: 1.3806485589501893 (23 / 100)
train_acc: 0.5327564894932015, val_acc: 0.4876847290640394, train_loss: 1.1367650541887884, val_loss: 1.164321536207434 (24 / 100)
train_acc: 0.5030902348578492, val_acc: 0.4630541871921182, train_loss: 1.1849849980015985, val_loss: 1.1783427111620974 (25 / 100)
train_acc: 0.5142150803461063, val_acc: 0.5073891625615764, train_loss: 1.1331071894896783, val_loss: 1.302083589173303 (26 / 100)
train_acc: 0.5710754017305315, val_acc: 0.5320197044334976, train_loss: 1.0780108476009298, val_loss: 1.0784663284940672 (27 / 100)
train_acc: 0.5587144622991347, val_acc: 0.45320197044334976, train_loss: 1.034307350481691, val_loss: 1.217477428208431 (28 / 100)
train_acc: 0.5451174289245982, val_acc: 0.5024630541871922, train_loss: 1.097415424247902, val_loss: 1.1569304408991865 (29 / 100)
train_acc: 0.5995055624227441, val_acc: 0.5073891625615764, train_loss: 0.9839347764499697, val_loss: 1.3014758453874165 (30 / 100)
train_acc: 0.6168108776266996, val_acc: 0.5172413793103449, train_loss: 0.9415264088379586, val_loss: 1.2713928856139112 (31 / 100)
train_acc: 0.6242274412855378, val_acc: 0.5812807881773399, train_loss: 0.9392363851238388, val_loss: 1.0496468693752008 (32 / 100)
train_acc: 0.6131025957972805, val_acc: 0.5566502463054187, train_loss: 0.9023256870665862, val_loss: 1.046317466961339 (33 / 100)
train_acc: 0.6600741656365884, val_acc: 0.5566502463054187, train_loss: 0.8505060386893482, val_loss: 0.9993808568992051 (34 / 100)
train_acc: 0.6835599505562423, val_acc: 0.5911330049261084, train_loss: 0.8165020176447808, val_loss: 1.039066458570546 (35 / 100)
train_acc: 0.6983930778739185, val_acc: 0.5763546798029556, train_loss: 0.7369238183878556, val_loss: 1.0009663210713804 (36 / 100)
train_acc: 0.723114956736712, val_acc: 0.5517241379310345, train_loss: 0.7077591345543031, val_loss: 1.348135481327038 (37 / 100)
train_acc: 0.7021013597033374, val_acc: 0.5369458128078818, train_loss: 0.749274175747658, val_loss: 1.1806703976222448 (38 / 100)
train_acc: 0.7589616810877626, val_acc: 0.5665024630541872, train_loss: 0.655102094849491, val_loss: 1.052058171756162 (39 / 100)
train_acc: 0.7849196538936959, val_acc: 0.5566502463054187, train_loss: 0.5810171208069262, val_loss: 1.4067580889304871 (40 / 100)
train_acc: 0.8046971569839307, val_acc: 0.6206896551724138, train_loss: 0.49984773304612734, val_loss: 1.0037392051936371 (41 / 100)
train_acc: 0.8071693448702101, val_acc: 0.625615763546798, train_loss: 0.4967429995978866, val_loss: 1.078432423140615 (42 / 100)
overfit -> train_accuracy 0.8158220024721878, val_accuracy 0.5517241379310345
lr 0.0009581247587444257, batch 8, decay 4.2121734739193815e-05, gamma 0.09029836928703246, val accuracy 0.625615763546798, val loss 1.078432423140615 [5 / 50]
-------------------------------------
{'lr': 0.0026015030607287972, 'batch_size': 8, 'weight_decay': 7.625366955300806e-06, 'gamma': 0.006894270697580138}
train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.7747868103503592, val_loss: 1.765917347569771 (1 / 100)
train_acc: 0.2138442521631644, val_acc: 0.3054187192118227, train_loss: 1.7527495411918839, val_loss: 1.7203814272810085 (2 / 100)
train_acc: 0.25339925834363414, val_acc: 0.33004926108374383, train_loss: 1.7085796401586166, val_loss: 1.5774775038799043 (3 / 100)
train_acc: 0.2892459826946848, val_acc: 0.3251231527093596, train_loss: 1.664642832040492, val_loss: 1.6120272798491229 (4 / 100)
train_acc: 0.32756489493201485, val_acc: 0.3103448275862069, train_loss: 1.5845020722105123, val_loss: 1.5346746615001134 (5 / 100)
train_acc: 0.33868974042027195, val_acc: 0.27586206896551724, train_loss: 1.5496551062179584, val_loss: 1.4805923565267929 (6 / 100)
train_acc: 0.34487021013597036, val_acc: 0.3251231527093596, train_loss: 1.5257905292864635, val_loss: 1.5034069386609081 (7 / 100)
train_acc: 0.3189122373300371, val_acc: 0.4088669950738916, train_loss: 1.5132368462636827, val_loss: 1.4581744001416737 (8 / 100)
train_acc: 0.3658838071693449, val_acc: 0.4236453201970443, train_loss: 1.4384231540859114, val_loss: 1.3574621366162605 (9 / 100)
train_acc: 0.37330037082818296, val_acc: 0.35960591133004927, train_loss: 1.413978977462712, val_loss: 1.490740223764786 (10 / 100)
train_acc: 0.4054388133498146, val_acc: 0.43349753694581283, train_loss: 1.3772712803004845, val_loss: 1.2521829141184615 (11 / 100)
train_acc: 0.3794808405438813, val_acc: 0.3842364532019704, train_loss: 1.3840711791671545, val_loss: 1.3351161791186028 (12 / 100)
train_acc: 0.40914709517923364, val_acc: 0.43842364532019706, train_loss: 1.341815702552701, val_loss: 1.3324803278363984 (13 / 100)
train_acc: 0.4215080346106304, val_acc: 0.458128078817734, train_loss: 1.304819672010443, val_loss: 1.192561359241091 (14 / 100)
train_acc: 0.45241038318912236, val_acc: 0.5073891625615764, train_loss: 1.2599915382446554, val_loss: 1.198419066485513 (15 / 100)
train_acc: 0.47095179233621753, val_acc: 0.46798029556650245, train_loss: 1.2195741033966667, val_loss: 1.2485957421692722 (16 / 100)
train_acc: 0.4746600741656366, val_acc: 0.4876847290640394, train_loss: 1.2478674458632215, val_loss: 1.2117315501415085 (17 / 100)
train_acc: 0.48825710754017304, val_acc: 0.46798029556650245, train_loss: 1.175153906177384, val_loss: 1.2012949999917317 (18 / 100)
train_acc: 0.5253399258343634, val_acc: 0.45320197044334976, train_loss: 1.12238812741302, val_loss: 1.2377244677449681 (19 / 100)
train_acc: 0.5327564894932015, val_acc: 0.4876847290640394, train_loss: 1.0859686817020657, val_loss: 1.1993066383700066 (20 / 100)
train_acc: 0.5512978986402967, val_acc: 0.5024630541871922, train_loss: 1.0509370708347694, val_loss: 1.200156926227908 (21 / 100)
train_acc: 0.595797280593325, val_acc: 0.5270935960591133, train_loss: 0.9980896169087206, val_loss: 1.224779993442479 (22 / 100)
train_acc: 0.6032138442521632, val_acc: 0.4433497536945813, train_loss: 0.9920568719635199, val_loss: 1.3812428096245075 (23 / 100)
train_acc: 0.5822002472187886, val_acc: 0.541871921182266, train_loss: 0.9717592873296278, val_loss: 1.257700174166064 (24 / 100)
train_acc: 0.6341161928306551, val_acc: 0.5024630541871922, train_loss: 0.8525735434554563, val_loss: 1.5865608759114307 (25 / 100)
train_acc: 0.65389369592089, val_acc: 0.5221674876847291, train_loss: 0.8277720209989323, val_loss: 1.0878441548112578 (26 / 100)
train_acc: 0.6650185414091471, val_acc: 0.5123152709359606, train_loss: 0.8357085995974736, val_loss: 1.5997078500944992 (27 / 100)
train_acc: 0.7169344870210136, val_acc: 0.4827586206896552, train_loss: 0.7070867013282918, val_loss: 1.2602045999959184 (28 / 100)
train_acc: 0.7354758961681088, val_acc: 0.5566502463054187, train_loss: 0.6285227037625496, val_loss: 1.201529073157334 (29 / 100)
train_acc: 0.761433868974042, val_acc: 0.5270935960591133, train_loss: 0.655094666593007, val_loss: 1.1842131089107157 (30 / 100)
train_acc: 0.7663782447466008, val_acc: 0.5270935960591133, train_loss: 0.6339515446143038, val_loss: 1.4929384803537078 (31 / 100)
overfit -> train_accuracy 0.8034610630407911, val_accuracy 0.541871921182266
lr 0.0026015030607287972, batch 8, decay 7.625366955300806e-06, gamma 0.006894270697580138, val accuracy 0.5566502463054187, val loss 1.201529073157334 [6 / 50]
-------------------------------------
{'lr': 0.0017106658984664923, 'batch_size': 8, 'weight_decay': 8.496152829280522e-05, 'gamma': 0.045079773264999005}
train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7786166330497846, val_loss: 1.7608689797922896 (1 / 100)
train_acc: 0.22620519159456118, val_acc: 0.18226600985221675, train_loss: 1.7522966401980746, val_loss: 1.735738793030161 (2 / 100)
train_acc: 0.24721878862793573, val_acc: 0.29064039408866993, train_loss: 1.7117385828892882, val_loss: 1.6351163275723386 (3 / 100)
train_acc: 0.29295426452410384, val_acc: 0.3251231527093596, train_loss: 1.6413896606938063, val_loss: 1.704227197346429 (4 / 100)
train_acc: 0.3164400494437577, val_acc: 0.3497536945812808, train_loss: 1.59388362919297, val_loss: 1.4950482017300986 (5 / 100)
train_acc: 0.30778739184178, val_acc: 0.35960591133004927, train_loss: 1.5351543948146409, val_loss: 1.4923193983256524 (6 / 100)
train_acc: 0.34239802224969096, val_acc: 0.3793103448275862, train_loss: 1.5287550801252405, val_loss: 1.3972725574606157 (7 / 100)
train_acc: 0.3572311495673671, val_acc: 0.3842364532019704, train_loss: 1.4819016463971697, val_loss: 1.4409475608412268 (8 / 100)
train_acc: 0.38936959208899874, val_acc: 0.39901477832512317, train_loss: 1.4067666757534107, val_loss: 1.5413994028650482 (9 / 100)
train_acc: 0.38442521631644005, val_acc: 0.43349753694581283, train_loss: 1.3858601114216487, val_loss: 1.3201673935199607 (10 / 100)
train_acc: 0.38813349814585907, val_acc: 0.3891625615763547, train_loss: 1.3565576571616609, val_loss: 1.328250961937928 (11 / 100)
train_acc: 0.4079110012360939, val_acc: 0.4827586206896552, train_loss: 1.3313406415569178, val_loss: 1.2616119017741951 (12 / 100)
train_acc: 0.41903584672435107, val_acc: 0.43349753694581283, train_loss: 1.2951008785787708, val_loss: 1.299658881619646 (13 / 100)
train_acc: 0.4338689740420272, val_acc: 0.4630541871921182, train_loss: 1.3178450209543349, val_loss: 1.405681293292586 (14 / 100)
train_acc: 0.4610630407911001, val_acc: 0.43842364532019706, train_loss: 1.2798784812094992, val_loss: 1.2584904121060676 (15 / 100)
train_acc: 0.4672435105067985, val_acc: 0.46798029556650245, train_loss: 1.2288650596981732, val_loss: 1.2171685965777619 (16 / 100)
train_acc: 0.4400494437577256, val_acc: 0.47783251231527096, train_loss: 1.2204821843594056, val_loss: 1.1691803250994002 (17 / 100)
train_acc: 0.4796044499381953, val_acc: 0.47783251231527096, train_loss: 1.200968143524434, val_loss: 1.230461243778614 (18 / 100)
train_acc: 0.484548825710754, val_acc: 0.47783251231527096, train_loss: 1.1796602418897177, val_loss: 1.1718074247754853 (19 / 100)
train_acc: 0.5142150803461063, val_acc: 0.46798029556650245, train_loss: 1.1422848966714036, val_loss: 1.1630890569076162 (20 / 100)
train_acc: 0.5043263288009888, val_acc: 0.45320197044334976, train_loss: 1.1191915776143115, val_loss: 1.383195863568724 (21 / 100)
train_acc: 0.546353522867738, val_acc: 0.5270935960591133, train_loss: 1.033940186753998, val_loss: 1.1153162823522031 (22 / 100)
train_acc: 0.5599505562422744, val_acc: 0.4729064039408867, train_loss: 1.0545714287586943, val_loss: 1.1234055951311084 (23 / 100)
train_acc: 0.5933250927070457, val_acc: 0.5517241379310345, train_loss: 0.9595511572617388, val_loss: 1.2016594562624476 (24 / 100)
train_acc: 0.5747836835599506, val_acc: 0.49261083743842365, train_loss: 1.0010284319501577, val_loss: 1.1163697407163422 (25 / 100)
train_acc: 0.619283065512979, val_acc: 0.5566502463054187, train_loss: 0.899729957981369, val_loss: 1.1433631457718723 (26 / 100)
train_acc: 0.619283065512979, val_acc: 0.5467980295566502, train_loss: 0.9017321780526594, val_loss: 1.2317517570086889 (27 / 100)
train_acc: 0.6452410383189122, val_acc: 0.5221674876847291, train_loss: 0.848546688135239, val_loss: 1.1381444343792393 (28 / 100)
train_acc: 0.6835599505562423, val_acc: 0.5665024630541872, train_loss: 0.7702625970757936, val_loss: 1.3545899690665635 (29 / 100)
train_acc: 0.6650185414091471, val_acc: 0.5812807881773399, train_loss: 0.7819390609326262, val_loss: 1.2844257532375787 (30 / 100)
train_acc: 0.7107540173053152, val_acc: 0.5665024630541872, train_loss: 0.7272281387385685, val_loss: 1.2084524352562251 (31 / 100)
train_acc: 0.7070457354758962, val_acc: 0.5369458128078818, train_loss: 0.6993103487087858, val_loss: 1.349631998926548 (32 / 100)
train_acc: 0.7527812113720643, val_acc: 0.5714285714285714, train_loss: 0.6332111895010705, val_loss: 1.218357848489813 (33 / 100)
train_acc: 0.619283065512979, val_acc: 0.5665024630541872, train_loss: 0.97189742024661, val_loss: 1.0274932184830088 (34 / 100)
train_acc: 0.7775030902348579, val_acc: 0.5911330049261084, train_loss: 0.5733304766406235, val_loss: 1.2479855943783162 (35 / 100)
train_acc: 0.7898640296662547, val_acc: 0.6354679802955665, train_loss: 0.5266951959124307, val_loss: 1.014032352146844 (36 / 100)
train_acc: 0.8220024721878862, val_acc: 0.5862068965517241, train_loss: 0.4868346628064131, val_loss: 1.4472684645887666 (37 / 100)
train_acc: 0.8380716934487021, val_acc: 0.6354679802955665, train_loss: 0.4552688318660439, val_loss: 1.3222276271857651 (38 / 100)
overfit -> train_accuracy 0.8355995055624228, val_accuracy 0.5763546798029556
lr 0.0017106658984664923, batch 8, decay 8.496152829280522e-05, gamma 0.045079773264999005, val accuracy 0.6354679802955665, val loss 1.014032352146844 [7 / 50]
-------------------------------------
{'lr': 0.003272122195577998, 'batch_size': 8, 'weight_decay': 1.206922354458021e-06, 'gamma': 0.0022285588422933434}
train_acc: 0.1557478368355995, val_acc: 0.28078817733990147, train_loss: 1.7771061790004208, val_loss: 1.7577286500648912 (1 / 100)
train_acc: 0.19283065512978986, val_acc: 0.20689655172413793, train_loss: 1.7613678746229344, val_loss: 1.7107423479333888 (2 / 100)
train_acc: 0.24969097651421507, val_acc: 0.33004926108374383, train_loss: 1.7121467012704819, val_loss: 1.7326875014845373 (3 / 100)
train_acc: 0.2978986402966625, val_acc: 0.35467980295566504, train_loss: 1.6425432993838165, val_loss: 1.5765673814735977 (4 / 100)
train_acc: 0.3189122373300371, val_acc: 0.3399014778325123, train_loss: 1.6155876146700827, val_loss: 1.5730369384652876 (5 / 100)
train_acc: 0.34487021013597036, val_acc: 0.39901477832512317, train_loss: 1.526191659704569, val_loss: 1.4072559314408326 (6 / 100)
train_acc: 0.30902348578491967, val_acc: 0.3793103448275862, train_loss: 1.549667093161452, val_loss: 1.3319787203971976 (7 / 100)
train_acc: 0.3646477132262052, val_acc: 0.39408866995073893, train_loss: 1.456201960630853, val_loss: 1.4530409586253425 (8 / 100)
train_acc: 0.34363411619283063, val_acc: 0.3842364532019704, train_loss: 1.4742732893845945, val_loss: 1.3395593865164395 (9 / 100)
train_acc: 0.3943139678615575, val_acc: 0.42857142857142855, train_loss: 1.3694026461343094, val_loss: 1.2836847202531223 (10 / 100)
train_acc: 0.40296662546353523, val_acc: 0.4236453201970443, train_loss: 1.3461956529888441, val_loss: 1.5521178386481524 (11 / 100)
train_acc: 0.41903584672435107, val_acc: 0.42857142857142855, train_loss: 1.320184270589991, val_loss: 1.3058449323541426 (12 / 100)
train_acc: 0.4252163164400494, val_acc: 0.43349753694581283, train_loss: 1.3087406628506146, val_loss: 1.2472616657247684 (13 / 100)
train_acc: 0.43510506798516685, val_acc: 0.4827586206896552, train_loss: 1.2672576496126626, val_loss: 1.1862257719039917 (14 / 100)
train_acc: 0.45241038318912236, val_acc: 0.41379310344827586, train_loss: 1.2493328349699344, val_loss: 1.2995007731057153 (15 / 100)
train_acc: 0.5055624227441285, val_acc: 0.43349753694581283, train_loss: 1.2176762175353701, val_loss: 1.331979508470432 (16 / 100)
train_acc: 0.511742892459827, val_acc: 0.46798029556650245, train_loss: 1.1900824345233856, val_loss: 1.2191228796108602 (17 / 100)
train_acc: 0.5352286773794809, val_acc: 0.46798029556650245, train_loss: 1.101623119913014, val_loss: 1.2473070280892509 (18 / 100)
train_acc: 0.5105067985166872, val_acc: 0.46798029556650245, train_loss: 1.149934685569169, val_loss: 1.2705491000208362 (19 / 100)
train_acc: 0.5624227441285538, val_acc: 0.5665024630541872, train_loss: 1.0596634383844061, val_loss: 1.1011686835970198 (20 / 100)
train_acc: 0.5735475896168108, val_acc: 0.5172413793103449, train_loss: 1.0011023478985421, val_loss: 1.1723707002665609 (21 / 100)
train_acc: 0.595797280593325, val_acc: 0.4876847290640394, train_loss: 1.0084483554248316, val_loss: 1.2176098550481749 (22 / 100)
train_acc: 0.6341161928306551, val_acc: 0.5123152709359606, train_loss: 0.9279083790089497, val_loss: 1.4528928105467058 (23 / 100)
train_acc: 0.6378244746600742, val_acc: 0.49261083743842365, train_loss: 0.8700759443276008, val_loss: 1.2214902100598284 (24 / 100)
train_acc: 0.6823238566131026, val_acc: 0.5714285714285714, train_loss: 0.81422517827179, val_loss: 1.2488635672724306 (25 / 100)
train_acc: 0.6847960444993819, val_acc: 0.5320197044334976, train_loss: 0.7997335093867204, val_loss: 1.352088227060628 (26 / 100)
train_acc: 0.7045735475896168, val_acc: 0.5467980295566502, train_loss: 0.728934471480484, val_loss: 1.1437994353289676 (27 / 100)
train_acc: 0.7292954264524104, val_acc: 0.5320197044334976, train_loss: 0.6675689460615293, val_loss: 1.3357003880251805 (28 / 100)
train_acc: 0.7737948084054388, val_acc: 0.5270935960591133, train_loss: 0.6035953742464629, val_loss: 1.6756852131171767 (29 / 100)
train_acc: 0.7750309023485785, val_acc: 0.5467980295566502, train_loss: 0.5873801671382965, val_loss: 1.7561707690431567 (30 / 100)
train_acc: 0.8207663782447466, val_acc: 0.6108374384236454, train_loss: 0.4799232652367119, val_loss: 1.5184048064823807 (31 / 100)
train_acc: 0.8182941903584673, val_acc: 0.5812807881773399, train_loss: 0.48738621132924914, val_loss: 1.4765430600772351 (32 / 100)
overfit -> train_accuracy 0.8491965389369592, val_accuracy 0.5517241379310345
lr 0.003272122195577998, batch 8, decay 1.206922354458021e-06, gamma 0.0022285588422933434, val accuracy 0.6108374384236454, val loss 1.5184048064823807 [8 / 50]
-------------------------------------
{'lr': 0.003335287237937998, 'batch_size': 8, 'weight_decay': 1.7629015412816786e-06, 'gamma': 0.0018632972275831991}
train_acc: 0.19777503090234858, val_acc: 0.18226600985221675, train_loss: 1.766557240220908, val_loss: 1.7543840936839288 (1 / 100)
train_acc: 0.242274412855377, val_acc: 0.3399014778325123, train_loss: 1.7363231575533251, val_loss: 1.651728527299289 (2 / 100)
train_acc: 0.2904820766378245, val_acc: 0.23645320197044334, train_loss: 1.6658599836422574, val_loss: 1.730810187133075 (3 / 100)
train_acc: 0.315203955500618, val_acc: 0.270935960591133, train_loss: 1.6074799807611, val_loss: 1.6481125067020286 (4 / 100)
train_acc: 0.3003708281829419, val_acc: 0.3448275862068966, train_loss: 1.646707633664493, val_loss: 1.6099526911533524 (5 / 100)
train_acc: 0.3522867737948084, val_acc: 0.3891625615763547, train_loss: 1.525592711564195, val_loss: 1.4537037017897432 (6 / 100)
train_acc: 0.33127317676143386, val_acc: 0.3399014778325123, train_loss: 1.5737704912428507, val_loss: 1.5778881599163186 (7 / 100)
train_acc: 0.3399258343634116, val_acc: 0.37438423645320196, train_loss: 1.4900199667042944, val_loss: 1.4792815904899184 (8 / 100)
train_acc: 0.38813349814585907, val_acc: 0.41379310344827586, train_loss: 1.383114048223413, val_loss: 1.4539001598733987 (9 / 100)
train_acc: 0.40296662546353523, val_acc: 0.39408866995073893, train_loss: 1.400196420247534, val_loss: 1.3044512315923944 (10 / 100)
train_acc: 0.38442521631644005, val_acc: 0.39408866995073893, train_loss: 1.398408243181679, val_loss: 1.2857266485397452 (11 / 100)
train_acc: 0.3930778739184178, val_acc: 0.37438423645320196, train_loss: 1.3249713519889728, val_loss: 1.317991599660789 (12 / 100)
train_acc: 0.4079110012360939, val_acc: 0.49261083743842365, train_loss: 1.3133871198731977, val_loss: 1.2115945745571493 (13 / 100)
train_acc: 0.4326328800988875, val_acc: 0.4187192118226601, train_loss: 1.27890128995049, val_loss: 1.3124150140532131 (14 / 100)
train_acc: 0.4672435105067985, val_acc: 0.47783251231527096, train_loss: 1.2706668584691578, val_loss: 1.2379154589375838 (15 / 100)
train_acc: 0.48331273176761436, val_acc: 0.4630541871921182, train_loss: 1.2000376081879265, val_loss: 1.305890436242954 (16 / 100)
train_acc: 0.5092707045735476, val_acc: 0.39408866995073893, train_loss: 1.1682587138507217, val_loss: 1.3101075505975432 (17 / 100)
train_acc: 0.515451174289246, val_acc: 0.4088669950738916, train_loss: 1.1299004433918354, val_loss: 1.389913889574887 (18 / 100)
train_acc: 0.5302843016069221, val_acc: 0.458128078817734, train_loss: 1.1167479141975658, val_loss: 1.187313384904063 (19 / 100)
train_acc: 0.5500618046971569, val_acc: 0.5320197044334976, train_loss: 1.0517295724234563, val_loss: 1.1615707621785807 (20 / 100)
train_acc: 0.5908529048207664, val_acc: 0.5320197044334976, train_loss: 1.0190480200410332, val_loss: 1.2842432513025595 (21 / 100)
train_acc: 0.6378244746600742, val_acc: 0.5221674876847291, train_loss: 0.8769024803552993, val_loss: 1.1268515067147504 (22 / 100)
train_acc: 0.6786155747836835, val_acc: 0.4827586206896552, train_loss: 0.7965118643379919, val_loss: 1.2747043882831564 (23 / 100)
train_acc: 0.6341161928306551, val_acc: 0.5615763546798029, train_loss: 0.9093405708246973, val_loss: 1.1163287673677718 (24 / 100)
train_acc: 0.6637824474660075, val_acc: 0.5320197044334976, train_loss: 0.8654390625075445, val_loss: 1.1233150759354014 (25 / 100)
train_acc: 0.7292954264524104, val_acc: 0.5517241379310345, train_loss: 0.6954079331514714, val_loss: 1.6629299682936645 (26 / 100)
train_acc: 0.7404202719406675, val_acc: 0.5665024630541872, train_loss: 0.6544490679822245, val_loss: 1.4190505100001256 (27 / 100)
overfit -> train_accuracy 0.6983930778739185, val_accuracy 0.42857142857142855
lr 0.003335287237937998, batch 8, decay 1.7629015412816786e-06, gamma 0.0018632972275831991, val accuracy 0.5665024630541872, val loss 1.4190505100001256 [9 / 50]
-------------------------------------
{'lr': 0.0024822798124892035, 'batch_size': 8, 'weight_decay': 3.441329483374953e-05, 'gamma': 0.0010537709071031227}
train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7769670773790263, val_loss: 1.7523624010273975 (1 / 100)
train_acc: 0.24474660074165636, val_acc: 0.2019704433497537, train_loss: 1.7459730879485387, val_loss: 1.7127836167518729 (2 / 100)
train_acc: 0.21013597033374537, val_acc: 0.3103448275862069, train_loss: 1.7527530352471048, val_loss: 1.6910405934150583 (3 / 100)
train_acc: 0.31396786155747836, val_acc: 0.2561576354679803, train_loss: 1.6950779920455405, val_loss: 1.5984790378016205 (4 / 100)
train_acc: 0.3164400494437577, val_acc: 0.3694581280788177, train_loss: 1.5967160856767992, val_loss: 1.474719904326453 (5 / 100)
train_acc: 0.34610630407911, val_acc: 0.21674876847290642, train_loss: 1.531841626420746, val_loss: 1.7011530287747312 (6 / 100)
train_acc: 0.32138442521631644, val_acc: 0.35960591133004927, train_loss: 1.541069196092773, val_loss: 1.3729700531278337 (7 / 100)
train_acc: 0.3473423980222497, val_acc: 0.3448275862068966, train_loss: 1.4972764720728136, val_loss: 1.3784207457979325 (8 / 100)
train_acc: 0.3621755253399258, val_acc: 0.3497536945812808, train_loss: 1.4348273610302484, val_loss: 1.4479519246247015 (9 / 100)
train_acc: 0.39555006180469715, val_acc: 0.3497536945812808, train_loss: 1.3720902004112272, val_loss: 1.42146408792787 (10 / 100)
train_acc: 0.39184177997527814, val_acc: 0.4433497536945813, train_loss: 1.3649443383564612, val_loss: 1.2664280894941884 (11 / 100)
train_acc: 0.40296662546353523, val_acc: 0.43842364532019706, train_loss: 1.3258754025283643, val_loss: 1.2916673151730316 (12 / 100)
train_acc: 0.4326328800988875, val_acc: 0.43842364532019706, train_loss: 1.3229042178178747, val_loss: 1.2747499349669282 (13 / 100)
train_acc: 0.41903584672435107, val_acc: 0.4482758620689655, train_loss: 1.3051722200014093, val_loss: 1.235101077944187 (14 / 100)
train_acc: 0.4227441285537701, val_acc: 0.4433497536945813, train_loss: 1.301502188292363, val_loss: 1.3239292020868199 (15 / 100)
train_acc: 0.44870210135970334, val_acc: 0.4187192118226601, train_loss: 1.2647717939171714, val_loss: 1.3008298750581413 (16 / 100)
train_acc: 0.4894932014833127, val_acc: 0.4876847290640394, train_loss: 1.1965061359853473, val_loss: 1.199482687001158 (17 / 100)
train_acc: 0.515451174289246, val_acc: 0.43349753694581283, train_loss: 1.1821873621239207, val_loss: 1.3082925448276725 (18 / 100)
train_acc: 0.46600741656365885, val_acc: 0.47783251231527096, train_loss: 1.2281709274343862, val_loss: 1.1837643647722422 (19 / 100)
train_acc: 0.5142150803461063, val_acc: 0.4827586206896552, train_loss: 1.1345133192017582, val_loss: 1.1799782755339674 (20 / 100)
train_acc: 0.5401730531520396, val_acc: 0.458128078817734, train_loss: 1.0751278930895114, val_loss: 1.1920146742477793 (21 / 100)
train_acc: 0.5772558714462299, val_acc: 0.49261083743842365, train_loss: 0.9943003100429388, val_loss: 1.3740378571261327 (22 / 100)
train_acc: 0.5723114956736712, val_acc: 0.5024630541871922, train_loss: 1.009168453947428, val_loss: 1.1577184115137373 (23 / 100)
train_acc: 0.6266996291718171, val_acc: 0.5270935960591133, train_loss: 0.9426226580540831, val_loss: 1.087581934012803 (24 / 100)
train_acc: 0.630407911001236, val_acc: 0.5960591133004927, train_loss: 0.8687295004522845, val_loss: 1.1533918380737305 (25 / 100)
train_acc: 0.6662546353522868, val_acc: 0.5123152709359606, train_loss: 0.8210611476002282, val_loss: 1.1540817169133077 (26 / 100)
train_acc: 0.6934487021013597, val_acc: 0.5615763546798029, train_loss: 0.7834285659754674, val_loss: 1.2735848895875104 (27 / 100)
train_acc: 0.646477132262052, val_acc: 0.5665024630541872, train_loss: 0.8893074768288026, val_loss: 1.2414632430804775 (28 / 100)
train_acc: 0.7194066749072929, val_acc: 0.5566502463054187, train_loss: 0.7226003722295773, val_loss: 1.4013910883753171 (29 / 100)
train_acc: 0.7527812113720643, val_acc: 0.5172413793103449, train_loss: 0.6748153606362927, val_loss: 1.779463227159284 (30 / 100)
train_acc: 0.7861557478368356, val_acc: 0.6108374384236454, train_loss: 0.6022878491834304, val_loss: 1.1625071822716098 (31 / 100)
train_acc: 0.8096415327564895, val_acc: 0.5960591133004927, train_loss: 0.5774419306235201, val_loss: 1.5429048502973735 (32 / 100)
train_acc: 0.7527812113720643, val_acc: 0.6108374384236454, train_loss: 0.6572486117979505, val_loss: 1.1071737235356949 (33 / 100)
overfit -> train_accuracy 0.8529048207663782, val_accuracy 0.5960591133004927
lr 0.0024822798124892035, batch 8, decay 3.441329483374953e-05, gamma 0.0010537709071031227, val accuracy 0.6108374384236454, val loss 1.1625071822716098 [10 / 50]
-------------------------------------
{'lr': 0.002226111012128276, 'batch_size': 8, 'weight_decay': 4.049420239538175e-06, 'gamma': 0.00978910016628904}
train_acc: 0.17799752781211373, val_acc: 0.21674876847290642, train_loss: 1.7762759785422289, val_loss: 1.7528396927077194 (1 / 100)
train_acc: 0.21013597033374537, val_acc: 0.22167487684729065, train_loss: 1.7608489738259239, val_loss: 1.7258115619274195 (2 / 100)
train_acc: 0.25710754017305315, val_acc: 0.30049261083743845, train_loss: 1.7027065705309987, val_loss: 1.6177755382847903 (3 / 100)
train_acc: 0.311495673671199, val_acc: 0.3399014778325123, train_loss: 1.6196628147356296, val_loss: 1.5220077478239689 (4 / 100)
train_acc: 0.30778739184178, val_acc: 0.2512315270935961, train_loss: 1.6208607357718299, val_loss: 1.6533604337664074 (5 / 100)
train_acc: 0.3127317676143387, val_acc: 0.3891625615763547, train_loss: 1.6211210437699213, val_loss: 1.4782532664942625 (6 / 100)
train_acc: 0.34610630407911, val_acc: 0.3448275862068966, train_loss: 1.5312171910690289, val_loss: 1.4581797252147657 (7 / 100)
train_acc: 0.3288009888751545, val_acc: 0.3497536945812808, train_loss: 1.5249075927899411, val_loss: 1.3896710719968297 (8 / 100)
train_acc: 0.37082818294190356, val_acc: 0.3497536945812808, train_loss: 1.433730791761495, val_loss: 1.4432725330878948 (9 / 100)
train_acc: 0.3794808405438813, val_acc: 0.4088669950738916, train_loss: 1.4191884082386315, val_loss: 1.3066825335248937 (10 / 100)
train_acc: 0.3992583436341162, val_acc: 0.4187192118226601, train_loss: 1.3863179825145029, val_loss: 1.3223138108042074 (11 / 100)
train_acc: 0.42027194066749074, val_acc: 0.4433497536945813, train_loss: 1.3716827947807548, val_loss: 1.2446687356591812 (12 / 100)
train_acc: 0.3868974042027194, val_acc: 0.42857142857142855, train_loss: 1.3366143564948045, val_loss: 1.2890696904342163 (13 / 100)
train_acc: 0.40667490729295425, val_acc: 0.46798029556650245, train_loss: 1.3335556479997481, val_loss: 1.2905526648601289 (14 / 100)
train_acc: 0.39184177997527814, val_acc: 0.43842364532019706, train_loss: 1.4017990336872002, val_loss: 1.2840289640896425 (15 / 100)
train_acc: 0.4647713226205192, val_acc: 0.4482758620689655, train_loss: 1.2653574946489559, val_loss: 1.4099421257455949 (16 / 100)
train_acc: 0.4326328800988875, val_acc: 0.43842364532019706, train_loss: 1.2810311464797728, val_loss: 1.2481283254811328 (17 / 100)
train_acc: 0.44870210135970334, val_acc: 0.42857142857142855, train_loss: 1.2483710756407975, val_loss: 1.1723662532609085 (18 / 100)
train_acc: 0.48825710754017304, val_acc: 0.5320197044334976, train_loss: 1.1832854962908883, val_loss: 1.1471127459568342 (19 / 100)
train_acc: 0.4907292954264524, val_acc: 0.49261083743842365, train_loss: 1.158731626197051, val_loss: 1.147232862822528 (20 / 100)
train_acc: 0.519159456118665, val_acc: 0.43349753694581283, train_loss: 1.141637083331793, val_loss: 1.6753841782438343 (21 / 100)
train_acc: 0.5129789864029666, val_acc: 0.4975369458128079, train_loss: 1.0985375984637493, val_loss: 1.214721075419722 (22 / 100)
train_acc: 0.5290482076637825, val_acc: 0.5073891625615764, train_loss: 1.0735696473139325, val_loss: 1.2209209647084691 (23 / 100)
train_acc: 0.5550061804697157, val_acc: 0.47783251231527096, train_loss: 1.0525690240529912, val_loss: 1.3756048726330836 (24 / 100)
train_acc: 0.5747836835599506, val_acc: 0.4975369458128079, train_loss: 0.9924498839195638, val_loss: 1.4065150782979767 (25 / 100)
train_acc: 0.5723114956736712, val_acc: 0.5369458128078818, train_loss: 1.0420028721298953, val_loss: 1.0300920784767038 (26 / 100)
train_acc: 0.6316440049443758, val_acc: 0.5270935960591133, train_loss: 0.8980306249908523, val_loss: 1.143670415056163 (27 / 100)
train_acc: 0.657601977750309, val_acc: 0.5566502463054187, train_loss: 0.807928524736421, val_loss: 1.0977627981472484 (28 / 100)
train_acc: 0.6835599505562423, val_acc: 0.5123152709359606, train_loss: 0.7847655181979072, val_loss: 1.2991227616230254 (29 / 100)
train_acc: 0.7169344870210136, val_acc: 0.49261083743842365, train_loss: 0.7028115995145404, val_loss: 1.821142065187393 (30 / 100)
train_acc: 0.6637824474660075, val_acc: 0.5320197044334976, train_loss: 0.8524507534666026, val_loss: 1.4713930320269957 (31 / 100)
train_acc: 0.7404202719406675, val_acc: 0.5763546798029556, train_loss: 0.6374741459659062, val_loss: 1.0136800490576645 (32 / 100)
train_acc: 0.765142150803461, val_acc: 0.5960591133004927, train_loss: 0.5903878984109433, val_loss: 1.2407180934116757 (33 / 100)
train_acc: 0.7898640296662547, val_acc: 0.5665024630541872, train_loss: 0.5771963171964818, val_loss: 1.1531734466552734 (34 / 100)
train_acc: 0.788627935723115, val_acc: 0.5960591133004927, train_loss: 0.5564190894917888, val_loss: 1.2065221482309803 (35 / 100)
train_acc: 0.8009888751545118, val_acc: 0.5812807881773399, train_loss: 0.5214299333404994, val_loss: 1.2272587424428592 (36 / 100)
overfit -> train_accuracy 0.8034610630407911, val_accuracy 0.5517241379310345
lr 0.002226111012128276, batch 8, decay 4.049420239538175e-06, gamma 0.00978910016628904, val accuracy 0.5960591133004927, val loss 1.2407180934116757 [11 / 50]
-------------------------------------
{'lr': 0.002711789120156523, 'batch_size': 8, 'weight_decay': 1.2442882577535575e-06, 'gamma': 0.005564807991479123}
train_acc: 0.18170580964153277, val_acc: 0.29064039408866993, train_loss: 1.7751637175587112, val_loss: 1.7580536733119947 (1 / 100)
train_acc: 0.22991347342398022, val_acc: 0.1921182266009852, train_loss: 1.7649213693050578, val_loss: 1.7209511725186126 (2 / 100)
train_acc: 0.25092707045735474, val_acc: 0.2315270935960591, train_loss: 1.7229319874229478, val_loss: 1.7588016082500588 (3 / 100)
train_acc: 0.27812113720642767, val_acc: 0.2660098522167488, train_loss: 1.6896540610250939, val_loss: 1.5735797600205896 (4 / 100)
train_acc: 0.26823238566131025, val_acc: 0.2512315270935961, train_loss: 1.6590603452972488, val_loss: 1.6734665033265288 (5 / 100)
train_acc: 0.31025957972805934, val_acc: 0.31527093596059114, train_loss: 1.6235501633586342, val_loss: 1.5609508283032572 (6 / 100)
train_acc: 0.3176761433868974, val_acc: 0.35960591133004927, train_loss: 1.5446354336437984, val_loss: 1.4874401591681494 (7 / 100)
train_acc: 0.3411619283065513, val_acc: 0.3842364532019704, train_loss: 1.4514403967981138, val_loss: 1.401829052147607 (8 / 100)
train_acc: 0.3584672435105068, val_acc: 0.3793103448275862, train_loss: 1.446622543340855, val_loss: 1.3670470978826137 (9 / 100)
train_acc: 0.35599505562422745, val_acc: 0.3842364532019704, train_loss: 1.4249439599045421, val_loss: 1.4049793628636251 (10 / 100)
train_acc: 0.3757725587144623, val_acc: 0.37438423645320196, train_loss: 1.4065847049095428, val_loss: 1.4702685390199934 (11 / 100)
train_acc: 0.3683559950556242, val_acc: 0.43842364532019706, train_loss: 1.4035232902604657, val_loss: 1.31496268364009 (12 / 100)
train_acc: 0.3992583436341162, val_acc: 0.39901477832512317, train_loss: 1.3334527410594437, val_loss: 1.4025689110967325 (13 / 100)
train_acc: 0.4561186650185414, val_acc: 0.4630541871921182, train_loss: 1.269203426074628, val_loss: 1.2709253192535175 (14 / 100)
train_acc: 0.44870210135970334, val_acc: 0.4876847290640394, train_loss: 1.288535655944692, val_loss: 1.2164611252657886 (15 / 100)
train_acc: 0.4338689740420272, val_acc: 0.42857142857142855, train_loss: 1.2707537493688068, val_loss: 1.3410193837922195 (16 / 100)
train_acc: 0.4684796044499382, val_acc: 0.4088669950738916, train_loss: 1.2092619735024621, val_loss: 1.284705497948407 (17 / 100)
train_acc: 0.4635352286773795, val_acc: 0.46798029556650245, train_loss: 1.199492500945281, val_loss: 1.4058408587436957 (18 / 100)
train_acc: 0.4796044499381953, val_acc: 0.46798029556650245, train_loss: 1.1628960549315652, val_loss: 1.3233440979360946 (19 / 100)
train_acc: 0.48084054388133496, val_acc: 0.5123152709359606, train_loss: 1.1604486773129004, val_loss: 1.1488948015156637 (20 / 100)
train_acc: 0.5241038318912238, val_acc: 0.5320197044334976, train_loss: 1.1020933326301527, val_loss: 1.1449038078045022 (21 / 100)
train_acc: 0.5834363411619283, val_acc: 0.5270935960591133, train_loss: 1.0258638502198774, val_loss: 1.306490060731108 (22 / 100)
train_acc: 0.6007416563658838, val_acc: 0.5615763546798029, train_loss: 0.9840185553830103, val_loss: 1.0243420261761238 (23 / 100)
train_acc: 0.5574783683559951, val_acc: 0.5073891625615764, train_loss: 1.099811577384345, val_loss: 1.1821446914978215 (24 / 100)
train_acc: 0.6044499381953028, val_acc: 0.5566502463054187, train_loss: 0.9381424817814963, val_loss: 1.128044281246627 (25 / 100)
train_acc: 0.6365883807169345, val_acc: 0.4876847290640394, train_loss: 0.8818246407620839, val_loss: 1.2980684746662383 (26 / 100)
train_acc: 0.6625463535228677, val_acc: 0.5517241379310345, train_loss: 0.8241680595282423, val_loss: 1.1781680736635707 (27 / 100)
overfit -> train_accuracy 0.7119901112484549, val_accuracy 0.43349753694581283
lr 0.002711789120156523, batch 8, decay 1.2442882577535575e-06, gamma 0.005564807991479123, val accuracy 0.5615763546798029, val loss 1.0243420261761238 [12 / 50]
-------------------------------------
{'lr': 0.002265042705296789, 'batch_size': 8, 'weight_decay': 7.97036358854321e-06, 'gamma': 0.0019647526119155982}
train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.771368239659756, val_loss: 1.7509317116197107 (1 / 100)
train_acc: 0.20024721878862795, val_acc: 0.22167487684729065, train_loss: 1.7642914232129072, val_loss: 1.7470125864292014 (2 / 100)
train_acc: 0.24474660074165636, val_acc: 0.23645320197044334, train_loss: 1.721638308172615, val_loss: 1.6896761297592389 (3 / 100)
train_acc: 0.315203955500618, val_acc: 0.270935960591133, train_loss: 1.6439642741152618, val_loss: 1.6300239416178812 (4 / 100)
train_acc: 0.3176761433868974, val_acc: 0.3399014778325123, train_loss: 1.5684641866365676, val_loss: 1.4166808357379708 (5 / 100)
train_acc: 0.3473423980222497, val_acc: 0.3399014778325123, train_loss: 1.5000573392262122, val_loss: 1.3807589062328995 (6 / 100)
train_acc: 0.3757725587144623, val_acc: 0.32019704433497537, train_loss: 1.4704119227872643, val_loss: 1.581918868525275 (7 / 100)
train_acc: 0.38813349814585907, val_acc: 0.2955665024630542, train_loss: 1.4322036901127422, val_loss: 1.620512618807149 (8 / 100)
train_acc: 0.3868974042027194, val_acc: 0.39901477832512317, train_loss: 1.4219246237770147, val_loss: 1.314340177427959 (9 / 100)
train_acc: 0.3967861557478368, val_acc: 0.3645320197044335, train_loss: 1.365867345088492, val_loss: 1.3332843252003486 (10 / 100)
train_acc: 0.3856613102595797, val_acc: 0.42857142857142855, train_loss: 1.3746981585423643, val_loss: 1.3261870903334594 (11 / 100)
train_acc: 0.446229913473424, val_acc: 0.45320197044334976, train_loss: 1.3046368684697947, val_loss: 1.2438691240813344 (12 / 100)
train_acc: 0.44252163164400493, val_acc: 0.43349753694581283, train_loss: 1.2746075461025732, val_loss: 1.4404681979729037 (13 / 100)
train_acc: 0.4289245982694685, val_acc: 0.4236453201970443, train_loss: 1.312965357554121, val_loss: 1.3787222889256594 (14 / 100)
train_acc: 0.4388133498145859, val_acc: 0.43349753694581283, train_loss: 1.3017954136738819, val_loss: 1.364932125425104 (15 / 100)
train_acc: 0.49443757725587145, val_acc: 0.4975369458128079, train_loss: 1.2033426614273317, val_loss: 1.169169540769361 (16 / 100)
train_acc: 0.4956736711990111, val_acc: 0.39408866995073893, train_loss: 1.1575937268171086, val_loss: 1.3086077996662684 (17 / 100)
train_acc: 0.5030902348578492, val_acc: 0.5123152709359606, train_loss: 1.176639891378075, val_loss: 1.1515243261905725 (18 / 100)
train_acc: 0.5265760197775031, val_acc: 0.4827586206896552, train_loss: 1.1121047898777041, val_loss: 1.156601100719621 (19 / 100)
train_acc: 0.5377008652657602, val_acc: 0.5221674876847291, train_loss: 1.0845738504961484, val_loss: 1.143361752843622 (20 / 100)
train_acc: 0.5747836835599506, val_acc: 0.46798029556650245, train_loss: 1.0176695981632793, val_loss: 1.4583346244736846 (21 / 100)
train_acc: 0.553770086526576, val_acc: 0.5517241379310345, train_loss: 1.094821825605093, val_loss: 1.120042513743997 (22 / 100)
train_acc: 0.6279357231149567, val_acc: 0.5763546798029556, train_loss: 0.8899503325944483, val_loss: 1.1714457719784064 (23 / 100)
train_acc: 0.6291718170580964, val_acc: 0.5369458128078818, train_loss: 0.890588068991568, val_loss: 1.199276894184169 (24 / 100)
train_acc: 0.6341161928306551, val_acc: 0.5714285714285714, train_loss: 0.8507913096136747, val_loss: 1.1144180285372758 (25 / 100)
train_acc: 0.6711990111248455, val_acc: 0.5665024630541872, train_loss: 0.8073042031418998, val_loss: 1.2602138777671776 (26 / 100)
train_acc: 0.7033374536464772, val_acc: 0.5517241379310345, train_loss: 0.7648854288094123, val_loss: 1.0634306357999153 (27 / 100)
train_acc: 0.7564894932014833, val_acc: 0.5714285714285714, train_loss: 0.6614446793262685, val_loss: 1.0325749754318463 (28 / 100)
train_acc: 0.7330037082818294, val_acc: 0.5467980295566502, train_loss: 0.6809126517386608, val_loss: 1.3167154307435887 (29 / 100)
train_acc: 0.7898640296662547, val_acc: 0.5615763546798029, train_loss: 0.5177091837518737, val_loss: 1.5414305461267825 (30 / 100)
train_acc: 0.7873918417799752, val_acc: 0.5812807881773399, train_loss: 0.5383074083198576, val_loss: 1.1813814027849676 (31 / 100)
overfit -> train_accuracy 0.8417799752781211, val_accuracy 0.5665024630541872
lr 0.002265042705296789, batch 8, decay 7.97036358854321e-06, gamma 0.0019647526119155982, val accuracy 0.5812807881773399, val loss 1.1813814027849676 [13 / 50]
-------------------------------------
{'lr': 0.0012293420305338138, 'batch_size': 8, 'weight_decay': 2.4963998697671876e-06, 'gamma': 0.04895790490752158}
train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.7822610595170294, val_loss: 1.7655875958832614 (1 / 100)
train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7681361812596268, val_loss: 1.749188975747583 (2 / 100)
train_acc: 0.23485784919653893, val_acc: 0.26108374384236455, train_loss: 1.7545441287409684, val_loss: 1.7327933669677509 (3 / 100)
train_acc: 0.24969097651421507, val_acc: 0.33497536945812806, train_loss: 1.7135213087456778, val_loss: 1.6197038536588546 (4 / 100)
train_acc: 0.27441285537700866, val_acc: 0.27586206896551724, train_loss: 1.6867143018873425, val_loss: 1.5773163916442194 (5 / 100)
train_acc: 0.30778739184178, val_acc: 0.270935960591133, train_loss: 1.5801655020201015, val_loss: 1.6869573863269074 (6 / 100)
train_acc: 0.3374536464771323, val_acc: 0.39901477832512317, train_loss: 1.551128593746605, val_loss: 1.4425082418131712 (7 / 100)
train_acc: 0.3473423980222497, val_acc: 0.3448275862068966, train_loss: 1.4996942552854162, val_loss: 1.538819066409407 (8 / 100)
train_acc: 0.3473423980222497, val_acc: 0.39408866995073893, train_loss: 1.4776036418117906, val_loss: 1.415419150455832 (9 / 100)
train_acc: 0.37824474660074164, val_acc: 0.4433497536945813, train_loss: 1.431210779583793, val_loss: 1.3660315780216836 (10 / 100)
train_acc: 0.3683559950556242, val_acc: 0.3645320197044335, train_loss: 1.4107097547929572, val_loss: 1.4225140199285422 (11 / 100)
train_acc: 0.3868974042027194, val_acc: 0.4433497536945813, train_loss: 1.4126554266336675, val_loss: 1.3446693420410156 (12 / 100)
train_acc: 0.40914709517923364, val_acc: 0.4433497536945813, train_loss: 1.399619272229698, val_loss: 1.3412030160133475 (13 / 100)
train_acc: 0.36711990111248455, val_acc: 0.4187192118226601, train_loss: 1.3711870253012413, val_loss: 1.3106417978925657 (14 / 100)
train_acc: 0.4289245982694685, val_acc: 0.4236453201970443, train_loss: 1.367989244508213, val_loss: 1.2875170801660698 (15 / 100)
train_acc: 0.42398022249690975, val_acc: 0.4433497536945813, train_loss: 1.3284713667903754, val_loss: 1.2420383357062128 (16 / 100)
train_acc: 0.4326328800988875, val_acc: 0.45320197044334976, train_loss: 1.3079257549255239, val_loss: 1.2702923619688438 (17 / 100)
train_acc: 0.4647713226205192, val_acc: 0.4827586206896552, train_loss: 1.2428781723946665, val_loss: 1.1678826424288633 (18 / 100)
train_acc: 0.4820766378244747, val_acc: 0.4729064039408867, train_loss: 1.21118901215024, val_loss: 1.1491613687552842 (19 / 100)
train_acc: 0.4956736711990111, val_acc: 0.4876847290640394, train_loss: 1.237008282221734, val_loss: 1.2060266703807663 (20 / 100)
train_acc: 0.4758961681087763, val_acc: 0.5172413793103449, train_loss: 1.2361772667492277, val_loss: 1.158628639329243 (21 / 100)
train_acc: 0.5006180469715699, val_acc: 0.4729064039408867, train_loss: 1.175528195644044, val_loss: 1.1462509385470687 (22 / 100)
train_acc: 0.5265760197775031, val_acc: 0.47783251231527096, train_loss: 1.1074120049423577, val_loss: 1.1873727455514993 (23 / 100)
train_acc: 0.5599505562422744, val_acc: 0.541871921182266, train_loss: 1.0620723673970207, val_loss: 1.0986592185321113 (24 / 100)
train_acc: 0.584672435105068, val_acc: 0.5369458128078818, train_loss: 1.0210360425659104, val_loss: 1.1914686416757518 (25 / 100)
train_acc: 0.5908529048207664, val_acc: 0.5467980295566502, train_loss: 1.0143389634060476, val_loss: 1.0825026648972422 (26 / 100)
train_acc: 0.61557478368356, val_acc: 0.541871921182266, train_loss: 0.9828951472257654, val_loss: 1.116101710079926 (27 / 100)
train_acc: 0.6266996291718171, val_acc: 0.5517241379310345, train_loss: 0.9133555750616991, val_loss: 1.0712466121013529 (28 / 100)
train_acc: 0.6180469715698393, val_acc: 0.5665024630541872, train_loss: 0.953591854227488, val_loss: 1.0888120725824328 (29 / 100)
train_acc: 0.6699629171817059, val_acc: 0.5270935960591133, train_loss: 0.7973945143785701, val_loss: 1.3795067500598326 (30 / 100)
train_acc: 0.6773794808405439, val_acc: 0.5566502463054187, train_loss: 0.8094091947352783, val_loss: 1.0199994788381266 (31 / 100)
train_acc: 0.7453646477132262, val_acc: 0.5566502463054187, train_loss: 0.6446349365896878, val_loss: 1.2165886134349655 (32 / 100)
train_acc: 0.7367119901112484, val_acc: 0.5960591133004927, train_loss: 0.6527918454890492, val_loss: 1.0513748511892234 (33 / 100)
train_acc: 0.788627935723115, val_acc: 0.6009852216748769, train_loss: 0.558384041261614, val_loss: 1.2224758610936808 (34 / 100)
overfit -> train_accuracy 0.8022249690976514, val_accuracy 0.5369458128078818
lr 0.0012293420305338138, batch 8, decay 2.4963998697671876e-06, gamma 0.04895790490752158, val accuracy 0.6009852216748769, val loss 1.2224758610936808 [14 / 50]
-------------------------------------
{'lr': 0.0017985736712632962, 'batch_size': 8, 'weight_decay': 6.770372407122572e-06, 'gamma': 0.0012705298026838524}
train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7777614936840402, val_loss: 1.7567907055023269 (1 / 100)
train_acc: 0.22126081582200247, val_acc: 0.27586206896551724, train_loss: 1.7564202417401948, val_loss: 1.7191515080447268 (2 / 100)
train_acc: 0.21755253399258342, val_acc: 0.2660098522167488, train_loss: 1.7250180860385023, val_loss: 1.7113807166151225 (3 / 100)
train_acc: 0.29295426452410384, val_acc: 0.2561576354679803, train_loss: 1.634912573068192, val_loss: 1.690915940430364 (4 / 100)
train_acc: 0.2867737948084054, val_acc: 0.3399014778325123, train_loss: 1.6621669983244944, val_loss: 1.5424410233943922 (5 / 100)
train_acc: 0.28182941903584674, val_acc: 0.3448275862068966, train_loss: 1.636900212767687, val_loss: 1.493179280769649 (6 / 100)
train_acc: 0.33127317676143386, val_acc: 0.3842364532019704, train_loss: 1.5350557773458058, val_loss: 1.5235548271921469 (7 / 100)
train_acc: 0.3522867737948084, val_acc: 0.39408866995073893, train_loss: 1.4854687968054867, val_loss: 1.3568593774523054 (8 / 100)
train_acc: 0.3547589616810878, val_acc: 0.3891625615763547, train_loss: 1.447929579925773, val_loss: 1.4085850057930782 (9 / 100)
train_acc: 0.39060568603213847, val_acc: 0.3891625615763547, train_loss: 1.418280850971585, val_loss: 1.3041380855250242 (10 / 100)
train_acc: 0.38442521631644005, val_acc: 0.41379310344827586, train_loss: 1.4423215887307825, val_loss: 1.404032007814041 (11 / 100)
train_acc: 0.4177997527812114, val_acc: 0.3842364532019704, train_loss: 1.3453505985227003, val_loss: 1.3218090730934895 (12 / 100)
train_acc: 0.3930778739184178, val_acc: 0.4236453201970443, train_loss: 1.3705928470059878, val_loss: 1.32759798571394 (13 / 100)
train_acc: 0.4079110012360939, val_acc: 0.4236453201970443, train_loss: 1.3179580194251646, val_loss: 1.262762841332722 (14 / 100)
train_acc: 0.43510506798516685, val_acc: 0.45320197044334976, train_loss: 1.2651163945239319, val_loss: 1.3449831449339542 (15 / 100)
train_acc: 0.43139678615574784, val_acc: 0.42857142857142855, train_loss: 1.3004981152648831, val_loss: 1.1877822127248265 (16 / 100)
train_acc: 0.45488257107540175, val_acc: 0.4975369458128079, train_loss: 1.2686549344080487, val_loss: 1.2462405192440953 (17 / 100)
train_acc: 0.4746600741656366, val_acc: 0.43842364532019706, train_loss: 1.2034630339284174, val_loss: 1.2866782349318706 (18 / 100)
train_acc: 0.519159456118665, val_acc: 0.43842364532019706, train_loss: 1.1631181969483497, val_loss: 1.3043699334994914 (19 / 100)
train_acc: 0.4857849196538937, val_acc: 0.46798029556650245, train_loss: 1.1865595439161152, val_loss: 1.280242227274796 (20 / 100)
train_acc: 0.5018541409147095, val_acc: 0.45320197044334976, train_loss: 1.128410342597254, val_loss: 1.202780145142466 (21 / 100)
train_acc: 0.49443757725587145, val_acc: 0.4630541871921182, train_loss: 1.141523564849118, val_loss: 1.2242781989680136 (22 / 100)
train_acc: 0.5574783683559951, val_acc: 0.4975369458128079, train_loss: 1.0479763388191667, val_loss: 1.2114894871641262 (23 / 100)
train_acc: 0.5574783683559951, val_acc: 0.5270935960591133, train_loss: 1.0219944917374695, val_loss: 1.0929036498657 (24 / 100)
train_acc: 0.5797280593325093, val_acc: 0.5270935960591133, train_loss: 0.9875876673661882, val_loss: 1.0891739846450355 (25 / 100)
train_acc: 0.6118665018541409, val_acc: 0.5369458128078818, train_loss: 0.9232137326993812, val_loss: 1.129308716710565 (26 / 100)
train_acc: 0.6205191594561187, val_acc: 0.4975369458128079, train_loss: 0.9207110699676023, val_loss: 1.1973873303441578 (27 / 100)
train_acc: 0.6415327564894932, val_acc: 0.5467980295566502, train_loss: 0.8794694969474312, val_loss: 1.1602861104634008 (28 / 100)
train_acc: 0.6847960444993819, val_acc: 0.5221674876847291, train_loss: 0.7767133721609788, val_loss: 1.2610819169453211 (29 / 100)
train_acc: 0.6761433868974042, val_acc: 0.5517241379310345, train_loss: 0.7726737114938139, val_loss: 1.043742491143384 (30 / 100)
train_acc: 0.7194066749072929, val_acc: 0.5270935960591133, train_loss: 0.6959862744410341, val_loss: 1.203360529955972 (31 / 100)
train_acc: 0.7218788627935723, val_acc: 0.5615763546798029, train_loss: 0.6642345611480317, val_loss: 1.1422639547897677 (32 / 100)
train_acc: 0.7824474660074165, val_acc: 0.541871921182266, train_loss: 0.5529982896316773, val_loss: 1.6829731728643031 (33 / 100)
train_acc: 0.7700865265760197, val_acc: 0.5812807881773399, train_loss: 0.5818547353756295, val_loss: 1.4406697004299445 (34 / 100)
train_acc: 0.7552533992583437, val_acc: 0.5911330049261084, train_loss: 0.5941677971734989, val_loss: 1.3781525980075593 (35 / 100)
overfit -> train_accuracy 0.8380716934487021, val_accuracy 0.5862068965517241
lr 0.0017985736712632962, batch 8, decay 6.770372407122572e-06, gamma 0.0012705298026838524, val accuracy 0.5911330049261084, val loss 1.3781525980075593 [15 / 50]
-------------------------------------
{'lr': 0.001186262178088991, 'batch_size': 8, 'weight_decay': 6.891577694584457e-05, 'gamma': 0.01957640469640343}
train_acc: 0.2138442521631644, val_acc: 0.18226600985221675, train_loss: 1.7738932023384368, val_loss: 1.7589508330293477 (1 / 100)
train_acc: 0.2126081582200247, val_acc: 0.20689655172413793, train_loss: 1.7646846545788797, val_loss: 1.7386675368388886 (2 / 100)
train_acc: 0.22991347342398022, val_acc: 0.1921182266009852, train_loss: 1.735667334351463, val_loss: 1.7231467762604136 (3 / 100)
train_acc: 0.2558714462299135, val_acc: 0.3399014778325123, train_loss: 1.697244792697751, val_loss: 1.5932923473160843 (4 / 100)
train_acc: 0.29295426452410384, val_acc: 0.30049261083743845, train_loss: 1.6687341249180665, val_loss: 1.6887059951650685 (5 / 100)
train_acc: 0.3411619283065513, val_acc: 0.35960591133004927, train_loss: 1.5825698434643456, val_loss: 1.4707990820184718 (6 / 100)
train_acc: 0.34610630407911, val_acc: 0.3793103448275862, train_loss: 1.5496369990783805, val_loss: 1.4684612856709898 (7 / 100)
train_acc: 0.37824474660074164, val_acc: 0.3694581280788177, train_loss: 1.4520814947793157, val_loss: 1.4083534385183174 (8 / 100)
train_acc: 0.35970333745364647, val_acc: 0.39901477832512317, train_loss: 1.485374904091485, val_loss: 1.3517908562580352 (9 / 100)
train_acc: 0.34239802224969096, val_acc: 0.31527093596059114, train_loss: 1.4342146905597266, val_loss: 1.7011127284007708 (10 / 100)
train_acc: 0.3572311495673671, val_acc: 0.3448275862068966, train_loss: 1.4503068997627135, val_loss: 1.5288730202050045 (11 / 100)
train_acc: 0.40667490729295425, val_acc: 0.4187192118226601, train_loss: 1.379623035565295, val_loss: 1.2900421860182814 (12 / 100)
train_acc: 0.40173053152039556, val_acc: 0.4236453201970443, train_loss: 1.34286871416165, val_loss: 1.2889534428789111 (13 / 100)
train_acc: 0.4054388133498146, val_acc: 0.39408866995073893, train_loss: 1.3488007864934406, val_loss: 1.3660990851266044 (14 / 100)
train_acc: 0.43510506798516685, val_acc: 0.42857142857142855, train_loss: 1.3018758504735524, val_loss: 1.218188942653205 (15 / 100)
train_acc: 0.4289245982694685, val_acc: 0.39901477832512317, train_loss: 1.3188393507958638, val_loss: 1.2815460225044213 (16 / 100)
train_acc: 0.4437577255871446, val_acc: 0.43842364532019706, train_loss: 1.2612182544394683, val_loss: 1.3358876634701131 (17 / 100)
train_acc: 0.45982694684796044, val_acc: 0.4482758620689655, train_loss: 1.2344994105868345, val_loss: 1.41732925029811 (18 / 100)
train_acc: 0.4635352286773795, val_acc: 0.46798029556650245, train_loss: 1.256229801879383, val_loss: 1.2090858802419577 (19 / 100)
train_acc: 0.4857849196538937, val_acc: 0.458128078817734, train_loss: 1.2001651005338236, val_loss: 1.2076979805096029 (20 / 100)
train_acc: 0.45982694684796044, val_acc: 0.42857142857142855, train_loss: 1.1946113209789262, val_loss: 1.2836655102339871 (21 / 100)
train_acc: 0.4956736711990111, val_acc: 0.4975369458128079, train_loss: 1.1737368044953411, val_loss: 1.1955267484552168 (22 / 100)
train_acc: 0.5080346106304079, val_acc: 0.4827586206896552, train_loss: 1.1429833352050027, val_loss: 1.2324176084231862 (23 / 100)
train_acc: 0.519159456118665, val_acc: 0.4630541871921182, train_loss: 1.1319534931843007, val_loss: 1.2107538075869895 (24 / 100)
train_acc: 0.5278121137206427, val_acc: 0.4729064039408867, train_loss: 1.1682914208128072, val_loss: 1.2271733771404023 (25 / 100)
train_acc: 0.5550061804697157, val_acc: 0.5024630541871922, train_loss: 1.0893989882451496, val_loss: 1.143706471755587 (26 / 100)
train_acc: 0.5475896168108776, val_acc: 0.5665024630541872, train_loss: 1.074445045480622, val_loss: 0.9935125708580017 (27 / 100)
train_acc: 0.5871446229913473, val_acc: 0.5073891625615764, train_loss: 0.9721482389494869, val_loss: 1.234652545064541 (28 / 100)
train_acc: 0.588380716934487, val_acc: 0.5123152709359606, train_loss: 0.952636272562449, val_loss: 1.3894308681411696 (29 / 100)
train_acc: 0.6143386897404203, val_acc: 0.541871921182266, train_loss: 0.9231101971474213, val_loss: 1.0002160166284721 (30 / 100)
train_acc: 0.6662546353522868, val_acc: 0.5467980295566502, train_loss: 0.853624851064128, val_loss: 1.1545628482485053 (31 / 100)
train_acc: 0.6711990111248455, val_acc: 0.5615763546798029, train_loss: 0.8488163335066938, val_loss: 1.064182674649901 (32 / 100)
train_acc: 0.6501854140914709, val_acc: 0.5369458128078818, train_loss: 0.859116861935157, val_loss: 1.059413513379731 (33 / 100)
train_acc: 0.6687268232385661, val_acc: 0.5172413793103449, train_loss: 0.8091242130666197, val_loss: 1.2904582499283288 (34 / 100)
train_acc: 0.681087762669963, val_acc: 0.5615763546798029, train_loss: 0.7704795383847098, val_loss: 1.2264628815533491 (35 / 100)
train_acc: 0.7268232385661311, val_acc: 0.5615763546798029, train_loss: 0.7136053299284983, val_loss: 1.1049482329138394 (36 / 100)
overfit -> train_accuracy 0.7676143386897404, val_accuracy 0.5073891625615764
lr 0.001186262178088991, batch 8, decay 6.891577694584457e-05, gamma 0.01957640469640343, val accuracy 0.5665024630541872, val loss 0.9935125708580017 [16 / 50]
-------------------------------------
{'lr': 0.0010339137462550302, 'batch_size': 8, 'weight_decay': 2.2486189682794548e-05, 'gamma': 0.00852620102848672}
train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7829768513277524, val_loss: 1.762414783679793 (1 / 100)
train_acc: 0.20395550061804696, val_acc: 0.22167487684729065, train_loss: 1.7698522115077902, val_loss: 1.7495194673538208 (2 / 100)
train_acc: 0.23980222496909764, val_acc: 0.2955665024630542, train_loss: 1.7561884213731669, val_loss: 1.7253366426881311 (3 / 100)
train_acc: 0.26823238566131025, val_acc: 0.22167487684729065, train_loss: 1.7195635390075381, val_loss: 1.6688553359120937 (4 / 100)
train_acc: 0.29171817058096416, val_acc: 0.3251231527093596, train_loss: 1.645380726556106, val_loss: 1.521500820596817 (5 / 100)
train_acc: 0.34239802224969096, val_acc: 0.3103448275862069, train_loss: 1.601610467224687, val_loss: 1.5384669039636998 (6 / 100)
train_acc: 0.3127317676143387, val_acc: 0.30049261083743845, train_loss: 1.5680341646904115, val_loss: 1.641422274077467 (7 / 100)
train_acc: 0.37453646477132263, val_acc: 0.37438423645320196, train_loss: 1.5460548770737148, val_loss: 1.4496139029563941 (8 / 100)
train_acc: 0.35599505562422745, val_acc: 0.41379310344827586, train_loss: 1.4640009320710587, val_loss: 1.3750205527385468 (9 / 100)
train_acc: 0.3535228677379481, val_acc: 0.39408866995073893, train_loss: 1.4593873793174075, val_loss: 1.3457055837650018 (10 / 100)
train_acc: 0.38442521631644005, val_acc: 0.3448275862068966, train_loss: 1.4310125405767793, val_loss: 1.5795449370821122 (11 / 100)
train_acc: 0.3868974042027194, val_acc: 0.4236453201970443, train_loss: 1.434323434187249, val_loss: 1.2804071386459426 (12 / 100)
train_acc: 0.37453646477132263, val_acc: 0.4187192118226601, train_loss: 1.4131842766910312, val_loss: 1.348330040283391 (13 / 100)
train_acc: 0.39555006180469715, val_acc: 0.45320197044334976, train_loss: 1.393487642957784, val_loss: 1.2651651366590866 (14 / 100)
train_acc: 0.4177997527812114, val_acc: 0.43349753694581283, train_loss: 1.381116369598728, val_loss: 1.2715984383240122 (15 / 100)
train_acc: 0.44870210135970334, val_acc: 0.43349753694581283, train_loss: 1.3159860721772032, val_loss: 1.2864778552736555 (16 / 100)
train_acc: 0.4388133498145859, val_acc: 0.42857142857142855, train_loss: 1.3057656273411584, val_loss: 1.2622148697012163 (17 / 100)
train_acc: 0.44128553770086526, val_acc: 0.4729064039408867, train_loss: 1.3013869661041184, val_loss: 1.2581456277170793 (18 / 100)
train_acc: 0.47713226205191595, val_acc: 0.43842364532019706, train_loss: 1.2540273799000328, val_loss: 1.2279297659549806 (19 / 100)
train_acc: 0.4672435105067985, val_acc: 0.5073891625615764, train_loss: 1.2479164538483685, val_loss: 1.1865068039870614 (20 / 100)
train_acc: 0.45982694684796044, val_acc: 0.5073891625615764, train_loss: 1.2333546574831893, val_loss: 1.210357987528364 (21 / 100)
train_acc: 0.5105067985166872, val_acc: 0.5073891625615764, train_loss: 1.1843543129297358, val_loss: 1.1022559668630214 (22 / 100)
train_acc: 0.4956736711990111, val_acc: 0.4975369458128079, train_loss: 1.1881384634411674, val_loss: 1.1664699891517902 (23 / 100)
train_acc: 0.4919653893695921, val_acc: 0.5073891625615764, train_loss: 1.1923012347392306, val_loss: 1.1182183138842654 (24 / 100)
train_acc: 0.5389369592088998, val_acc: 0.45320197044334976, train_loss: 1.1224648009419294, val_loss: 1.1624178786583135 (25 / 100)
train_acc: 0.5414091470951793, val_acc: 0.4975369458128079, train_loss: 1.096029799270394, val_loss: 1.1593590320037503 (26 / 100)
train_acc: 0.5772558714462299, val_acc: 0.4876847290640394, train_loss: 1.0548790152494338, val_loss: 1.1620461579614085 (27 / 100)
train_acc: 0.5723114956736712, val_acc: 0.5172413793103449, train_loss: 1.015988510382927, val_loss: 1.0694121322021108 (28 / 100)
train_acc: 0.5896168108776267, val_acc: 0.5270935960591133, train_loss: 0.9858182337139682, val_loss: 1.1215921511203784 (29 / 100)
train_acc: 0.5772558714462299, val_acc: 0.5320197044334976, train_loss: 0.9999625697566199, val_loss: 1.0601092309787357 (30 / 100)
train_acc: 0.6316440049443758, val_acc: 0.5615763546798029, train_loss: 0.8913124583118189, val_loss: 1.1036004468137994 (31 / 100)
train_acc: 0.6489493201483313, val_acc: 0.5320197044334976, train_loss: 0.8791353755592564, val_loss: 1.1304518066603562 (32 / 100)
train_acc: 0.6489493201483313, val_acc: 0.5960591133004927, train_loss: 0.8502535999779058, val_loss: 1.062395201674823 (33 / 100)
train_acc: 0.7107540173053152, val_acc: 0.5369458128078818, train_loss: 0.7312602604864848, val_loss: 1.195276477066754 (34 / 100)
train_acc: 0.6909765142150803, val_acc: 0.5862068965517241, train_loss: 0.8096007328245637, val_loss: 1.0844670005619819 (35 / 100)
train_acc: 0.7243510506798516, val_acc: 0.5714285714285714, train_loss: 0.7026837401101262, val_loss: 1.1046459639600932 (36 / 100)
train_acc: 0.7601977750309024, val_acc: 0.5517241379310345, train_loss: 0.6145602006405334, val_loss: 1.2463834323906546 (37 / 100)
train_acc: 0.7935723114956736, val_acc: 0.5911330049261084, train_loss: 0.5482396320595287, val_loss: 1.2741284299953817 (38 / 100)
train_acc: 0.8046971569839307, val_acc: 0.5566502463054187, train_loss: 0.558502946265401, val_loss: 1.2834677743207057 (39 / 100)
train_acc: 0.754017305315204, val_acc: 0.5763546798029556, train_loss: 0.628413886310733, val_loss: 1.1019341839945376 (40 / 100)
overfit -> train_accuracy 0.8331273176761433, val_accuracy 0.5665024630541872
lr 0.0010339137462550302, batch 8, decay 2.2486189682794548e-05, gamma 0.00852620102848672, val accuracy 0.5960591133004927, val loss 1.062395201674823 [17 / 50]
-------------------------------------
{'lr': 0.0014189436352533258, 'batch_size': 8, 'weight_decay': 2.6942389037779982e-06, 'gamma': 0.02008944129453069}
train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7668264099340356, val_loss: 1.760201949791368 (1 / 100)
train_acc: 0.20148331273176762, val_acc: 0.3251231527093596, train_loss: 1.7637080431573913, val_loss: 1.7328840294495005 (2 / 100)
train_acc: 0.27935723114956734, val_acc: 0.33004926108374383, train_loss: 1.7303790427256278, val_loss: 1.652577748439582 (3 / 100)
train_acc: 0.2978986402966625, val_acc: 0.28078817733990147, train_loss: 1.6463484811252362, val_loss: 1.6081860130056371 (4 / 100)
train_acc: 0.3053152039555006, val_acc: 0.35960591133004927, train_loss: 1.6313729809300124, val_loss: 1.5471642457792911 (5 / 100)
train_acc: 0.3374536464771323, val_acc: 0.39901477832512317, train_loss: 1.5406832264733992, val_loss: 1.402942794590748 (6 / 100)
train_acc: 0.3683559950556242, val_acc: 0.3645320197044335, train_loss: 1.4672036204850865, val_loss: 1.3744171758003423 (7 / 100)
train_acc: 0.3362175525339926, val_acc: 0.4039408866995074, train_loss: 1.4933542132524977, val_loss: 1.355611337816774 (8 / 100)
train_acc: 0.3794808405438813, val_acc: 0.4088669950738916, train_loss: 1.4563230032973884, val_loss: 1.3911420001185 (9 / 100)
train_acc: 0.411619283065513, val_acc: 0.4039408866995074, train_loss: 1.3748369386375907, val_loss: 1.3875047403016114 (10 / 100)
train_acc: 0.38813349814585907, val_acc: 0.3891625615763547, train_loss: 1.3711814000668425, val_loss: 1.3061374402398547 (11 / 100)
train_acc: 0.39184177997527814, val_acc: 0.39901477832512317, train_loss: 1.3898617882369033, val_loss: 1.3140507428516894 (12 / 100)
train_acc: 0.42027194066749074, val_acc: 0.4088669950738916, train_loss: 1.3487982244515153, val_loss: 1.2570013594744829 (13 / 100)
train_acc: 0.415327564894932, val_acc: 0.4433497536945813, train_loss: 1.3126851557506471, val_loss: 1.2516055676737443 (14 / 100)
train_acc: 0.44128553770086526, val_acc: 0.4088669950738916, train_loss: 1.2827782244853243, val_loss: 1.2715334181715114 (15 / 100)
train_acc: 0.4264524103831891, val_acc: 0.4187192118226601, train_loss: 1.2986845298220113, val_loss: 1.3402485254362886 (16 / 100)
train_acc: 0.44252163164400493, val_acc: 0.41379310344827586, train_loss: 1.28630947860416, val_loss: 1.225843451880469 (17 / 100)
train_acc: 0.46600741656365885, val_acc: 0.43842364532019706, train_loss: 1.2375619731226428, val_loss: 1.2027225694045645 (18 / 100)
train_acc: 0.4820766378244747, val_acc: 0.4975369458128079, train_loss: 1.1875931544710592, val_loss: 1.255514788510177 (19 / 100)
train_acc: 0.47713226205191595, val_acc: 0.5024630541871922, train_loss: 1.202028457254945, val_loss: 1.1937572615487235 (20 / 100)
train_acc: 0.5055624227441285, val_acc: 0.43842364532019706, train_loss: 1.1591068148170915, val_loss: 1.2385102375387558 (21 / 100)
train_acc: 0.5030902348578492, val_acc: 0.45320197044334976, train_loss: 1.1449606387665303, val_loss: 1.2124040214886218 (22 / 100)
train_acc: 0.4956736711990111, val_acc: 0.4975369458128079, train_loss: 1.1472394649118958, val_loss: 1.131776174300997 (23 / 100)
train_acc: 0.5327564894932015, val_acc: 0.4433497536945813, train_loss: 1.0706533181799947, val_loss: 1.7504760787404816 (24 / 100)
train_acc: 0.5512978986402967, val_acc: 0.43349753694581283, train_loss: 1.0375268338489887, val_loss: 1.2268411053225325 (25 / 100)
train_acc: 0.5438813349814586, val_acc: 0.4630541871921182, train_loss: 1.054695997161535, val_loss: 1.2724141974754521 (26 / 100)
train_acc: 0.553770086526576, val_acc: 0.47783251231527096, train_loss: 1.0934806662820031, val_loss: 1.4240365078296569 (27 / 100)
train_acc: 0.6069221260815822, val_acc: 0.5123152709359606, train_loss: 0.9690903043570124, val_loss: 1.0974815725692975 (28 / 100)
train_acc: 0.6242274412855378, val_acc: 0.541871921182266, train_loss: 0.9335413226680497, val_loss: 1.0583017254110627 (29 / 100)
train_acc: 0.6365883807169345, val_acc: 0.5172413793103449, train_loss: 0.8800559326803729, val_loss: 1.2517877077234203 (30 / 100)
train_acc: 0.6378244746600742, val_acc: 0.5763546798029556, train_loss: 0.8772361246116082, val_loss: 1.068245877186066 (31 / 100)
train_acc: 0.6687268232385661, val_acc: 0.5320197044334976, train_loss: 0.7812423626483592, val_loss: 1.2567463672807064 (32 / 100)
train_acc: 0.688504326328801, val_acc: 0.5517241379310345, train_loss: 0.7529596466069168, val_loss: 1.1072535409128725 (33 / 100)
train_acc: 0.7144622991347342, val_acc: 0.5714285714285714, train_loss: 0.7284217788498245, val_loss: 1.2960440763111771 (34 / 100)
train_acc: 0.7404202719406675, val_acc: 0.5566502463054187, train_loss: 0.6880946073897543, val_loss: 1.01224093719069 (35 / 100)
train_acc: 0.7861557478368356, val_acc: 0.5862068965517241, train_loss: 0.5928508772690895, val_loss: 1.1196815533003783 (36 / 100)
overfit -> train_accuracy 0.788627935723115, val_accuracy 0.5270935960591133
lr 0.0014189436352533258, batch 8, decay 2.6942389037779982e-06, gamma 0.02008944129453069, val accuracy 0.5862068965517241, val loss 1.1196815533003783 [18 / 50]
-------------------------------------
{'lr': 0.003095908403444121, 'batch_size': 8, 'weight_decay': 7.317625559589007e-05, 'gamma': 0.001976319782361576}
train_acc: 0.1903584672435105, val_acc: 0.3399014778325123, train_loss: 1.7815425233876307, val_loss: 1.7586471177087042 (1 / 100)
train_acc: 0.2323856613102596, val_acc: 0.1921182266009852, train_loss: 1.7333555690142959, val_loss: 1.7174075055004927 (2 / 100)
train_acc: 0.2595797280593325, val_acc: 0.3497536945812808, train_loss: 1.6819885059988542, val_loss: 1.5810155310654288 (3 / 100)
train_acc: 0.30284301606922126, val_acc: 0.3251231527093596, train_loss: 1.6392167811045983, val_loss: 1.5679173181796897 (4 / 100)
train_acc: 0.3176761433868974, val_acc: 0.37438423645320196, train_loss: 1.5868986075829221, val_loss: 1.5177574833038405 (5 / 100)
train_acc: 0.33127317676143386, val_acc: 0.39408866995073893, train_loss: 1.5680486460403988, val_loss: 1.684713358949558 (6 / 100)
train_acc: 0.3510506798516687, val_acc: 0.3793103448275862, train_loss: 1.541703535540879, val_loss: 1.485978614520557 (7 / 100)
train_acc: 0.3695920889987639, val_acc: 0.35960591133004927, train_loss: 1.4800508104826522, val_loss: 1.3828536462901262 (8 / 100)
train_acc: 0.3720642768850433, val_acc: 0.3645320197044335, train_loss: 1.4285282010053675, val_loss: 1.337253319218828 (9 / 100)
train_acc: 0.37330037082818296, val_acc: 0.3694581280788177, train_loss: 1.4260145377169728, val_loss: 1.3821532297604189 (10 / 100)
train_acc: 0.36093943139678614, val_acc: 0.37438423645320196, train_loss: 1.37710408492495, val_loss: 1.4396196092878069 (11 / 100)
train_acc: 0.3819530284301607, val_acc: 0.4088669950738916, train_loss: 1.3832267110221024, val_loss: 1.3280995391272559 (12 / 100)
train_acc: 0.40296662546353523, val_acc: 0.42857142857142855, train_loss: 1.3755755148947755, val_loss: 1.2426868964885842 (13 / 100)
train_acc: 0.41656365883807167, val_acc: 0.4236453201970443, train_loss: 1.3298860999061386, val_loss: 1.300671802365721 (14 / 100)
train_acc: 0.45488257107540175, val_acc: 0.4482758620689655, train_loss: 1.2606468449711652, val_loss: 1.3211201291366164 (15 / 100)
train_acc: 0.4647713226205192, val_acc: 0.4236453201970443, train_loss: 1.2432157761971057, val_loss: 1.302813455095432 (16 / 100)
train_acc: 0.4796044499381953, val_acc: 0.5172413793103449, train_loss: 1.1874707501073114, val_loss: 1.176500224714796 (17 / 100)
train_acc: 0.4969097651421508, val_acc: 0.5221674876847291, train_loss: 1.1602216438840434, val_loss: 1.1929378938205137 (18 / 100)
train_acc: 0.4610630407911001, val_acc: 0.46798029556650245, train_loss: 1.273311399264153, val_loss: 1.2054612865588936 (19 / 100)
train_acc: 0.5401730531520396, val_acc: 0.5024630541871922, train_loss: 1.1088915384007325, val_loss: 1.1815694229943412 (20 / 100)
train_acc: 0.5142150803461063, val_acc: 0.5024630541871922, train_loss: 1.0848323986468416, val_loss: 1.0953441511821278 (21 / 100)
train_acc: 0.580964153275649, val_acc: 0.5172413793103449, train_loss: 1.0004748269565615, val_loss: 1.5165705654421464 (22 / 100)
train_acc: 0.6341161928306551, val_acc: 0.4827586206896552, train_loss: 0.9016872582535809, val_loss: 1.1280601711696006 (23 / 100)
train_acc: 0.6254635352286774, val_acc: 0.5566502463054187, train_loss: 0.9105172062981114, val_loss: 1.236368681996914 (24 / 100)
train_acc: 0.65389369592089, val_acc: 0.625615763546798, train_loss: 0.8356680643131176, val_loss: 1.219398353193781 (25 / 100)
train_acc: 0.69221260815822, val_acc: 0.5812807881773399, train_loss: 0.7620537652368157, val_loss: 1.0943063755928002 (26 / 100)
train_acc: 0.6996291718170581, val_acc: 0.5221674876847291, train_loss: 0.7935385415226921, val_loss: 1.1409287511421542 (27 / 100)
train_acc: 0.7268232385661311, val_acc: 0.5911330049261084, train_loss: 0.7170080649395956, val_loss: 1.2264747091114814 (28 / 100)
train_acc: 0.7713226205191595, val_acc: 0.5665024630541872, train_loss: 0.5843997291935095, val_loss: 1.4674180423097658 (29 / 100)
train_acc: 0.7466007416563659, val_acc: 0.6009852216748769, train_loss: 0.6696771950597964, val_loss: 1.2498369479707896 (30 / 100)
train_acc: 0.7762669962917181, val_acc: 0.5763546798029556, train_loss: 0.5829502186462817, val_loss: 1.4593753205437965 (31 / 100)
train_acc: 0.8009888751545118, val_acc: 0.6600985221674877, train_loss: 0.6013466029880368, val_loss: 1.1879625777012022 (32 / 100)
train_acc: 0.823238566131026, val_acc: 0.6059113300492611, train_loss: 0.4617657539134267, val_loss: 1.5166643123908583 (33 / 100)
overfit -> train_accuracy 0.8776266996291718, val_accuracy 0.6206896551724138
lr 0.003095908403444121, batch 8, decay 7.317625559589007e-05, gamma 0.001976319782361576, val accuracy 0.6600985221674877, val loss 1.1879625777012022 [19 / 50]
-------------------------------------
{'lr': 0.0011173806830772862, 'batch_size': 8, 'weight_decay': 1.2017541542928068e-06, 'gamma': 0.09078346296373202}
train_acc: 0.17428924598269468, val_acc: 0.18226600985221675, train_loss: 1.782107857897491, val_loss: 1.7628919720062481 (1 / 100)
train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.762585682391531, val_loss: 1.7400849740493474 (2 / 100)
train_acc: 0.2088998763906057, val_acc: 0.23645320197044334, train_loss: 1.7443594310722776, val_loss: 1.704471542330211 (3 / 100)
train_acc: 0.29295426452410384, val_acc: 0.19704433497536947, train_loss: 1.6751719882668021, val_loss: 1.7684564390793223 (4 / 100)
train_acc: 0.2880098887515451, val_acc: 0.3399014778325123, train_loss: 1.6347756164772402, val_loss: 1.567533143048216 (5 / 100)
train_acc: 0.3300370828182942, val_acc: 0.22167487684729065, train_loss: 1.5901369846648132, val_loss: 1.716700213296073 (6 / 100)
train_acc: 0.33498145859085293, val_acc: 0.3891625615763547, train_loss: 1.5552523139085994, val_loss: 1.4400130040539896 (7 / 100)
train_acc: 0.3510506798516687, val_acc: 0.4088669950738916, train_loss: 1.5419549840342424, val_loss: 1.4704077020654538 (8 / 100)
train_acc: 0.3547589616810878, val_acc: 0.4187192118226601, train_loss: 1.447437133128917, val_loss: 1.3701954316623106 (9 / 100)
train_acc: 0.38813349814585907, val_acc: 0.3793103448275862, train_loss: 1.4002534196756973, val_loss: 1.4294870498732393 (10 / 100)
train_acc: 0.4004944375772559, val_acc: 0.4187192118226601, train_loss: 1.4294596085294953, val_loss: 1.2932045213107406 (11 / 100)
train_acc: 0.377008652657602, val_acc: 0.39408866995073893, train_loss: 1.3933650952187102, val_loss: 1.2803175525712263 (12 / 100)
train_acc: 0.3992583436341162, val_acc: 0.3793103448275862, train_loss: 1.3937048856053864, val_loss: 1.2728415433996416 (13 / 100)
train_acc: 0.4215080346106304, val_acc: 0.3891625615763547, train_loss: 1.3601444103514455, val_loss: 1.255609334396024 (14 / 100)
train_acc: 0.40296662546353523, val_acc: 0.43842364532019706, train_loss: 1.3564953428263717, val_loss: 1.2842923837342286 (15 / 100)
train_acc: 0.4511742892459827, val_acc: 0.4088669950738916, train_loss: 1.2830602288688215, val_loss: 1.3228129541932656 (16 / 100)
train_acc: 0.446229913473424, val_acc: 0.4236453201970443, train_loss: 1.290879914433464, val_loss: 1.3470262788199439 (17 / 100)
train_acc: 0.4363411619283066, val_acc: 0.4433497536945813, train_loss: 1.236468359330676, val_loss: 1.2109677286570883 (18 / 100)
train_acc: 0.45859085290482077, val_acc: 0.4630541871921182, train_loss: 1.2261294074936762, val_loss: 1.1959185700111201 (19 / 100)
train_acc: 0.4437577255871446, val_acc: 0.45320197044334976, train_loss: 1.2438358696193719, val_loss: 1.3033995816273054 (20 / 100)
train_acc: 0.49443757725587145, val_acc: 0.43842364532019706, train_loss: 1.1954080777056284, val_loss: 1.1910914405813358 (21 / 100)
train_acc: 0.48084054388133496, val_acc: 0.4236453201970443, train_loss: 1.1877276764811928, val_loss: 1.3100814161629513 (22 / 100)
train_acc: 0.5055624227441285, val_acc: 0.45320197044334976, train_loss: 1.1817506229037555, val_loss: 1.19092339952591 (23 / 100)
train_acc: 0.5377008652657602, val_acc: 0.5123152709359606, train_loss: 1.0998617575696137, val_loss: 1.199177140085568 (24 / 100)
train_acc: 0.5302843016069221, val_acc: 0.46798029556650245, train_loss: 1.117503683410852, val_loss: 1.208380981031897 (25 / 100)
train_acc: 0.5142150803461063, val_acc: 0.4876847290640394, train_loss: 1.1619020320281699, val_loss: 1.1266896983085595 (26 / 100)
train_acc: 0.5327564894932015, val_acc: 0.5467980295566502, train_loss: 1.091183309944068, val_loss: 1.0948902759058723 (27 / 100)
train_acc: 0.5525339925834364, val_acc: 0.4975369458128079, train_loss: 1.062360705197668, val_loss: 1.1954968806558055 (28 / 100)
train_acc: 0.553770086526576, val_acc: 0.4975369458128079, train_loss: 1.0252992856340444, val_loss: 1.178620521070922 (29 / 100)
train_acc: 0.6106304079110012, val_acc: 0.5172413793103449, train_loss: 0.9690123252727193, val_loss: 1.0939620612877343 (30 / 100)
train_acc: 0.6056860321384425, val_acc: 0.5221674876847291, train_loss: 0.9539500871311749, val_loss: 1.0861350450609706 (31 / 100)
train_acc: 0.6526576019777504, val_acc: 0.5763546798029556, train_loss: 0.8516622705129522, val_loss: 1.0618327051547949 (32 / 100)
train_acc: 0.6625463535228677, val_acc: 0.5615763546798029, train_loss: 0.8277550247602616, val_loss: 1.2062807271046003 (33 / 100)
train_acc: 0.6872682323856613, val_acc: 0.5517241379310345, train_loss: 0.7839102862939257, val_loss: 1.164483733952339 (34 / 100)
train_acc: 0.6724351050679852, val_acc: 0.5320197044334976, train_loss: 0.7938357615205649, val_loss: 1.1122014628255308 (35 / 100)
train_acc: 0.7601977750309024, val_acc: 0.5467980295566502, train_loss: 0.6437207774267208, val_loss: 1.508764133077537 (36 / 100)
train_acc: 0.6946847960444994, val_acc: 0.5911330049261084, train_loss: 0.7449831178662804, val_loss: 1.1629903927225198 (37 / 100)
train_acc: 0.7503090234857849, val_acc: 0.541871921182266, train_loss: 0.6466524906005199, val_loss: 1.0787021964054389 (38 / 100)
overfit -> train_accuracy 0.7861557478368356, val_accuracy 0.5270935960591133
lr 0.0011173806830772862, batch 8, decay 1.2017541542928068e-06, gamma 0.09078346296373202, val accuracy 0.5911330049261084, val loss 1.1629903927225198 [20 / 50]
-------------------------------------
{'lr': 0.0024220311316078664, 'batch_size': 8, 'weight_decay': 1.3469735469059525e-06, 'gamma': 0.002055893687523462}
train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7796201697091385, val_loss: 1.756952010352036 (1 / 100)
train_acc: 0.20148331273176762, val_acc: 0.270935960591133, train_loss: 1.7516298205655054, val_loss: 1.7463821642504538 (2 / 100)
train_acc: 0.28059332509270707, val_acc: 0.1921182266009852, train_loss: 1.7057959081510679, val_loss: 1.7137608087708798 (3 / 100)
train_acc: 0.26823238566131025, val_acc: 0.23645320197044334, train_loss: 1.703765794874269, val_loss: 1.7440203903931115 (4 / 100)
train_acc: 0.2719406674907293, val_acc: 0.27586206896551724, train_loss: 1.6446975456327384, val_loss: 1.6791929071172704 (5 / 100)
train_acc: 0.33868974042027195, val_acc: 0.35467980295566504, train_loss: 1.58782591130147, val_loss: 1.4332733929450876 (6 / 100)
train_acc: 0.34363411619283063, val_acc: 0.3891625615763547, train_loss: 1.5634191230731487, val_loss: 1.4531468798961547 (7 / 100)
train_acc: 0.34487021013597036, val_acc: 0.4039408866995074, train_loss: 1.482888437761512, val_loss: 1.3712600733846279 (8 / 100)
train_acc: 0.3720642768850433, val_acc: 0.35467980295566504, train_loss: 1.4647199081991453, val_loss: 1.3183498265120783 (9 / 100)
train_acc: 0.3794808405438813, val_acc: 0.39408866995073893, train_loss: 1.4053538222837507, val_loss: 1.3533978817498156 (10 / 100)
train_acc: 0.3980222496909765, val_acc: 0.4088669950738916, train_loss: 1.366696340191939, val_loss: 1.2829773246948355 (11 / 100)
train_acc: 0.3473423980222497, val_acc: 0.39901477832512317, train_loss: 1.4081978392689425, val_loss: 1.3188531668902619 (12 / 100)
train_acc: 0.377008652657602, val_acc: 0.3694581280788177, train_loss: 1.3713484473517268, val_loss: 1.3103253917741071 (13 / 100)
train_acc: 0.41656365883807167, val_acc: 0.3694581280788177, train_loss: 1.318334857966608, val_loss: 1.2781154774679926 (14 / 100)
train_acc: 0.41285537700865266, val_acc: 0.41379310344827586, train_loss: 1.3240810625337995, val_loss: 1.3653268749490748 (15 / 100)
train_acc: 0.411619283065513, val_acc: 0.42857142857142855, train_loss: 1.3134386838587606, val_loss: 1.2797014105496147 (16 / 100)
train_acc: 0.44870210135970334, val_acc: 0.42857142857142855, train_loss: 1.2463657298989879, val_loss: 1.2200420237527105 (17 / 100)
train_acc: 0.5105067985166872, val_acc: 0.4433497536945813, train_loss: 1.1688990878823071, val_loss: 1.2670453146760687 (18 / 100)
train_acc: 0.47713226205191595, val_acc: 0.5024630541871922, train_loss: 1.2070754876979644, val_loss: 1.139530755028936 (19 / 100)
train_acc: 0.48825710754017304, val_acc: 0.4975369458128079, train_loss: 1.1397676627037698, val_loss: 1.205862687799731 (20 / 100)
train_acc: 0.519159456118665, val_acc: 0.49261083743842365, train_loss: 1.1009187383026953, val_loss: 1.2409879459536135 (21 / 100)
train_acc: 0.519159456118665, val_acc: 0.5073891625615764, train_loss: 1.0886976172219691, val_loss: 1.1128094724833673 (22 / 100)
train_acc: 0.5698393077873919, val_acc: 0.5172413793103449, train_loss: 1.038203650852953, val_loss: 1.1856682721910805 (23 / 100)
train_acc: 0.595797280593325, val_acc: 0.5320197044334976, train_loss: 0.9401733598249362, val_loss: 1.2263227196162558 (24 / 100)
train_acc: 0.6093943139678616, val_acc: 0.5566502463054187, train_loss: 0.9803729844181736, val_loss: 1.1229823281612303 (25 / 100)
train_acc: 0.6452410383189122, val_acc: 0.5172413793103449, train_loss: 0.8709384357678728, val_loss: 1.1216768179033778 (26 / 100)
train_acc: 0.657601977750309, val_acc: 0.5911330049261084, train_loss: 0.8895312823235473, val_loss: 1.0352724402996119 (27 / 100)
train_acc: 0.6687268232385661, val_acc: 0.5221674876847291, train_loss: 0.8163481669313976, val_loss: 1.384628440358956 (28 / 100)
train_acc: 0.6847960444993819, val_acc: 0.5714285714285714, train_loss: 0.7611448586502829, val_loss: 1.2975183690122782 (29 / 100)
train_acc: 0.7342398022249691, val_acc: 0.5369458128078818, train_loss: 0.7041418820582744, val_loss: 1.4970643129842034 (30 / 100)
train_acc: 0.7379480840543882, val_acc: 0.5517241379310345, train_loss: 0.664841446652548, val_loss: 1.741895376167861 (31 / 100)
train_acc: 0.7626699629171817, val_acc: 0.5665024630541872, train_loss: 0.6367491877123216, val_loss: 1.1111228507140587 (32 / 100)
train_acc: 0.7676143386897404, val_acc: 0.6059113300492611, train_loss: 0.6071012910128524, val_loss: 1.3657387436317106 (33 / 100)
overfit -> train_accuracy 0.8207663782447466, val_accuracy 0.5123152709359606
lr 0.0024220311316078664, batch 8, decay 1.3469735469059525e-06, gamma 0.002055893687523462, val accuracy 0.6059113300492611, val loss 1.3657387436317106 [21 / 50]
-------------------------------------
{'lr': 0.002239481967607394, 'batch_size': 8, 'weight_decay': 1.1249186713728107e-05, 'gamma': 0.0029540916327290205}
train_acc: 0.18912237330037082, val_acc: 0.2019704433497537, train_loss: 1.7749991126644007, val_loss: 1.7475577769021096 (1 / 100)
train_acc: 0.19777503090234858, val_acc: 0.35960591133004927, train_loss: 1.7438070606094944, val_loss: 1.7112709371914416 (2 / 100)
train_acc: 0.22620519159456118, val_acc: 0.3054187192118227, train_loss: 1.7470636341274153, val_loss: 1.7044306171351467 (3 / 100)
train_acc: 0.2669962917181706, val_acc: 0.2857142857142857, train_loss: 1.6970767691934066, val_loss: 1.6422068849572995 (4 / 100)
train_acc: 0.3189122373300371, val_acc: 0.32019704433497537, train_loss: 1.623761045033911, val_loss: 1.6072347399049205 (5 / 100)
train_acc: 0.3164400494437577, val_acc: 0.3842364532019704, train_loss: 1.5956830109005657, val_loss: 1.4541915179473426 (6 / 100)
train_acc: 0.34857849196538937, val_acc: 0.31527093596059114, train_loss: 1.5037325740007887, val_loss: 1.5261468540858754 (7 / 100)
train_acc: 0.34981458590852904, val_acc: 0.33004926108374383, train_loss: 1.4609128721564897, val_loss: 1.475145649440183 (8 / 100)
train_acc: 0.3831891223733004, val_acc: 0.3645320197044335, train_loss: 1.479817540448145, val_loss: 1.3494882193104973 (9 / 100)
train_acc: 0.38442521631644005, val_acc: 0.4187192118226601, train_loss: 1.371189507330009, val_loss: 1.4120028033632364 (10 / 100)
train_acc: 0.39184177997527814, val_acc: 0.42857142857142855, train_loss: 1.368261636703359, val_loss: 1.295343090160727 (11 / 100)
train_acc: 0.4326328800988875, val_acc: 0.43842364532019706, train_loss: 1.3139983483386424, val_loss: 1.2630442856567834 (12 / 100)
train_acc: 0.42027194066749074, val_acc: 0.46798029556650245, train_loss: 1.342353398484559, val_loss: 1.2313224834761596 (13 / 100)
train_acc: 0.47713226205191595, val_acc: 0.43349753694581283, train_loss: 1.2587778606579831, val_loss: 1.351489027145461 (14 / 100)
train_acc: 0.43510506798516685, val_acc: 0.47783251231527096, train_loss: 1.2680589068215915, val_loss: 1.1903812767836848 (15 / 100)
train_acc: 0.45488257107540175, val_acc: 0.4975369458128079, train_loss: 1.2283088639286452, val_loss: 1.2532448281208282 (16 / 100)
train_acc: 0.4622991347342398, val_acc: 0.46798029556650245, train_loss: 1.2529768053769181, val_loss: 1.1499242215908219 (17 / 100)
train_acc: 0.484548825710754, val_acc: 0.49261083743842365, train_loss: 1.1647934916582332, val_loss: 1.153972263993888 (18 / 100)
train_acc: 0.5129789864029666, val_acc: 0.47783251231527096, train_loss: 1.1726435253440377, val_loss: 1.2287813102083254 (19 / 100)
train_acc: 0.5067985166872683, val_acc: 0.4876847290640394, train_loss: 1.1473690946258337, val_loss: 1.2376993260360116 (20 / 100)
train_acc: 0.5352286773794809, val_acc: 0.5221674876847291, train_loss: 1.063695527419761, val_loss: 1.1802857180534325 (21 / 100)
train_acc: 0.588380716934487, val_acc: 0.45320197044334976, train_loss: 0.9931641114214884, val_loss: 1.3217741905762057 (22 / 100)
train_acc: 0.6069221260815822, val_acc: 0.5270935960591133, train_loss: 0.9560519169523336, val_loss: 1.0753628822970274 (23 / 100)
train_acc: 0.6217552533992583, val_acc: 0.5467980295566502, train_loss: 0.9372130599982659, val_loss: 1.2374291449344803 (24 / 100)
train_acc: 0.6514215080346106, val_acc: 0.5566502463054187, train_loss: 0.8410413058932839, val_loss: 1.6145750348791112 (25 / 100)
train_acc: 0.646477132262052, val_acc: 0.5763546798029556, train_loss: 0.8488426989471662, val_loss: 1.049113079832105 (26 / 100)
train_acc: 0.7144622991347342, val_acc: 0.5172413793103449, train_loss: 0.7043639061919543, val_loss: 1.7804728423433351 (27 / 100)
train_acc: 0.5982694684796045, val_acc: 0.49261083743842365, train_loss: 1.0216522237426418, val_loss: 1.1839633440149242 (28 / 100)
train_acc: 0.7033374536464772, val_acc: 0.5270935960591133, train_loss: 0.7673216042913524, val_loss: 1.2825170309085565 (29 / 100)
train_acc: 0.7058096415327565, val_acc: 0.541871921182266, train_loss: 0.725444245986797, val_loss: 1.372762894395537 (30 / 100)
train_acc: 0.7021013597033374, val_acc: 0.6059113300492611, train_loss: 0.7484681569454255, val_loss: 1.1181254762734099 (31 / 100)
train_acc: 0.7985166872682324, val_acc: 0.6403940886699507, train_loss: 0.5228162775522993, val_loss: 1.321651192721475 (32 / 100)
train_acc: 0.8096415327564895, val_acc: 0.5615763546798029, train_loss: 0.5043821163908365, val_loss: 1.2763285818945598 (33 / 100)
overfit -> train_accuracy 0.8108776266996292, val_accuracy 0.5566502463054187
lr 0.002239481967607394, batch 8, decay 1.1249186713728107e-05, gamma 0.0029540916327290205, val accuracy 0.6403940886699507, val loss 1.321651192721475 [22 / 50]
-------------------------------------
{'lr': 0.0018607318301047905, 'batch_size': 8, 'weight_decay': 1.7502967106330016e-06, 'gamma': 0.09377630828243301}
train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7739100957240987, val_loss: 1.756662139164403 (1 / 100)
train_acc: 0.24351050679851668, val_acc: 0.30049261083743845, train_loss: 1.7464199328452017, val_loss: 1.716278550072844 (2 / 100)
train_acc: 0.28182941903584674, val_acc: 0.3054187192118227, train_loss: 1.6978072359771164, val_loss: 1.6433784063226484 (3 / 100)
train_acc: 0.3127317676143387, val_acc: 0.21674876847290642, train_loss: 1.655786829914239, val_loss: 1.6803130868620473 (4 / 100)
train_acc: 0.2669962917181706, val_acc: 0.32019704433497537, train_loss: 1.7181523108511831, val_loss: 1.5334466595955083 (5 / 100)
train_acc: 0.31396786155747836, val_acc: 0.26108374384236455, train_loss: 1.5633007091998465, val_loss: 1.9543431274996603 (6 / 100)
train_acc: 0.3374536464771323, val_acc: 0.3497536945812808, train_loss: 1.5838222096965398, val_loss: 1.4484135993008542 (7 / 100)
train_acc: 0.3263288009888752, val_acc: 0.3891625615763547, train_loss: 1.4931473083637554, val_loss: 1.3976517774788617 (8 / 100)
train_acc: 0.3757725587144623, val_acc: 0.3694581280788177, train_loss: 1.4876994632819969, val_loss: 1.4202337223907997 (9 / 100)
train_acc: 0.38442521631644005, val_acc: 0.35960591133004927, train_loss: 1.4189163763237236, val_loss: 1.4480963264192854 (10 / 100)
train_acc: 0.4004944375772559, val_acc: 0.3694581280788177, train_loss: 1.400422814749964, val_loss: 1.3408178960161257 (11 / 100)
train_acc: 0.3930778739184178, val_acc: 0.3842364532019704, train_loss: 1.3528827468603297, val_loss: 1.3165760515945886 (12 / 100)
train_acc: 0.39555006180469715, val_acc: 0.39901477832512317, train_loss: 1.3586015518575134, val_loss: 1.3736270630888163 (13 / 100)
train_acc: 0.41285537700865266, val_acc: 0.41379310344827586, train_loss: 1.3593217359927146, val_loss: 1.4527677931809073 (14 / 100)
train_acc: 0.43139678615574784, val_acc: 0.43349753694581283, train_loss: 1.3094863717723983, val_loss: 1.2506533937501203 (15 / 100)
train_acc: 0.42398022249690975, val_acc: 0.46798029556650245, train_loss: 1.3168899671108967, val_loss: 1.2428405320115865 (16 / 100)
train_acc: 0.4647713226205192, val_acc: 0.4088669950738916, train_loss: 1.2585260095938176, val_loss: 1.3515518293004904 (17 / 100)
train_acc: 0.4635352286773795, val_acc: 0.458128078817734, train_loss: 1.2611568183333086, val_loss: 1.2180400182460915 (18 / 100)
train_acc: 0.4721878862793572, val_acc: 0.42857142857142855, train_loss: 1.197046550154244, val_loss: 1.288102051307415 (19 / 100)
train_acc: 0.4857849196538937, val_acc: 0.41379310344827586, train_loss: 1.195851921298448, val_loss: 1.2232464369881917 (20 / 100)
train_acc: 0.5327564894932015, val_acc: 0.43349753694581283, train_loss: 1.1695622117321924, val_loss: 1.3021020879005563 (21 / 100)
train_acc: 0.5018541409147095, val_acc: 0.458128078817734, train_loss: 1.1512449169335761, val_loss: 1.2404845113237504 (22 / 100)
train_acc: 0.5426452410383189, val_acc: 0.458128078817734, train_loss: 1.1156000059526252, val_loss: 1.2799846597492988 (23 / 100)
train_acc: 0.5562422744128553, val_acc: 0.5320197044334976, train_loss: 1.0734286829921311, val_loss: 1.2403099043615933 (24 / 100)
train_acc: 0.5686032138442522, val_acc: 0.49261083743842365, train_loss: 1.0335695475376727, val_loss: 1.2833107962396932 (25 / 100)
train_acc: 0.5970333745364648, val_acc: 0.5517241379310345, train_loss: 1.0033893922645465, val_loss: 1.1816455234447722 (26 / 100)
train_acc: 0.6007416563658838, val_acc: 0.5369458128078818, train_loss: 0.9550479902472573, val_loss: 1.124219010616171 (27 / 100)
train_acc: 0.6143386897404203, val_acc: 0.4482758620689655, train_loss: 0.9251359051326002, val_loss: 1.2852022595006256 (28 / 100)
train_acc: 0.6341161928306551, val_acc: 0.5123152709359606, train_loss: 0.8826388060236154, val_loss: 1.3973736249167343 (29 / 100)
train_acc: 0.6996291718170581, val_acc: 0.5714285714285714, train_loss: 0.8225461054496623, val_loss: 1.1229681742602382 (30 / 100)
train_acc: 0.6687268232385661, val_acc: 0.5024630541871922, train_loss: 0.8176103422167275, val_loss: 1.1615438155940014 (31 / 100)
train_acc: 0.6946847960444994, val_acc: 0.6009852216748769, train_loss: 0.7607254895055839, val_loss: 1.1919369662336528 (32 / 100)
train_acc: 0.7045735475896168, val_acc: 0.5714285714285714, train_loss: 0.7690972978311357, val_loss: 1.263024657524278 (33 / 100)
train_acc: 0.788627935723115, val_acc: 0.5763546798029556, train_loss: 0.5922565423366608, val_loss: 1.269463808078484 (34 / 100)
train_acc: 0.7466007416563659, val_acc: 0.5369458128078818, train_loss: 0.663668982620145, val_loss: 1.2761418009039216 (35 / 100)
train_acc: 0.7861557478368356, val_acc: 0.6206896551724138, train_loss: 0.5495159976709316, val_loss: 1.23720026119002 (36 / 100)
overfit -> train_accuracy 0.8145859085290482, val_accuracy 0.5615763546798029
lr 0.0018607318301047905, batch 8, decay 1.7502967106330016e-06, gamma 0.09377630828243301, val accuracy 0.6206896551724138, val loss 1.23720026119002 [23 / 50]
-------------------------------------
{'lr': 0.0029061979391119385, 'batch_size': 8, 'weight_decay': 2.7332446184140206e-06, 'gamma': 0.06835018971677514}
train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7724593173440808, val_loss: 1.7333686357648501 (1 / 100)
train_acc: 0.2311495673671199, val_acc: 0.3842364532019704, train_loss: 1.718128320755269, val_loss: 1.62633517692829 (2 / 100)
train_acc: 0.31396786155747836, val_acc: 0.270935960591133, train_loss: 1.628379722460828, val_loss: 1.6635114594633356 (3 / 100)
train_acc: 0.30284301606922126, val_acc: 0.3399014778325123, train_loss: 1.638765165331337, val_loss: 1.5594376102456906 (4 / 100)
train_acc: 0.24351050679851668, val_acc: 0.32019704433497537, train_loss: 1.6997682648919274, val_loss: 1.5467775789974945 (5 / 100)
train_acc: 0.34857849196538937, val_acc: 0.3645320197044335, train_loss: 1.5743643883868996, val_loss: 1.548655273291865 (6 / 100)
train_acc: 0.34857849196538937, val_acc: 0.3497536945812808, train_loss: 1.5273778435621037, val_loss: 1.594111285773404 (7 / 100)
train_acc: 0.3374536464771323, val_acc: 0.39408866995073893, train_loss: 1.53763872289245, val_loss: 1.4706358281262402 (8 / 100)
train_acc: 0.35970333745364647, val_acc: 0.3842364532019704, train_loss: 1.4580923600308828, val_loss: 1.4109316394834095 (9 / 100)
train_acc: 0.33868974042027195, val_acc: 0.3694581280788177, train_loss: 1.5112287361630699, val_loss: 1.4057321548461914 (10 / 100)
train_acc: 0.3584672435105068, val_acc: 0.35960591133004927, train_loss: 1.4356784950228056, val_loss: 1.3180064967113176 (11 / 100)
train_acc: 0.38813349814585907, val_acc: 0.39901477832512317, train_loss: 1.3828456684744401, val_loss: 1.3170717543569104 (12 / 100)
train_acc: 0.4103831891223733, val_acc: 0.3891625615763547, train_loss: 1.3503813127652087, val_loss: 1.2636412728596202 (13 / 100)
train_acc: 0.4042027194066749, val_acc: 0.3842364532019704, train_loss: 1.383470010403797, val_loss: 1.3284662210295353 (14 / 100)
train_acc: 0.40173053152039556, val_acc: 0.43842364532019706, train_loss: 1.3207367534837853, val_loss: 1.2853592270113565 (15 / 100)
train_acc: 0.43757725587144625, val_acc: 0.4088669950738916, train_loss: 1.2910371121428954, val_loss: 1.3166535194284223 (16 / 100)
train_acc: 0.45859085290482077, val_acc: 0.46798029556650245, train_loss: 1.2576196137700593, val_loss: 1.2874132177512634 (17 / 100)
train_acc: 0.48331273176761436, val_acc: 0.4876847290640394, train_loss: 1.1936114223395349, val_loss: 1.167209959382494 (18 / 100)
train_acc: 0.4969097651421508, val_acc: 0.49261083743842365, train_loss: 1.1393758369464662, val_loss: 1.4186653480153952 (19 / 100)
train_acc: 0.511742892459827, val_acc: 0.5221674876847291, train_loss: 1.1214041418139218, val_loss: 1.253816915263096 (20 / 100)
train_acc: 0.5512978986402967, val_acc: 0.4729064039408867, train_loss: 1.0656224632145301, val_loss: 1.407286938775349 (21 / 100)
train_acc: 0.5377008652657602, val_acc: 0.5024630541871922, train_loss: 1.108456841358001, val_loss: 1.1264249074635246 (22 / 100)
train_acc: 0.6081582200247219, val_acc: 0.5024630541871922, train_loss: 0.9443049772708172, val_loss: 1.2151514373976608 (23 / 100)
train_acc: 0.6056860321384425, val_acc: 0.541871921182266, train_loss: 0.9108931570324231, val_loss: 1.169631336122898 (24 / 100)
train_acc: 0.6427688504326329, val_acc: 0.49261083743842365, train_loss: 0.8568918363125568, val_loss: 1.358440437046765 (25 / 100)
train_acc: 0.65389369592089, val_acc: 0.4729064039408867, train_loss: 0.8545815632281404, val_loss: 1.4928603407197398 (26 / 100)
train_acc: 0.6674907292954264, val_acc: 0.5320197044334976, train_loss: 0.7757122983743883, val_loss: 1.1900133722521402 (27 / 100)
train_acc: 0.6971569839307787, val_acc: 0.5073891625615764, train_loss: 0.7635658839429706, val_loss: 1.5107533611687534 (28 / 100)
train_acc: 0.7144622991347342, val_acc: 0.5369458128078818, train_loss: 0.726042172228008, val_loss: 1.3866670061214803 (29 / 100)
train_acc: 0.7367119901112484, val_acc: 0.5320197044334976, train_loss: 0.6769762955725709, val_loss: 1.2228100514177032 (30 / 100)
train_acc: 0.7775030902348579, val_acc: 0.5517241379310345, train_loss: 0.5643139286300602, val_loss: 2.2599713326674964 (31 / 100)
train_acc: 0.7849196538936959, val_acc: 0.5812807881773399, train_loss: 0.5433088040027689, val_loss: 1.9687853928270012 (32 / 100)
train_acc: 0.8244746600741656, val_acc: 0.5812807881773399, train_loss: 0.4339686617420984, val_loss: 1.2827061197440612 (33 / 100)
train_acc: 0.7985166872682324, val_acc: 0.5911330049261084, train_loss: 0.5462895469700892, val_loss: 1.297801498121816 (34 / 100)
train_acc: 0.7985166872682324, val_acc: 0.6403940886699507, train_loss: 0.5342895090211602, val_loss: 1.5876015712474953 (35 / 100)
overfit -> train_accuracy 0.861557478368356, val_accuracy 0.6108374384236454
lr 0.0029061979391119385, batch 8, decay 2.7332446184140206e-06, gamma 0.06835018971677514, val accuracy 0.6403940886699507, val loss 1.5876015712474953 [24 / 50]
-------------------------------------
{'lr': 0.0023072781188235797, 'batch_size': 8, 'weight_decay': 7.284167667666521e-06, 'gamma': 0.01160019768286816}
train_acc: 0.15945611866501855, val_acc: 0.18226600985221675, train_loss: 1.7768359160688516, val_loss: 1.7548265973922654 (1 / 100)
train_acc: 0.19530284301606923, val_acc: 0.270935960591133, train_loss: 1.7646917352275588, val_loss: 1.7331449738864242 (2 / 100)
train_acc: 0.25339925834363414, val_acc: 0.24630541871921183, train_loss: 1.7251762944482019, val_loss: 1.6866704630734297 (3 / 100)
train_acc: 0.32014833127317677, val_acc: 0.2857142857142857, train_loss: 1.652484391027387, val_loss: 1.7023068672330508 (4 / 100)
train_acc: 0.3658838071693449, val_acc: 0.33497536945812806, train_loss: 1.550189320619675, val_loss: 1.5840536590867442 (5 / 100)
train_acc: 0.34487021013597036, val_acc: 0.3891625615763547, train_loss: 1.509372023628433, val_loss: 1.5335485283377135 (6 / 100)
train_acc: 0.35970333745364647, val_acc: 0.35467980295566504, train_loss: 1.5494909398487973, val_loss: 1.56706770006659 (7 / 100)
train_acc: 0.3584672435105068, val_acc: 0.3842364532019704, train_loss: 1.4704049113949091, val_loss: 1.3768440819726202 (8 / 100)
train_acc: 0.34857849196538937, val_acc: 0.37438423645320196, train_loss: 1.4691188279424225, val_loss: 1.3285807664758467 (9 / 100)
train_acc: 0.3992583436341162, val_acc: 0.39901477832512317, train_loss: 1.3757032310712176, val_loss: 1.4119666571100358 (10 / 100)
train_acc: 0.4054388133498146, val_acc: 0.4187192118226601, train_loss: 1.3391771080762112, val_loss: 1.2959345781744407 (11 / 100)
train_acc: 0.3943139678615575, val_acc: 0.3448275862068966, train_loss: 1.3651105676799533, val_loss: 2.052969103963504 (12 / 100)
train_acc: 0.40173053152039556, val_acc: 0.43842364532019706, train_loss: 1.376970255478056, val_loss: 1.2808641717938953 (13 / 100)
train_acc: 0.43139678615574784, val_acc: 0.43349753694581283, train_loss: 1.301651781510069, val_loss: 1.3231148895958962 (14 / 100)
train_acc: 0.4289245982694685, val_acc: 0.3645320197044335, train_loss: 1.2788194594778148, val_loss: 1.4305798264559855 (15 / 100)
train_acc: 0.44252163164400493, val_acc: 0.4482758620689655, train_loss: 1.269626598275636, val_loss: 1.2264380860211226 (16 / 100)
train_acc: 0.4338689740420272, val_acc: 0.458128078817734, train_loss: 1.2218859847013381, val_loss: 1.179009774635578 (17 / 100)
train_acc: 0.47095179233621753, val_acc: 0.42857142857142855, train_loss: 1.2108804442826544, val_loss: 1.3194738256520238 (18 / 100)
train_acc: 0.4956736711990111, val_acc: 0.5221674876847291, train_loss: 1.1737235396989933, val_loss: 1.1712502887096312 (19 / 100)
train_acc: 0.5055624227441285, val_acc: 0.5467980295566502, train_loss: 1.1387499799245073, val_loss: 1.1329333359384772 (20 / 100)
train_acc: 0.511742892459827, val_acc: 0.4729064039408867, train_loss: 1.1179655992203796, val_loss: 1.3608992472658017 (21 / 100)
train_acc: 0.5562422744128553, val_acc: 0.4827586206896552, train_loss: 1.0739189213963758, val_loss: 1.1275334775154227 (22 / 100)
train_acc: 0.5871446229913473, val_acc: 0.5123152709359606, train_loss: 0.9892081003106569, val_loss: 1.3045345374516077 (23 / 100)
train_acc: 0.6131025957972805, val_acc: 0.5270935960591133, train_loss: 0.9427914470618382, val_loss: 1.2434231724057878 (24 / 100)
train_acc: 0.6279357231149567, val_acc: 0.5123152709359606, train_loss: 0.8924176975882098, val_loss: 1.1841049602466265 (25 / 100)
train_acc: 0.6341161928306551, val_acc: 0.5812807881773399, train_loss: 0.8803543399084631, val_loss: 1.0039730715046962 (26 / 100)
train_acc: 0.65389369592089, val_acc: 0.6009852216748769, train_loss: 0.8237845284682417, val_loss: 1.0677986379180635 (27 / 100)
train_acc: 0.6427688504326329, val_acc: 0.5517241379310345, train_loss: 0.8625929446980743, val_loss: 1.0513167592692259 (28 / 100)
train_acc: 0.7033374536464772, val_acc: 0.5566502463054187, train_loss: 0.7710136323983059, val_loss: 1.3369192996342194 (29 / 100)
train_acc: 0.7206427688504327, val_acc: 0.5911330049261084, train_loss: 0.7318596038299674, val_loss: 1.1023932724750687 (30 / 100)
train_acc: 0.7700865265760197, val_acc: 0.541871921182266, train_loss: 0.6069452229182711, val_loss: 1.9227309033201245 (31 / 100)
train_acc: 0.8108776266996292, val_acc: 0.6009852216748769, train_loss: 0.4992860169581636, val_loss: 1.215792609259413 (32 / 100)
train_acc: 0.7898640296662547, val_acc: 0.541871921182266, train_loss: 0.5368217988715626, val_loss: 1.843686254153698 (33 / 100)
train_acc: 0.7898640296662547, val_acc: 0.5763546798029556, train_loss: 0.5378821047923179, val_loss: 1.5734340052299312 (34 / 100)
train_acc: 0.823238566131026, val_acc: 0.6009852216748769, train_loss: 0.4628178663689952, val_loss: 1.2440205650963807 (35 / 100)
train_acc: 0.8504326328800988, val_acc: 0.6009852216748769, train_loss: 0.4133203176986449, val_loss: 1.4320084531906203 (36 / 100)
overfit -> train_accuracy 0.830655129789864, val_accuracy 0.5467980295566502
lr 0.0023072781188235797, batch 8, decay 7.284167667666521e-06, gamma 0.01160019768286816, val accuracy 0.6009852216748769, val loss 1.0677986379180635 [25 / 50]
-------------------------------------
{'lr': 0.0035079185962785435, 'batch_size': 8, 'weight_decay': 5.685388156392595e-06, 'gamma': 0.0022738111103517895}
train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7789069275626146, val_loss: 1.764574199474504 (1 / 100)
train_acc: 0.21508034610630408, val_acc: 0.30049261083743845, train_loss: 1.7559687865826639, val_loss: 1.7351238087480292 (2 / 100)
train_acc: 0.2311495673671199, val_acc: 0.3497536945812808, train_loss: 1.7206655762840997, val_loss: 1.6301956217864464 (3 / 100)
train_acc: 0.311495673671199, val_acc: 0.3054187192118227, train_loss: 1.6318047789324936, val_loss: 1.5658517136362387 (4 / 100)
train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7745300809886164, val_loss: 1.769744238242727 (5 / 100)
train_acc: 0.207663782447466, val_acc: 0.2857142857142857, train_loss: 1.7574852809623087, val_loss: 1.6868952489251574 (6 / 100)
train_acc: 0.24351050679851668, val_acc: 0.2660098522167488, train_loss: 1.723187285094385, val_loss: 1.6128216752865043 (7 / 100)
train_acc: 0.2521631644004944, val_acc: 0.270935960591133, train_loss: 1.6787600022163909, val_loss: 1.7396973148355344 (8 / 100)
train_acc: 0.2978986402966625, val_acc: 0.33497536945812806, train_loss: 1.5989914977801007, val_loss: 1.5373792824486794 (9 / 100)
train_acc: 0.3473423980222497, val_acc: 0.3694581280788177, train_loss: 1.4631704862686554, val_loss: 1.4396354470934187 (10 / 100)
train_acc: 0.3572311495673671, val_acc: 0.37438423645320196, train_loss: 1.4757342239834912, val_loss: 1.380815541802956 (11 / 100)
train_acc: 0.35599505562422745, val_acc: 0.3694581280788177, train_loss: 1.435290218725782, val_loss: 1.3496002587191578 (12 / 100)
train_acc: 0.3831891223733004, val_acc: 0.4187192118226601, train_loss: 1.387646059760057, val_loss: 1.3316836474564275 (13 / 100)
train_acc: 0.3930778739184178, val_acc: 0.3694581280788177, train_loss: 1.3698351993254294, val_loss: 1.3074515928775805 (14 / 100)
train_acc: 0.4054388133498146, val_acc: 0.42857142857142855, train_loss: 1.3536481995812453, val_loss: 1.280914823409959 (15 / 100)
train_acc: 0.4252163164400494, val_acc: 0.2955665024630542, train_loss: 1.3520545325850968, val_loss: 1.589407459268429 (16 / 100)
train_acc: 0.40667490729295425, val_acc: 0.4236453201970443, train_loss: 1.3419236117151965, val_loss: 1.2345539031944839 (17 / 100)
train_acc: 0.47342398022249693, val_acc: 0.5024630541871922, train_loss: 1.2070070942783238, val_loss: 1.1838192769459315 (18 / 100)
train_acc: 0.4511742892459827, val_acc: 0.4729064039408867, train_loss: 1.2412311809761416, val_loss: 1.2030376871231154 (19 / 100)
train_acc: 0.4684796044499382, val_acc: 0.46798029556650245, train_loss: 1.1893159544512133, val_loss: 1.1996853087335972 (20 / 100)
train_acc: 0.4746600741656366, val_acc: 0.4482758620689655, train_loss: 1.1755101545190045, val_loss: 1.2303248843536 (21 / 100)
train_acc: 0.5278121137206427, val_acc: 0.46798029556650245, train_loss: 1.1123901831057517, val_loss: 1.220387641725869 (22 / 100)
train_acc: 0.5834363411619283, val_acc: 0.47783251231527096, train_loss: 1.0563054093619064, val_loss: 1.1622239087015538 (23 / 100)
train_acc: 0.453646477132262, val_acc: 0.43349753694581283, train_loss: 1.3174339728832833, val_loss: 1.2896803564626007 (24 / 100)
train_acc: 0.5525339925834364, val_acc: 0.5320197044334976, train_loss: 1.0557203555136587, val_loss: 1.2556604558023914 (25 / 100)
train_acc: 0.5611866501854141, val_acc: 0.5024630541871922, train_loss: 1.0242469045523512, val_loss: 1.1689078056166324 (26 / 100)
train_acc: 0.6242274412855378, val_acc: 0.5073891625615764, train_loss: 0.897352307334966, val_loss: 1.2076455812736098 (27 / 100)
train_acc: 0.6328800988875154, val_acc: 0.5615763546798029, train_loss: 0.8589702558753223, val_loss: 1.3113876540085365 (28 / 100)
train_acc: 0.6316440049443758, val_acc: 0.5467980295566502, train_loss: 0.8927673214887659, val_loss: 1.0918800008708034 (29 / 100)
train_acc: 0.6749072929542645, val_acc: 0.5221674876847291, train_loss: 0.8138337085214622, val_loss: 1.1312058852811164 (30 / 100)
train_acc: 0.7058096415327565, val_acc: 0.4975369458128079, train_loss: 0.7143559794785508, val_loss: 1.320732008647449 (31 / 100)
train_acc: 0.7169344870210136, val_acc: 0.5812807881773399, train_loss: 0.7105614156157184, val_loss: 1.3705782090208214 (32 / 100)
train_acc: 0.7243510506798516, val_acc: 0.5812807881773399, train_loss: 0.7618082492106925, val_loss: 1.0803721321040187 (33 / 100)
train_acc: 0.7750309023485785, val_acc: 0.5467980295566502, train_loss: 0.5866709191660651, val_loss: 2.214858304984464 (34 / 100)
train_acc: 0.7861557478368356, val_acc: 0.5862068965517241, train_loss: 0.5739294993420614, val_loss: 1.437114764316916 (35 / 100)
overfit -> train_accuracy 0.8084054388133498, val_accuracy 0.5467980295566502
lr 0.0035079185962785435, batch 8, decay 5.685388156392595e-06, gamma 0.0022738111103517895, val accuracy 0.5862068965517241, val loss 1.437114764316916 [26 / 50]
-------------------------------------
{'lr': 0.0025108773912956702, 'batch_size': 8, 'weight_decay': 2.6449675595585823e-05, 'gamma': 0.0029772463579610525}
train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7721511300915693, val_loss: 1.7445795119102365 (1 / 100)
train_acc: 0.22867737948084055, val_acc: 0.30049261083743845, train_loss: 1.7353641111564873, val_loss: 1.6800237964526774 (2 / 100)
train_acc: 0.2880098887515451, val_acc: 0.3251231527093596, train_loss: 1.6704204747055016, val_loss: 1.5991218236866842 (3 / 100)
train_acc: 0.28182941903584674, val_acc: 0.3793103448275862, train_loss: 1.6566997006148727, val_loss: 1.5119047035724658 (4 / 100)
train_acc: 0.3337453646477132, val_acc: 0.3448275862068966, train_loss: 1.5679452176147102, val_loss: 1.5072807284998777 (5 / 100)
train_acc: 0.33250927070457353, val_acc: 0.3251231527093596, train_loss: 1.6068736496018834, val_loss: 1.4845648868917831 (6 / 100)
train_acc: 0.34610630407911, val_acc: 0.3694581280788177, train_loss: 1.5341086134185602, val_loss: 1.445332958780486 (7 / 100)
train_acc: 0.36341161928306553, val_acc: 0.3399014778325123, train_loss: 1.4646449065473672, val_loss: 1.418408327501983 (8 / 100)
train_acc: 0.3572311495673671, val_acc: 0.31527093596059114, train_loss: 1.4367049129401208, val_loss: 1.7649051943436045 (9 / 100)
train_acc: 0.3374536464771323, val_acc: 0.4187192118226601, train_loss: 1.4500503333743628, val_loss: 1.3957021007396904 (10 / 100)
train_acc: 0.38936959208899874, val_acc: 0.4088669950738916, train_loss: 1.3929940686090325, val_loss: 1.2930242780394154 (11 / 100)
train_acc: 0.3856613102595797, val_acc: 0.39408866995073893, train_loss: 1.3624251469398754, val_loss: 1.2531027811501414 (12 / 100)
train_acc: 0.4103831891223733, val_acc: 0.3842364532019704, train_loss: 1.3032577273577488, val_loss: 1.469181581083777 (13 / 100)
train_acc: 0.411619283065513, val_acc: 0.458128078817734, train_loss: 1.293190688815783, val_loss: 1.2113596106984932 (14 / 100)
train_acc: 0.4388133498145859, val_acc: 0.5024630541871922, train_loss: 1.26046054487028, val_loss: 1.2110509029750167 (15 / 100)
train_acc: 0.449938195302843, val_acc: 0.43842364532019706, train_loss: 1.2599065224231394, val_loss: 1.2213087093653938 (16 / 100)
train_acc: 0.4969097651421508, val_acc: 0.4729064039408867, train_loss: 1.1969147521279504, val_loss: 1.2434468286965281 (17 / 100)
train_acc: 0.48331273176761436, val_acc: 0.43349753694581283, train_loss: 1.1499874255860545, val_loss: 1.374954334033534 (18 / 100)
train_acc: 0.46600741656365885, val_acc: 0.5073891625615764, train_loss: 1.1655911472436082, val_loss: 1.18171253286559 (19 / 100)
train_acc: 0.5166872682323856, val_acc: 0.46798029556650245, train_loss: 1.1218756757059558, val_loss: 1.2556255847362463 (20 / 100)
train_acc: 0.5587144622991347, val_acc: 0.5073891625615764, train_loss: 1.1197600741910994, val_loss: 1.1011500446667224 (21 / 100)
train_acc: 0.5500618046971569, val_acc: 0.5073891625615764, train_loss: 1.0266094268178763, val_loss: 1.0541836968783675 (22 / 100)
train_acc: 0.584672435105068, val_acc: 0.5073891625615764, train_loss: 0.9469535761916887, val_loss: 1.3052393479887487 (23 / 100)
train_acc: 0.6007416563658838, val_acc: 0.5369458128078818, train_loss: 0.9451474029437279, val_loss: 1.1300956374905966 (24 / 100)
train_acc: 0.6205191594561187, val_acc: 0.4975369458128079, train_loss: 0.916015000808961, val_loss: 1.2766281900734737 (25 / 100)
train_acc: 0.6674907292954264, val_acc: 0.5369458128078818, train_loss: 0.8561397360635482, val_loss: 1.1398319973440594 (26 / 100)
train_acc: 0.6674907292954264, val_acc: 0.458128078817734, train_loss: 0.8378391481005807, val_loss: 2.966650206467201 (27 / 100)
train_acc: 0.630407911001236, val_acc: 0.5517241379310345, train_loss: 0.9175862293455598, val_loss: 1.2179491537545115 (28 / 100)
train_acc: 0.7119901112484549, val_acc: 0.5221674876847291, train_loss: 0.7173220427575601, val_loss: 1.611422881704246 (29 / 100)
train_acc: 0.7601977750309024, val_acc: 0.541871921182266, train_loss: 0.618654829315261, val_loss: 1.53024983934581 (30 / 100)
train_acc: 0.73053152039555, val_acc: 0.5172413793103449, train_loss: 0.6599938204910316, val_loss: 1.5666146843597806 (31 / 100)
train_acc: 0.73053152039555, val_acc: 0.5665024630541872, train_loss: 0.6342955835080707, val_loss: 1.2265549724912408 (32 / 100)
train_acc: 0.8145859085290482, val_acc: 0.5763546798029556, train_loss: 0.46860835107206855, val_loss: 1.581136313565259 (33 / 100)
train_acc: 0.8195302843016069, val_acc: 0.5862068965517241, train_loss: 0.43740767233156597, val_loss: 2.195527244671225 (34 / 100)
train_acc: 0.7812113720642769, val_acc: 0.5763546798029556, train_loss: 0.6020514129560869, val_loss: 1.3509970551053878 (35 / 100)
train_acc: 0.7898640296662547, val_acc: 0.5566502463054187, train_loss: 0.5849263682795691, val_loss: 1.8188588454805572 (36 / 100)
overfit -> train_accuracy 0.8454882571075402, val_accuracy 0.5763546798029556
lr 0.0025108773912956702, batch 8, decay 2.6449675595585823e-05, gamma 0.0029772463579610525, val accuracy 0.5862068965517241, val loss 2.195527244671225 [27 / 50]
-------------------------------------
{'lr': 0.0011230697132414082, 'batch_size': 8, 'weight_decay': 6.876236641162592e-05, 'gamma': 0.0013061543344017107}
train_acc: 0.1915945611866502, val_acc: 0.2413793103448276, train_loss: 1.780601174928644, val_loss: 1.7599981471235528 (1 / 100)
train_acc: 0.1903584672435105, val_acc: 0.19704433497536947, train_loss: 1.7571258519871715, val_loss: 1.7410241764754497 (2 / 100)
train_acc: 0.22249690976514216, val_acc: 0.2512315270935961, train_loss: 1.7414323932897617, val_loss: 1.7122327852718935 (3 / 100)
train_acc: 0.26328800988875156, val_acc: 0.3399014778325123, train_loss: 1.6954257841900047, val_loss: 1.5813914566791702 (4 / 100)
train_acc: 0.34363411619283063, val_acc: 0.3891625615763547, train_loss: 1.587024336692282, val_loss: 1.5316419877442233 (5 / 100)
train_acc: 0.34981458590852904, val_acc: 0.3251231527093596, train_loss: 1.5745984631209498, val_loss: 1.518775848332297 (6 / 100)
train_acc: 0.34363411619283063, val_acc: 0.3694581280788177, train_loss: 1.5029979368075452, val_loss: 1.4453341127029193 (7 / 100)
train_acc: 0.37824474660074164, val_acc: 0.43349753694581283, train_loss: 1.479961402336658, val_loss: 1.3624717108721804 (8 / 100)
train_acc: 0.377008652657602, val_acc: 0.4039408866995074, train_loss: 1.4409609340472038, val_loss: 1.3913356893755533 (9 / 100)
train_acc: 0.4004944375772559, val_acc: 0.4088669950738916, train_loss: 1.404440481082176, val_loss: 1.3022846359337492 (10 / 100)
train_acc: 0.4042027194066749, val_acc: 0.3842364532019704, train_loss: 1.391217356706579, val_loss: 1.4283153975538432 (11 / 100)
train_acc: 0.40296662546353523, val_acc: 0.4187192118226601, train_loss: 1.3749798602609906, val_loss: 1.2727631559512886 (12 / 100)
train_acc: 0.4177997527812114, val_acc: 0.41379310344827586, train_loss: 1.3552324279719141, val_loss: 1.2766743886646965 (13 / 100)
train_acc: 0.4276885043263288, val_acc: 0.4630541871921182, train_loss: 1.309321436805395, val_loss: 1.2355686837229236 (14 / 100)
train_acc: 0.411619283065513, val_acc: 0.43842364532019706, train_loss: 1.292285181535926, val_loss: 1.2435247698440928 (15 / 100)
train_acc: 0.4388133498145859, val_acc: 0.3891625615763547, train_loss: 1.2877236589955163, val_loss: 1.349356521526581 (16 / 100)
train_acc: 0.4561186650185414, val_acc: 0.458128078817734, train_loss: 1.2814552669030037, val_loss: 1.2022914322726246 (17 / 100)
train_acc: 0.4177997527812114, val_acc: 0.4630541871921182, train_loss: 1.3409649845401495, val_loss: 1.2533402645529197 (18 / 100)
train_acc: 0.4610630407911001, val_acc: 0.4433497536945813, train_loss: 1.2598683607150951, val_loss: 1.2209043626127571 (19 / 100)
train_acc: 0.46600741656365885, val_acc: 0.4433497536945813, train_loss: 1.2297551290066486, val_loss: 1.2716319196917154 (20 / 100)
train_acc: 0.48702101359703337, val_acc: 0.5024630541871922, train_loss: 1.1711686933585828, val_loss: 1.1861876360888552 (21 / 100)
train_acc: 0.46600741656365885, val_acc: 0.46798029556650245, train_loss: 1.1682531306121788, val_loss: 1.2251795007677502 (22 / 100)
train_acc: 0.5080346106304079, val_acc: 0.5270935960591133, train_loss: 1.15873435165443, val_loss: 1.0725015942099059 (23 / 100)
train_acc: 0.5216316440049443, val_acc: 0.5024630541871922, train_loss: 1.108327594764153, val_loss: 1.1510163563225657 (24 / 100)
train_acc: 0.5302843016069221, val_acc: 0.5270935960591133, train_loss: 1.1065113046407995, val_loss: 1.1314583649776253 (25 / 100)
train_acc: 0.5500618046971569, val_acc: 0.49261083743842365, train_loss: 1.103091614502764, val_loss: 1.099743732090654 (26 / 100)
train_acc: 0.5525339925834364, val_acc: 0.5517241379310345, train_loss: 1.030926421781995, val_loss: 1.1638884379945953 (27 / 100)
train_acc: 0.5772558714462299, val_acc: 0.5123152709359606, train_loss: 1.0105969095406928, val_loss: 1.178521493972816 (28 / 100)
train_acc: 0.5871446229913473, val_acc: 0.5320197044334976, train_loss: 0.9718693017370179, val_loss: 1.2020329750817398 (29 / 100)
train_acc: 0.6056860321384425, val_acc: 0.5221674876847291, train_loss: 0.9340484961591043, val_loss: 1.1680756255323663 (30 / 100)
train_acc: 0.6365883807169345, val_acc: 0.5763546798029556, train_loss: 0.9012337150915591, val_loss: 1.0825748947159997 (31 / 100)
train_acc: 0.630407911001236, val_acc: 0.5665024630541872, train_loss: 0.8661351693133341, val_loss: 1.0817362316723527 (32 / 100)
train_acc: 0.553770086526576, val_acc: 0.5123152709359606, train_loss: 1.1009947438764631, val_loss: 1.1983238640677165 (33 / 100)
train_acc: 0.6625463535228677, val_acc: 0.5123152709359606, train_loss: 0.8603289999684828, val_loss: 1.1607668737472572 (34 / 100)
train_acc: 0.6971569839307787, val_acc: 0.5517241379310345, train_loss: 0.8312462994135796, val_loss: 0.9804274210788934 (35 / 100)
train_acc: 0.7441285537700866, val_acc: 0.5517241379310345, train_loss: 0.680736410603093, val_loss: 1.4402880052040363 (36 / 100)
train_acc: 0.7688504326328801, val_acc: 0.5812807881773399, train_loss: 0.6193202826826475, val_loss: 0.9918821794646127 (37 / 100)
train_acc: 0.7849196538936959, val_acc: 0.6157635467980296, train_loss: 0.5880584122960736, val_loss: 1.0883627677785939 (38 / 100)
train_acc: 0.7206427688504327, val_acc: 0.5270935960591133, train_loss: 0.665145217119837, val_loss: 1.4388927617683787 (39 / 100)
train_acc: 0.788627935723115, val_acc: 0.625615763546798, train_loss: 0.5365241396117711, val_loss: 0.9908043369283817 (40 / 100)
train_acc: 0.823238566131026, val_acc: 0.5960591133004927, train_loss: 0.4903018462790547, val_loss: 1.2677586824435907 (41 / 100)
train_acc: 0.8454882571075402, val_acc: 0.625615763546798, train_loss: 0.4349713894286468, val_loss: 1.1386058999987072 (42 / 100)
train_acc: 0.7589616810877626, val_acc: 0.5467980295566502, train_loss: 0.6121350005176954, val_loss: 1.3708340922012705 (43 / 100)
train_acc: 0.799752781211372, val_acc: 0.6305418719211823, train_loss: 0.5313982291628315, val_loss: 1.188432938946879 (44 / 100)
overfit -> train_accuracy 0.8763906056860321, val_accuracy 0.6157635467980296
lr 0.0011230697132414082, batch 8, decay 6.876236641162592e-05, gamma 0.0013061543344017107, val accuracy 0.6305418719211823, val loss 1.188432938946879 [28 / 50]
-------------------------------------
{'lr': 0.002810721124196567, 'batch_size': 8, 'weight_decay': 1.5952910114731925e-06, 'gamma': 0.031229574205693476}
train_acc: 0.1841779975278121, val_acc: 0.23645320197044334, train_loss: 1.7736322156283706, val_loss: 1.7493092062438063 (1 / 100)
train_acc: 0.2323856613102596, val_acc: 0.19704433497536947, train_loss: 1.7460698481690604, val_loss: 1.704850966707239 (2 / 100)
train_acc: 0.276885043263288, val_acc: 0.33497536945812806, train_loss: 1.6719996045045415, val_loss: 1.6002097029991338 (3 / 100)
train_acc: 0.3053152039555006, val_acc: 0.30049261083743845, train_loss: 1.6247756743460562, val_loss: 1.6239364070845355 (4 / 100)
train_acc: 0.29171817058096416, val_acc: 0.30049261083743845, train_loss: 1.6737023634727866, val_loss: 1.6083707551063575 (5 / 100)
train_acc: 0.3337453646477132, val_acc: 0.37438423645320196, train_loss: 1.5671307781276067, val_loss: 1.4157556175011132 (6 / 100)
train_acc: 0.3263288009888752, val_acc: 0.2955665024630542, train_loss: 1.5206579963120601, val_loss: 1.551535383821121 (7 / 100)
train_acc: 0.36711990111248455, val_acc: 0.3694581280788177, train_loss: 1.450901416529831, val_loss: 1.3925152512019492 (8 / 100)
train_acc: 0.34857849196538937, val_acc: 0.3694581280788177, train_loss: 1.4387141519188145, val_loss: 1.4564336908274684 (9 / 100)
train_acc: 0.38936959208899874, val_acc: 0.4236453201970443, train_loss: 1.437618048435678, val_loss: 1.3374366636933952 (10 / 100)
train_acc: 0.3930778739184178, val_acc: 0.41379310344827586, train_loss: 1.3740236679614695, val_loss: 1.462115119830728 (11 / 100)
train_acc: 0.3980222496909765, val_acc: 0.3694581280788177, train_loss: 1.3488264770531389, val_loss: 1.3840846680655268 (12 / 100)
train_acc: 0.41903584672435107, val_acc: 0.39901477832512317, train_loss: 1.3276239008779727, val_loss: 1.4560800602870623 (13 / 100)
train_acc: 0.44128553770086526, val_acc: 0.35960591133004927, train_loss: 1.2892667389623313, val_loss: 1.468719789547286 (14 / 100)
train_acc: 0.4054388133498146, val_acc: 0.43349753694581283, train_loss: 1.3174137211553245, val_loss: 1.286850679684155 (15 / 100)
train_acc: 0.453646477132262, val_acc: 0.4630541871921182, train_loss: 1.2374647731981407, val_loss: 1.3789824520425844 (16 / 100)
train_acc: 0.4635352286773795, val_acc: 0.458128078817734, train_loss: 1.249988094985264, val_loss: 1.5080437983198118 (17 / 100)
train_acc: 0.5006180469715699, val_acc: 0.4630541871921182, train_loss: 1.185962899653667, val_loss: 1.1667210762136675 (18 / 100)
train_acc: 0.4758961681087763, val_acc: 0.46798029556650245, train_loss: 1.1752881470363132, val_loss: 1.2625848329419573 (19 / 100)
train_acc: 0.5129789864029666, val_acc: 0.458128078817734, train_loss: 1.1589262323414882, val_loss: 1.263379683635505 (20 / 100)
train_acc: 0.5562422744128553, val_acc: 0.4433497536945813, train_loss: 1.1065873750502748, val_loss: 1.2985853649712549 (21 / 100)
train_acc: 0.5995055624227441, val_acc: 0.4975369458128079, train_loss: 1.001440333200179, val_loss: 1.3766123397009713 (22 / 100)
train_acc: 0.588380716934487, val_acc: 0.541871921182266, train_loss: 1.0177188774269208, val_loss: 1.2527888395956583 (23 / 100)
train_acc: 0.6081582200247219, val_acc: 0.5073891625615764, train_loss: 0.930257657549732, val_loss: 1.1609079615382725 (24 / 100)
train_acc: 0.6526576019777504, val_acc: 0.5172413793103449, train_loss: 0.850367421125452, val_loss: 1.276600499458501 (25 / 100)
train_acc: 0.6588380716934487, val_acc: 0.5073891625615764, train_loss: 0.7897051580167377, val_loss: 1.2004812703344034 (26 / 100)
train_acc: 0.6328800988875154, val_acc: 0.5467980295566502, train_loss: 0.9570895284598484, val_loss: 1.1442816163518745 (27 / 100)
train_acc: 0.7268232385661311, val_acc: 0.4975369458128079, train_loss: 0.7086417888681438, val_loss: 1.3896057015569339 (28 / 100)
train_acc: 0.7466007416563659, val_acc: 0.5320197044334976, train_loss: 0.6770254092104503, val_loss: 1.4742763300834618 (29 / 100)
train_acc: 0.7589616810877626, val_acc: 0.5812807881773399, train_loss: 0.6070078937025978, val_loss: 1.5290578480791577 (30 / 100)
train_acc: 0.7948084054388134, val_acc: 0.5517241379310345, train_loss: 0.5292767303983125, val_loss: 1.71521067266981 (31 / 100)
train_acc: 0.7342398022249691, val_acc: 0.5566502463054187, train_loss: 0.67958457007249, val_loss: 1.2846663870247714 (32 / 100)
train_acc: 0.823238566131026, val_acc: 0.6009852216748769, train_loss: 0.4684014902421364, val_loss: 1.4769244241009791 (33 / 100)
train_acc: 0.823238566131026, val_acc: 0.5960591133004927, train_loss: 0.46872311675799055, val_loss: 1.243909595047899 (34 / 100)
overfit -> train_accuracy 0.8788627935723115, val_accuracy 0.6157635467980296
lr 0.002810721124196567, batch 8, decay 1.5952910114731925e-06, gamma 0.031229574205693476, val accuracy 0.6157635467980296, val loss 1.7045065663718237 [29 / 50]
-------------------------------------
{'lr': 0.0009508825967508436, 'batch_size': 8, 'weight_decay': 1.3797496581449174e-05, 'gamma': 0.036120032191297086}
train_acc: 0.1619283065512979, val_acc: 0.18226600985221675, train_loss: 1.781864113949138, val_loss: 1.7589624767820236 (1 / 100)
train_acc: 0.1965389369592089, val_acc: 0.270935960591133, train_loss: 1.759624323827227, val_loss: 1.7404160945873541 (2 / 100)
train_acc: 0.2200247218788628, val_acc: 0.1921182266009852, train_loss: 1.754249202158896, val_loss: 1.7289161494212786 (3 / 100)
train_acc: 0.26081582200247216, val_acc: 0.3399014778325123, train_loss: 1.716293647056457, val_loss: 1.6204250598775929 (4 / 100)
train_acc: 0.3127317676143387, val_acc: 0.31527093596059114, train_loss: 1.6536123631763813, val_loss: 1.6705643608065075 (5 / 100)
train_acc: 0.32014833127317677, val_acc: 0.3497536945812808, train_loss: 1.6019831181161925, val_loss: 1.5361537504665956 (6 / 100)
train_acc: 0.3374536464771323, val_acc: 0.3645320197044335, train_loss: 1.5585837468818033, val_loss: 1.442990404044466 (7 / 100)
train_acc: 0.37082818294190356, val_acc: 0.35960591133004927, train_loss: 1.5157278617026633, val_loss: 1.3911107385099815 (8 / 100)
train_acc: 0.3621755253399258, val_acc: 0.43842364532019706, train_loss: 1.478961429430911, val_loss: 1.4288324440641356 (9 / 100)
train_acc: 0.36711990111248455, val_acc: 0.3793103448275862, train_loss: 1.4373016926207856, val_loss: 1.4333670297866972 (10 / 100)
train_acc: 0.315203955500618, val_acc: 0.33497536945812806, train_loss: 1.5890693437625805, val_loss: 1.468652308280832 (11 / 100)
train_acc: 0.37330037082818296, val_acc: 0.4088669950738916, train_loss: 1.4852028850866632, val_loss: 1.4573635966906995 (12 / 100)
train_acc: 0.3856613102595797, val_acc: 0.3842364532019704, train_loss: 1.4166843982503206, val_loss: 1.418854081571983 (13 / 100)
train_acc: 0.3868974042027194, val_acc: 0.39408866995073893, train_loss: 1.402106260339763, val_loss: 1.3416198627114884 (14 / 100)
train_acc: 0.40173053152039556, val_acc: 0.4236453201970443, train_loss: 1.3623870189758698, val_loss: 1.3089972229426718 (15 / 100)
train_acc: 0.4276885043263288, val_acc: 0.458128078817734, train_loss: 1.336798009972637, val_loss: 1.2991627155266372 (16 / 100)
train_acc: 0.4326328800988875, val_acc: 0.43349753694581283, train_loss: 1.3267810144294767, val_loss: 1.3518719103535994 (17 / 100)
train_acc: 0.4103831891223733, val_acc: 0.458128078817734, train_loss: 1.3543075352575635, val_loss: 1.274064039361888 (18 / 100)
train_acc: 0.4363411619283066, val_acc: 0.4236453201970443, train_loss: 1.2665077190021945, val_loss: 1.2577732184837604 (19 / 100)
train_acc: 0.45488257107540175, val_acc: 0.4827586206896552, train_loss: 1.2418702235180603, val_loss: 1.2045756596062571 (20 / 100)
train_acc: 0.4635352286773795, val_acc: 0.46798029556650245, train_loss: 1.2650912619933798, val_loss: 1.1935991865073519 (21 / 100)
train_acc: 0.48825710754017304, val_acc: 0.4482758620689655, train_loss: 1.2401033732740783, val_loss: 1.224204240761367 (22 / 100)
train_acc: 0.48825710754017304, val_acc: 0.43842364532019706, train_loss: 1.19390144365827, val_loss: 1.1982240671007505 (23 / 100)
train_acc: 0.49938195302843014, val_acc: 0.4827586206896552, train_loss: 1.1900386875138442, val_loss: 1.1727215503824169 (24 / 100)
train_acc: 0.5203955500618047, val_acc: 0.4975369458128079, train_loss: 1.1331769662675515, val_loss: 1.1725253092831578 (25 / 100)
train_acc: 0.5142150803461063, val_acc: 0.5369458128078818, train_loss: 1.1441521765422467, val_loss: 1.1172201733283809 (26 / 100)
train_acc: 0.5500618046971569, val_acc: 0.4876847290640394, train_loss: 1.0893191330512464, val_loss: 1.1962883296271263 (27 / 100)
train_acc: 0.5475896168108776, val_acc: 0.5123152709359606, train_loss: 1.0841944096851703, val_loss: 1.145369449823067 (28 / 100)
train_acc: 0.5512978986402967, val_acc: 0.5517241379310345, train_loss: 1.062163628223358, val_loss: 1.064674987581563 (29 / 100)
train_acc: 0.595797280593325, val_acc: 0.5763546798029556, train_loss: 0.9825177970716479, val_loss: 1.0264245891218702 (30 / 100)
train_acc: 0.5995055624227441, val_acc: 0.5073891625615764, train_loss: 1.0008013967530542, val_loss: 1.2024767566197023 (31 / 100)
train_acc: 0.630407911001236, val_acc: 0.5073891625615764, train_loss: 0.9359703413193541, val_loss: 1.0821401544392402 (32 / 100)
train_acc: 0.6477132262051916, val_acc: 0.5566502463054187, train_loss: 0.9057285187418292, val_loss: 1.089969476749157 (33 / 100)
train_acc: 0.6514215080346106, val_acc: 0.5615763546798029, train_loss: 0.8430195358686601, val_loss: 1.1566067000328026 (34 / 100)
train_acc: 0.6934487021013597, val_acc: 0.5665024630541872, train_loss: 0.7741972451156974, val_loss: 1.1606165696247457 (35 / 100)
train_acc: 0.6872682323856613, val_acc: 0.6108374384236454, train_loss: 0.7673873591924038, val_loss: 0.9294878589402279 (36 / 100)
train_acc: 0.757725587144623, val_acc: 0.5665024630541872, train_loss: 0.6412065740863532, val_loss: 1.1277177844728743 (37 / 100)
train_acc: 0.7601977750309024, val_acc: 0.6108374384236454, train_loss: 0.6339699909625154, val_loss: 0.9588682369645594 (38 / 100)
train_acc: 0.792336217552534, val_acc: 0.5467980295566502, train_loss: 0.5346269404195589, val_loss: 1.2747649359585616 (39 / 100)
train_acc: 0.8430160692212608, val_acc: 0.6305418719211823, train_loss: 0.43500381877602107, val_loss: 1.332298170756824 (40 / 100)
train_acc: 0.8084054388133498, val_acc: 0.5812807881773399, train_loss: 0.461229841405588, val_loss: 1.1479333034289882 (41 / 100)
train_acc: 0.830655129789864, val_acc: 0.5911330049261084, train_loss: 0.4666678021363776, val_loss: 1.2600511089334347 (42 / 100)
train_acc: 0.7935723114956736, val_acc: 0.5517241379310345, train_loss: 0.5214921230144053, val_loss: 1.1563951100034666 (43 / 100)
train_acc: 0.823238566131026, val_acc: 0.5812807881773399, train_loss: 0.4857614668102288, val_loss: 1.1774621291700842 (44 / 100)
overfit -> train_accuracy 0.8343634116192831, val_accuracy 0.5172413793103449
lr 0.0009508825967508436, batch 8, decay 1.3797496581449174e-05, gamma 0.036120032191297086, val accuracy 0.6305418719211823, val loss 1.332298170756824 [30 / 50]
-------------------------------------
{'lr': 0.003993721796611854, 'batch_size': 8, 'weight_decay': 6.384992308014864e-06, 'gamma': 0.01646644227119102}
train_acc: 0.16934487021013597, val_acc: 0.18226600985221675, train_loss: 1.7773343418967738, val_loss: 1.757788545979655 (1 / 100)
train_acc: 0.22126081582200247, val_acc: 0.29064039408866993, train_loss: 1.7562532485341849, val_loss: 1.7037626152555343 (2 / 100)
train_acc: 0.2657601977750309, val_acc: 0.3497536945812808, train_loss: 1.6935711917830043, val_loss: 1.5694205561294932 (3 / 100)
train_acc: 0.2249690976514215, val_acc: 0.21674876847290642, train_loss: 1.7505938936075558, val_loss: 1.7334262919543413 (4 / 100)
train_acc: 0.2595797280593325, val_acc: 0.29064039408866993, train_loss: 1.7202907423448504, val_loss: 1.6773274520347858 (5 / 100)
train_acc: 0.30902348578491967, val_acc: 0.3251231527093596, train_loss: 1.6292340065846485, val_loss: 1.5751112135760303 (6 / 100)
train_acc: 0.3065512978986403, val_acc: 0.3891625615763547, train_loss: 1.5864470379314257, val_loss: 1.5192026275719328 (7 / 100)
train_acc: 0.33250927070457353, val_acc: 0.39901477832512317, train_loss: 1.5465317550489428, val_loss: 1.3847259500343811 (8 / 100)
train_acc: 0.33498145859085293, val_acc: 0.3448275862068966, train_loss: 1.5367532902211871, val_loss: 1.399384277207511 (9 / 100)
train_acc: 0.3757725587144623, val_acc: 0.35960591133004927, train_loss: 1.4482202335400105, val_loss: 1.4026338772233484 (10 / 100)
train_acc: 0.37453646477132263, val_acc: 0.37438423645320196, train_loss: 1.4305173163066247, val_loss: 1.441780612973744 (11 / 100)
train_acc: 0.4004944375772559, val_acc: 0.41379310344827586, train_loss: 1.3877492438730115, val_loss: 1.3404999416449974 (12 / 100)
train_acc: 0.41903584672435107, val_acc: 0.4088669950738916, train_loss: 1.3534382491824948, val_loss: 1.4705587590269267 (13 / 100)
train_acc: 0.40914709517923364, val_acc: 0.35467980295566504, train_loss: 1.3599770266576514, val_loss: 1.4444876486444709 (14 / 100)
train_acc: 0.39555006180469715, val_acc: 0.37438423645320196, train_loss: 1.340909650210839, val_loss: 1.3051633394410458 (15 / 100)
train_acc: 0.4338689740420272, val_acc: 0.43842364532019706, train_loss: 1.2862306733066573, val_loss: 1.253230606981099 (16 / 100)
train_acc: 0.45488257107540175, val_acc: 0.47783251231527096, train_loss: 1.2516583093754, val_loss: 1.4525698605429362 (17 / 100)
train_acc: 0.4758961681087763, val_acc: 0.5123152709359606, train_loss: 1.2358478544963747, val_loss: 1.1830708123193 (18 / 100)
train_acc: 0.5241038318912238, val_acc: 0.5073891625615764, train_loss: 1.1376069096021806, val_loss: 1.3588648394410834 (19 / 100)
train_acc: 0.5686032138442522, val_acc: 0.5123152709359606, train_loss: 1.0714956816695678, val_loss: 1.1671583182706033 (20 / 100)
train_acc: 0.6007416563658838, val_acc: 0.5270935960591133, train_loss: 1.0223081615858232, val_loss: 1.172164631594578 (21 / 100)
train_acc: 0.6180469715698393, val_acc: 0.46798029556650245, train_loss: 0.9281010601222883, val_loss: 1.3738498423487095 (22 / 100)
train_acc: 0.6415327564894932, val_acc: 0.5073891625615764, train_loss: 0.9045623674970328, val_loss: 1.1892235202742327 (23 / 100)
train_acc: 0.646477132262052, val_acc: 0.5172413793103449, train_loss: 0.8691253451393326, val_loss: 1.1854921159485878 (24 / 100)
train_acc: 0.7021013597033374, val_acc: 0.5714285714285714, train_loss: 0.7542121519411745, val_loss: 1.2423115034995995 (25 / 100)
train_acc: 0.723114956736712, val_acc: 0.5024630541871922, train_loss: 0.7146939015653726, val_loss: 1.4040465636793615 (26 / 100)
overfit -> train_accuracy 0.7700865265760197, val_accuracy 0.5073891625615764
lr 0.003993721796611854, batch 8, decay 6.384992308014864e-06, gamma 0.01646644227119102, val accuracy 0.5714285714285714, val loss 1.2423115034995995 [31 / 50]
-------------------------------------
{'lr': 0.003655275098687901, 'batch_size': 8, 'weight_decay': 7.451897411905578e-05, 'gamma': 0.018906581126934444}
train_acc: 0.18046971569839307, val_acc: 0.1921182266009852, train_loss: 1.7743258292064974, val_loss: 1.7384046245678304 (1 / 100)
train_acc: 0.2484548825710754, val_acc: 0.26108374384236455, train_loss: 1.6947831090802168, val_loss: 1.6283064763534245 (2 / 100)
train_acc: 0.25339925834363414, val_acc: 0.29064039408866993, train_loss: 1.6934433180852637, val_loss: 1.5803817851202828 (3 / 100)
train_acc: 0.32014833127317677, val_acc: 0.3103448275862069, train_loss: 1.6026609822168338, val_loss: 1.5725892142122015 (4 / 100)
train_acc: 0.3337453646477132, val_acc: 0.35467980295566504, train_loss: 1.568256705888859, val_loss: 1.50492961007386 (5 / 100)
train_acc: 0.32756489493201485, val_acc: 0.35467980295566504, train_loss: 1.5500515860002326, val_loss: 1.7422800956688491 (6 / 100)
train_acc: 0.34981458590852904, val_acc: 0.35960591133004927, train_loss: 1.5418073744944796, val_loss: 1.4995457897045341 (7 / 100)
train_acc: 0.3646477132262052, val_acc: 0.3497536945812808, train_loss: 1.4898959642877097, val_loss: 1.4359590032417786 (8 / 100)
train_acc: 0.3646477132262052, val_acc: 0.37438423645320196, train_loss: 1.4476470844117908, val_loss: 1.3990206019631748 (9 / 100)
train_acc: 0.3868974042027194, val_acc: 0.43842364532019706, train_loss: 1.4280001642677487, val_loss: 1.3387408579511595 (10 / 100)
train_acc: 0.4042027194066749, val_acc: 0.42857142857142855, train_loss: 1.3647719313688715, val_loss: 1.2927695800518166 (11 / 100)
train_acc: 0.3856613102595797, val_acc: 0.35467980295566504, train_loss: 1.3657265558231009, val_loss: 1.5131840335911717 (12 / 100)
train_acc: 0.41409147095179233, val_acc: 0.42857142857142855, train_loss: 1.2972227704834438, val_loss: 1.7003162200814985 (13 / 100)
train_acc: 0.44746600741656367, val_acc: 0.4975369458128079, train_loss: 1.2893182314812621, val_loss: 1.1810132071302442 (14 / 100)
train_acc: 0.4684796044499382, val_acc: 0.5073891625615764, train_loss: 1.2198519226352422, val_loss: 1.1777402059785251 (15 / 100)
train_acc: 0.4969097651421508, val_acc: 0.4827586206896552, train_loss: 1.1823021991291212, val_loss: 1.2557686537944626 (16 / 100)
train_acc: 0.4894932014833127, val_acc: 0.47783251231527096, train_loss: 1.195055755018746, val_loss: 1.1987376976482973 (17 / 100)
train_acc: 0.5377008652657602, val_acc: 0.47783251231527096, train_loss: 1.1224674157365437, val_loss: 1.214100143592346 (18 / 100)
train_acc: 0.5574783683559951, val_acc: 0.4236453201970443, train_loss: 1.0500382659461795, val_loss: 1.6655321051481322 (19 / 100)
train_acc: 0.5599505562422744, val_acc: 0.46798029556650245, train_loss: 0.9995708067720693, val_loss: 1.2592195712874088 (20 / 100)
train_acc: 0.588380716934487, val_acc: 0.5123152709359606, train_loss: 0.9332507104602527, val_loss: 1.1592241120455888 (21 / 100)
train_acc: 0.6093943139678616, val_acc: 0.5369458128078818, train_loss: 0.92438361877564, val_loss: 1.3876624383362643 (22 / 100)
train_acc: 0.6390605686032138, val_acc: 0.4827586206896552, train_loss: 0.8662884279588244, val_loss: 1.3591400243965863 (23 / 100)
train_acc: 0.6711990111248455, val_acc: 0.541871921182266, train_loss: 0.7931703214150276, val_loss: 1.5216865269421356 (24 / 100)
train_acc: 0.7082818294190358, val_acc: 0.5665024630541872, train_loss: 0.7738731314136896, val_loss: 1.114430329922972 (25 / 100)
train_acc: 0.7317676143386898, val_acc: 0.49261083743842365, train_loss: 0.6527844854574121, val_loss: 1.2624888801809602 (26 / 100)
train_acc: 0.7725587144622992, val_acc: 0.5714285714285714, train_loss: 0.5941292479247481, val_loss: 1.7719388836123087 (27 / 100)
train_acc: 0.7787391841779975, val_acc: 0.5615763546798029, train_loss: 0.5535000017458488, val_loss: 1.4615258947381833 (28 / 100)
train_acc: 0.7775030902348579, val_acc: 0.5960591133004927, train_loss: 0.5497178257764196, val_loss: 1.3934292599485425 (29 / 100)
overfit -> train_accuracy 0.8430160692212608, val_accuracy 0.5911330049261084
lr 0.003655275098687901, batch 8, decay 7.451897411905578e-05, gamma 0.018906581126934444, val accuracy 0.5960591133004927, val loss 1.3934292599485425 [32 / 50]
-------------------------------------
{'lr': 0.002428189260812071, 'batch_size': 8, 'weight_decay': 1.4123906877399055e-06, 'gamma': 0.0014886774025640642}
train_acc: 0.17181705809641531, val_acc: 0.18226600985221675, train_loss: 1.7792438554528027, val_loss: 1.7688642346800254 (1 / 100)
train_acc: 0.22249690976514216, val_acc: 0.2561576354679803, train_loss: 1.7530880528415826, val_loss: 1.739744089507117 (2 / 100)
train_acc: 0.24103831891223734, val_acc: 0.19704433497536947, train_loss: 1.730865490893351, val_loss: 1.7017715112329117 (3 / 100)
train_acc: 0.2484548825710754, val_acc: 0.3251231527093596, train_loss: 1.689850763278485, val_loss: 1.5859369720731462 (4 / 100)
train_acc: 0.2843016069221261, val_acc: 0.3645320197044335, train_loss: 1.6672826703310897, val_loss: 1.5679288256931774 (5 / 100)
train_acc: 0.32138442521631644, val_acc: 0.3448275862068966, train_loss: 1.6086824765458831, val_loss: 1.4657851023039794 (6 / 100)
train_acc: 0.3720642768850433, val_acc: 0.3694581280788177, train_loss: 1.5107244234002566, val_loss: 1.4893689716390788 (7 / 100)
train_acc: 0.30778739184178, val_acc: 0.4187192118226601, train_loss: 1.533573260266053, val_loss: 1.3258932053749197 (8 / 100)
train_acc: 0.3819530284301607, val_acc: 0.3793103448275862, train_loss: 1.4215935284481356, val_loss: 1.3690241170046953 (9 / 100)
train_acc: 0.39184177997527814, val_acc: 0.37438423645320196, train_loss: 1.3859775387607192, val_loss: 1.3102904370265642 (10 / 100)
train_acc: 0.40173053152039556, val_acc: 0.4236453201970443, train_loss: 1.3729213362718542, val_loss: 1.3162552869965878 (11 / 100)
train_acc: 0.3943139678615575, val_acc: 0.3793103448275862, train_loss: 1.3383059737414158, val_loss: 1.3124813187885753 (12 / 100)
train_acc: 0.4388133498145859, val_acc: 0.41379310344827586, train_loss: 1.294705562450093, val_loss: 1.354150097945641 (13 / 100)
train_acc: 0.42398022249690975, val_acc: 0.4433497536945813, train_loss: 1.2912853710141554, val_loss: 1.297954519981234 (14 / 100)
train_acc: 0.4388133498145859, val_acc: 0.4729064039408867, train_loss: 1.2812837502275616, val_loss: 1.2070770351757556 (15 / 100)
train_acc: 0.4573547589616811, val_acc: 0.45320197044334976, train_loss: 1.2195125606062975, val_loss: 1.2194344087187292 (16 / 100)
train_acc: 0.4932014833127318, val_acc: 0.4630541871921182, train_loss: 1.1968370226611313, val_loss: 1.229086629275618 (17 / 100)
train_acc: 0.5092707045735476, val_acc: 0.45320197044334976, train_loss: 1.171350221551393, val_loss: 1.1982326278545585 (18 / 100)
train_acc: 0.5339925834363412, val_acc: 0.5320197044334976, train_loss: 1.1173446691227784, val_loss: 1.173800376835715 (19 / 100)
train_acc: 0.5364647713226205, val_acc: 0.541871921182266, train_loss: 1.0842972971747626, val_loss: 1.1064708024410193 (20 / 100)
train_acc: 0.5352286773794809, val_acc: 0.541871921182266, train_loss: 1.0435116674166232, val_loss: 1.1130993515987115 (21 / 100)
train_acc: 0.5648949320148331, val_acc: 0.5073891625615764, train_loss: 1.0354605236807919, val_loss: 1.1180044737061843 (22 / 100)
train_acc: 0.5908529048207664, val_acc: 0.43842364532019706, train_loss: 0.9860732904618101, val_loss: 1.1944345558805418 (23 / 100)
train_acc: 0.6118665018541409, val_acc: 0.5172413793103449, train_loss: 0.9248802258146118, val_loss: 1.1576721782754795 (24 / 100)
train_acc: 0.6563658838071693, val_acc: 0.5714285714285714, train_loss: 0.8756928494009011, val_loss: 1.0388976364887406 (25 / 100)
train_acc: 0.6378244746600742, val_acc: 0.5714285714285714, train_loss: 0.8537485166297413, val_loss: 1.147002384286796 (26 / 100)
train_acc: 0.7119901112484549, val_acc: 0.5073891625615764, train_loss: 0.7313042450304821, val_loss: 1.292736715871125 (27 / 100)
train_acc: 0.7181705809641533, val_acc: 0.5517241379310345, train_loss: 0.7051530895186001, val_loss: 1.2095101114564342 (28 / 100)
train_acc: 0.757725587144623, val_acc: 0.5812807881773399, train_loss: 0.6124361691575115, val_loss: 1.1597866929810623 (29 / 100)
train_acc: 0.792336217552534, val_acc: 0.5665024630541872, train_loss: 0.5368664024492129, val_loss: 1.3396275295999838 (30 / 100)
train_acc: 0.796044499381953, val_acc: 0.5862068965517241, train_loss: 0.5474046003096478, val_loss: 1.4291274863217265 (31 / 100)
overfit -> train_accuracy 0.8405438813349815, val_accuracy 0.5320197044334976
lr 0.002428189260812071, batch 8, decay 1.4123906877399055e-06, gamma 0.0014886774025640642, val accuracy 0.5862068965517241, val loss 1.4291274863217265 [33 / 50]
-------------------------------------
{'lr': 0.002135484385150383, 'batch_size': 8, 'weight_decay': 8.25347825157533e-05, 'gamma': 0.0032738185056202665}
train_acc: 0.1915945611866502, val_acc: 0.22167487684729065, train_loss: 1.7796845686597789, val_loss: 1.7570985267902244 (1 / 100)
train_acc: 0.19530284301606923, val_acc: 0.33497536945812806, train_loss: 1.7568954254405018, val_loss: 1.7320859596647065 (2 / 100)
train_acc: 0.2373300370828183, val_acc: 0.3891625615763547, train_loss: 1.7302985217868914, val_loss: 1.6679977307765943 (3 / 100)
train_acc: 0.3053152039555006, val_acc: 0.23645320197044334, train_loss: 1.6595837247681118, val_loss: 1.6469827279668723 (4 / 100)
train_acc: 0.315203955500618, val_acc: 0.3891625615763547, train_loss: 1.6251008192305216, val_loss: 1.4909016034873248 (5 / 100)
train_acc: 0.3411619283065513, val_acc: 0.28078817733990147, train_loss: 1.5246680683199643, val_loss: 1.5057907409855884 (6 / 100)
train_acc: 0.3720642768850433, val_acc: 0.39408866995073893, train_loss: 1.4869153150374574, val_loss: 1.4670364633569577 (7 / 100)
train_acc: 0.34610630407911, val_acc: 0.39408866995073893, train_loss: 1.4637848163859364, val_loss: 1.3815152374981658 (8 / 100)
train_acc: 0.3794808405438813, val_acc: 0.4236453201970443, train_loss: 1.4084813755433845, val_loss: 1.2877359454854955 (9 / 100)
train_acc: 0.39555006180469715, val_acc: 0.42857142857142855, train_loss: 1.372819039229262, val_loss: 1.335607145807426 (10 / 100)
train_acc: 0.3967861557478368, val_acc: 0.43842364532019706, train_loss: 1.326978405120199, val_loss: 1.2532846293425912 (11 / 100)
train_acc: 0.4363411619283066, val_acc: 0.3645320197044335, train_loss: 1.3143538939496053, val_loss: 1.8450641314971623 (12 / 100)
train_acc: 0.3831891223733004, val_acc: 0.4039408866995074, train_loss: 1.355723165316399, val_loss: 1.4495194557265108 (13 / 100)
train_acc: 0.4227441285537701, val_acc: 0.47783251231527096, train_loss: 1.2874181208710735, val_loss: 1.2122024579588415 (14 / 100)
train_acc: 0.44746600741656367, val_acc: 0.4827586206896552, train_loss: 1.2750703625390203, val_loss: 1.2566849328026983 (15 / 100)
train_acc: 0.4610630407911001, val_acc: 0.49261083743842365, train_loss: 1.2300737125175107, val_loss: 1.1577348162975218 (16 / 100)
train_acc: 0.48084054388133496, val_acc: 0.4630541871921182, train_loss: 1.2046146180632678, val_loss: 1.3222737171379804 (17 / 100)
train_acc: 0.47713226205191595, val_acc: 0.4630541871921182, train_loss: 1.1814190006373986, val_loss: 1.2525099568766327 (18 / 100)
train_acc: 0.4932014833127318, val_acc: 0.4729064039408867, train_loss: 1.1870062215661237, val_loss: 1.2282260938230993 (19 / 100)
train_acc: 0.5352286773794809, val_acc: 0.458128078817734, train_loss: 1.1350595618060553, val_loss: 1.1414017436539599 (20 / 100)
train_acc: 0.5179233621755254, val_acc: 0.49261083743842365, train_loss: 1.0893974127374562, val_loss: 1.3658897909037586 (21 / 100)
train_acc: 0.553770086526576, val_acc: 0.5467980295566502, train_loss: 1.0379544005847832, val_loss: 1.0512111263322126 (22 / 100)
train_acc: 0.6019777503090235, val_acc: 0.5172413793103449, train_loss: 0.9400293402677709, val_loss: 1.3543661639020947 (23 / 100)
train_acc: 0.5982694684796045, val_acc: 0.5221674876847291, train_loss: 0.9394106179733653, val_loss: 1.0252800586775606 (24 / 100)
train_acc: 0.6427688504326329, val_acc: 0.4975369458128079, train_loss: 0.8692531184890804, val_loss: 1.2678552266999419 (25 / 100)
train_acc: 0.6205191594561187, val_acc: 0.5073891625615764, train_loss: 0.8945398292376467, val_loss: 1.2328403401257368 (26 / 100)
train_acc: 0.6786155747836835, val_acc: 0.5221674876847291, train_loss: 0.7748077561150964, val_loss: 1.23681206656207 (27 / 100)
train_acc: 0.7021013597033374, val_acc: 0.5665024630541872, train_loss: 0.7473605200740404, val_loss: 1.3924285543376003 (28 / 100)
train_acc: 0.7206427688504327, val_acc: 0.5270935960591133, train_loss: 0.714719716933808, val_loss: 1.2819634678915803 (29 / 100)
train_acc: 0.757725587144623, val_acc: 0.5615763546798029, train_loss: 0.6283730156489444, val_loss: 1.2263755510593284 (30 / 100)
train_acc: 0.7218788627935723, val_acc: 0.5024630541871922, train_loss: 0.6727813902243106, val_loss: 2.172042814968842 (31 / 100)
train_acc: 0.788627935723115, val_acc: 0.5714285714285714, train_loss: 0.5633845780187543, val_loss: 1.5463382228841922 (32 / 100)
overfit -> train_accuracy 0.8133498145859085, val_accuracy 0.5369458128078818
lr 0.002135484385150383, batch 8, decay 8.25347825157533e-05, gamma 0.0032738185056202665, val accuracy 0.5714285714285714, val loss 1.5463382228841922 [34 / 50]
-------------------------------------
{'lr': 0.0021239328786315627, 'batch_size': 8, 'weight_decay': 1.8002685899584096e-06, 'gamma': 0.0014912770153576727}
train_acc: 0.1792336217552534, val_acc: 0.19704433497536947, train_loss: 1.774921822754208, val_loss: 1.7605619618458113 (1 / 100)
train_acc: 0.2373300370828183, val_acc: 0.23645320197044334, train_loss: 1.7410226033261444, val_loss: 1.7114179334029775 (2 / 100)
train_acc: 0.242274412855377, val_acc: 0.3399014778325123, train_loss: 1.7159704755351628, val_loss: 1.6048505617479973 (3 / 100)
train_acc: 0.2694684796044499, val_acc: 0.3497536945812808, train_loss: 1.6865272252315053, val_loss: 1.5564316898731176 (4 / 100)
train_acc: 0.3176761433868974, val_acc: 0.3645320197044335, train_loss: 1.5653406123737765, val_loss: 1.4522875823410861 (5 / 100)
train_acc: 0.3337453646477132, val_acc: 0.3842364532019704, train_loss: 1.5672870872931368, val_loss: 1.397706307213882 (6 / 100)
train_acc: 0.37330037082818296, val_acc: 0.43349753694581283, train_loss: 1.4596958595094338, val_loss: 1.3529750936724283 (7 / 100)
train_acc: 0.3658838071693449, val_acc: 0.32019704433497537, train_loss: 1.4275559743933095, val_loss: 1.3926235369860833 (8 / 100)
train_acc: 0.3943139678615575, val_acc: 0.41379310344827586, train_loss: 1.390916929846199, val_loss: 1.3010299264503817 (9 / 100)
train_acc: 0.3943139678615575, val_acc: 0.4236453201970443, train_loss: 1.3361500766869676, val_loss: 1.296840513281047 (10 / 100)
train_acc: 0.4079110012360939, val_acc: 0.4088669950738916, train_loss: 1.309666874676906, val_loss: 1.2572472530045533 (11 / 100)
train_acc: 0.41903584672435107, val_acc: 0.47783251231527096, train_loss: 1.3183415609472908, val_loss: 1.2383446687548032 (12 / 100)
train_acc: 0.4215080346106304, val_acc: 0.5073891625615764, train_loss: 1.3203161207795586, val_loss: 1.2180242221343693 (13 / 100)
train_acc: 0.4783683559950556, val_acc: 0.4088669950738916, train_loss: 1.2373215456680844, val_loss: 1.310778415849056 (14 / 100)
train_acc: 0.4635352286773795, val_acc: 0.5320197044334976, train_loss: 1.233522721657205, val_loss: 1.1820104809230185 (15 / 100)
train_acc: 0.4796044499381953, val_acc: 0.45320197044334976, train_loss: 1.2115205607396564, val_loss: 1.198667357120608 (16 / 100)
train_acc: 0.47095179233621753, val_acc: 0.4876847290640394, train_loss: 1.2317187276257868, val_loss: 1.2379324782658092 (17 / 100)
train_acc: 0.5438813349814586, val_acc: 0.42857142857142855, train_loss: 1.1218160967007555, val_loss: 1.2373175077837677 (18 / 100)
train_acc: 0.5265760197775031, val_acc: 0.5024630541871922, train_loss: 1.1069472764419537, val_loss: 1.2325490807077568 (19 / 100)
train_acc: 0.5735475896168108, val_acc: 0.5221674876847291, train_loss: 1.0479844636174451, val_loss: 1.1161143151410107 (20 / 100)
train_acc: 0.6180469715698393, val_acc: 0.5467980295566502, train_loss: 0.9653413283957539, val_loss: 1.0952987374343308 (21 / 100)
train_acc: 0.6044499381953028, val_acc: 0.5123152709359606, train_loss: 0.9748528000745549, val_loss: 1.1120644772581278 (22 / 100)
train_acc: 0.6724351050679852, val_acc: 0.4975369458128079, train_loss: 0.8436870937441718, val_loss: 1.5641551998448489 (23 / 100)
train_acc: 0.6093943139678616, val_acc: 0.6009852216748769, train_loss: 0.9208822241524978, val_loss: 1.09176496302553 (24 / 100)
train_acc: 0.6674907292954264, val_acc: 0.5270935960591133, train_loss: 0.8277573267225872, val_loss: 1.2997439424392625 (25 / 100)
train_acc: 0.6946847960444994, val_acc: 0.5172413793103449, train_loss: 0.7515395651346968, val_loss: 1.7026134829399326 (26 / 100)
train_acc: 0.7354758961681088, val_acc: 0.5714285714285714, train_loss: 0.6676735462452779, val_loss: 1.4058496159285747 (27 / 100)
train_acc: 0.7379480840543882, val_acc: 0.5566502463054187, train_loss: 0.7275275998416142, val_loss: 1.2723554437383642 (28 / 100)
train_acc: 0.7564894932014833, val_acc: 0.5763546798029556, train_loss: 0.6456212412441616, val_loss: 1.1383921477594987 (29 / 100)
train_acc: 0.7985166872682324, val_acc: 0.5911330049261084, train_loss: 0.5355366497900341, val_loss: 1.2972672760780222 (30 / 100)
train_acc: 0.8170580964153276, val_acc: 0.6059113300492611, train_loss: 0.4896031616644747, val_loss: 1.1484346225343902 (31 / 100)
train_acc: 0.8331273176761433, val_acc: 0.645320197044335, train_loss: 0.43965921958975207, val_loss: 1.019117813389932 (32 / 100)
train_acc: 0.8318912237330037, val_acc: 0.5862068965517241, train_loss: 0.42981787856636, val_loss: 1.9265087707876571 (33 / 100)
overfit -> train_accuracy 0.761433868974042, val_accuracy 0.5073891625615764
lr 0.0021239328786315627, batch 8, decay 1.8002685899584096e-06, gamma 0.0014912770153576727, val accuracy 0.645320197044335, val loss 1.019117813389932 [35 / 50]
-------------------------------------
{'lr': 0.0035698408335762617, 'batch_size': 8, 'weight_decay': 1.8460696188488813e-06, 'gamma': 0.019453739757377877}
train_acc: 0.20148331273176762, val_acc: 0.28078817733990147, train_loss: 1.7793107156553138, val_loss: 1.7650725612499443 (1 / 100)
train_acc: 0.2249690976514215, val_acc: 0.2660098522167488, train_loss: 1.7553792469875775, val_loss: 1.7175018135549986 (2 / 100)
train_acc: 0.2323856613102596, val_acc: 0.3448275862068966, train_loss: 1.7089878241122873, val_loss: 1.6026869237129324 (3 / 100)
train_acc: 0.31396786155747836, val_acc: 0.3103448275862069, train_loss: 1.6168405514565032, val_loss: 1.738271737333589 (4 / 100)
train_acc: 0.3399258343634116, val_acc: 0.3645320197044335, train_loss: 1.5625925469310087, val_loss: 1.4511867843825241 (5 / 100)
train_acc: 0.33868974042027195, val_acc: 0.3694581280788177, train_loss: 1.5233530432391078, val_loss: 1.4542329563883138 (6 / 100)
train_acc: 0.3572311495673671, val_acc: 0.35960591133004927, train_loss: 1.4528769605386684, val_loss: 1.4684496237139397 (7 / 100)
train_acc: 0.3572311495673671, val_acc: 0.42857142857142855, train_loss: 1.4505259187613488, val_loss: 1.3029247131840935 (8 / 100)
train_acc: 0.3683559950556242, val_acc: 0.3399014778325123, train_loss: 1.4410061901078384, val_loss: 1.4572819712126783 (9 / 100)
train_acc: 0.3967861557478368, val_acc: 0.4187192118226601, train_loss: 1.377019749288948, val_loss: 1.320100895876955 (10 / 100)
train_acc: 0.39555006180469715, val_acc: 0.42857142857142855, train_loss: 1.326821488709326, val_loss: 1.3434989417127787 (11 / 100)
train_acc: 0.4363411619283066, val_acc: 0.41379310344827586, train_loss: 1.2738280741334405, val_loss: 1.2509511471381916 (12 / 100)
train_acc: 0.44252163164400493, val_acc: 0.4827586206896552, train_loss: 1.269016259238216, val_loss: 1.2827756087768254 (13 / 100)
train_acc: 0.453646477132262, val_acc: 0.4975369458128079, train_loss: 1.2258079396779518, val_loss: 1.1936339092959325 (14 / 100)
train_acc: 0.4932014833127318, val_acc: 0.4975369458128079, train_loss: 1.1847160784953603, val_loss: 1.0994109713972495 (15 / 100)
train_acc: 0.4919653893695921, val_acc: 0.4482758620689655, train_loss: 1.1734514887754348, val_loss: 1.372427956811313 (16 / 100)
train_acc: 0.5216316440049443, val_acc: 0.4827586206896552, train_loss: 1.144024688616966, val_loss: 1.2329977821246745 (17 / 100)
train_acc: 0.5475896168108776, val_acc: 0.46798029556650245, train_loss: 1.0765251992512104, val_loss: 1.4742708852138426 (18 / 100)
train_acc: 0.5822002472187886, val_acc: 0.5221674876847291, train_loss: 1.0035264589288473, val_loss: 1.0799006144401475 (19 / 100)
train_acc: 0.5784919653893696, val_acc: 0.4827586206896552, train_loss: 1.02760723376893, val_loss: 1.2747541166878686 (20 / 100)
train_acc: 0.6291718170580964, val_acc: 0.5123152709359606, train_loss: 0.9089728282909606, val_loss: 1.0909299022458456 (21 / 100)
train_acc: 0.630407911001236, val_acc: 0.5073891625615764, train_loss: 0.8987188041578559, val_loss: 1.3172427914999976 (22 / 100)
train_acc: 0.688504326328801, val_acc: 0.5123152709359606, train_loss: 0.7861603927258656, val_loss: 1.5573338226144537 (23 / 100)
train_acc: 0.6909765142150803, val_acc: 0.5123152709359606, train_loss: 0.7787428017894477, val_loss: 1.3817379034211483 (24 / 100)
train_acc: 0.7045735475896168, val_acc: 0.5665024630541872, train_loss: 0.7267565243913158, val_loss: 1.1701856833960622 (25 / 100)
train_acc: 0.7268232385661311, val_acc: 0.5369458128078818, train_loss: 0.7482292372157754, val_loss: 1.2726656938421315 (26 / 100)
train_acc: 0.7663782447466008, val_acc: 0.5566502463054187, train_loss: 0.6319353984224487, val_loss: 1.4585603352250724 (27 / 100)
train_acc: 0.7787391841779975, val_acc: 0.5517241379310345, train_loss: 0.591094493571259, val_loss: 1.1552335358605597 (28 / 100)
train_acc: 0.8084054388133498, val_acc: 0.6059113300492611, train_loss: 0.5412626406465679, val_loss: 1.5204266280376266 (29 / 100)
train_acc: 0.8084054388133498, val_acc: 0.5714285714285714, train_loss: 0.53801130408557, val_loss: 1.3523123957253442 (30 / 100)
overfit -> train_accuracy 0.8257107540173053, val_accuracy 0.5270935960591133
lr 0.0035698408335762617, batch 8, decay 1.8460696188488813e-06, gamma 0.019453739757377877, val accuracy 0.6059113300492611, val loss 1.5204266280376266 [36 / 50]
-------------------------------------
{'lr': 0.003381617040598148, 'batch_size': 8, 'weight_decay': 2.635937854463478e-06, 'gamma': 0.00474259092069249}
train_acc: 0.15451174289245984, val_acc: 0.2955665024630542, train_loss: 1.7805941303816655, val_loss: 1.759442695843175 (1 / 100)
train_acc: 0.22126081582200247, val_acc: 0.21674876847290642, train_loss: 1.7479659735935433, val_loss: 1.7491567264049512 (2 / 100)
train_acc: 0.24598269468479605, val_acc: 0.31527093596059114, train_loss: 1.6869624288768792, val_loss: 1.594120082009602 (3 / 100)
train_acc: 0.31025957972805934, val_acc: 0.3399014778325123, train_loss: 1.5954059536289078, val_loss: 1.5532651511319164 (4 / 100)
train_acc: 0.2954264524103832, val_acc: 0.22660098522167488, train_loss: 1.5981764668145197, val_loss: 1.8358975096876398 (5 / 100)
train_acc: 0.22620519159456118, val_acc: 0.32019704433497537, train_loss: 1.7565534478801732, val_loss: 1.6288828209703192 (6 / 100)
train_acc: 0.27070457354758964, val_acc: 0.31527093596059114, train_loss: 1.6996671431438886, val_loss: 1.7011911334662602 (7 / 100)
train_acc: 0.3003708281829419, val_acc: 0.33497536945812806, train_loss: 1.6062228079926688, val_loss: 1.6901592004475334 (8 / 100)
train_acc: 0.31396786155747836, val_acc: 0.4187192118226601, train_loss: 1.5796471664725777, val_loss: 1.4041048012343533 (9 / 100)
train_acc: 0.33127317676143386, val_acc: 0.3448275862068966, train_loss: 1.4948989926516199, val_loss: 1.5659192791713283 (10 / 100)
train_acc: 0.32014833127317677, val_acc: 0.3497536945812808, train_loss: 1.5000480733194812, val_loss: 1.413641918468945 (11 / 100)
train_acc: 0.37824474660074164, val_acc: 0.35960591133004927, train_loss: 1.3908621554616356, val_loss: 1.4924429837118816 (12 / 100)
train_acc: 0.4079110012360939, val_acc: 0.3891625615763547, train_loss: 1.3779854345675304, val_loss: 1.3154010414489972 (13 / 100)
train_acc: 0.40667490729295425, val_acc: 0.4088669950738916, train_loss: 1.3474329664327012, val_loss: 1.37426802677474 (14 / 100)
train_acc: 0.3831891223733004, val_acc: 0.39408866995073893, train_loss: 1.3539641103874178, val_loss: 1.3081490200728618 (15 / 100)
train_acc: 0.40667490729295425, val_acc: 0.47783251231527096, train_loss: 1.325885893829968, val_loss: 1.3620935376054548 (16 / 100)
train_acc: 0.41903584672435107, val_acc: 0.4236453201970443, train_loss: 1.3085741896564498, val_loss: 1.2791056580144196 (17 / 100)
train_acc: 0.48084054388133496, val_acc: 0.35467980295566504, train_loss: 1.218135237251725, val_loss: 1.625952569134717 (18 / 100)
train_acc: 0.45982694684796044, val_acc: 0.41379310344827586, train_loss: 1.2650093232156026, val_loss: 1.3686640932054943 (19 / 100)
train_acc: 0.47713226205191595, val_acc: 0.43842364532019706, train_loss: 1.167486331076793, val_loss: 1.2841156315920976 (20 / 100)
train_acc: 0.4932014833127318, val_acc: 0.5221674876847291, train_loss: 1.18402247199022, val_loss: 1.1139702127484852 (21 / 100)
train_acc: 0.5203955500618047, val_acc: 0.4039408866995074, train_loss: 1.0896511295964604, val_loss: 1.3051199563618363 (22 / 100)
train_acc: 0.5500618046971569, val_acc: 0.4876847290640394, train_loss: 1.0540865584563266, val_loss: 1.1653243091893313 (23 / 100)
train_acc: 0.5611866501854141, val_acc: 0.47783251231527096, train_loss: 1.041749223348384, val_loss: 1.1946896942965504 (24 / 100)
train_acc: 0.5933250927070457, val_acc: 0.4876847290640394, train_loss: 0.9708791091503997, val_loss: 1.5175418549864164 (25 / 100)
train_acc: 0.6056860321384425, val_acc: 0.5714285714285714, train_loss: 0.9251834803959642, val_loss: 1.1153950450455614 (26 / 100)
train_acc: 0.6353522867737948, val_acc: 0.5467980295566502, train_loss: 0.9037794498047517, val_loss: 1.1227539246305456 (27 / 100)
train_acc: 0.688504326328801, val_acc: 0.5320197044334976, train_loss: 0.7872428541867194, val_loss: 1.4048902953199565 (28 / 100)
train_acc: 0.6711990111248455, val_acc: 0.4876847290640394, train_loss: 0.7919913349104458, val_loss: 1.505513495999604 (29 / 100)
train_acc: 0.6378244746600742, val_acc: 0.5665024630541872, train_loss: 0.9170583104910456, val_loss: 1.0902459609684685 (30 / 100)
train_acc: 0.7243510506798516, val_acc: 0.5812807881773399, train_loss: 0.6955423372785005, val_loss: 1.1305126069214544 (31 / 100)
train_acc: 0.7132262051915945, val_acc: 0.5369458128078818, train_loss: 0.6902007537954965, val_loss: 1.1915480867762285 (32 / 100)
train_acc: 0.65389369592089, val_acc: 0.5960591133004927, train_loss: 0.9397023233996039, val_loss: 1.0891152026471247 (33 / 100)
train_acc: 0.7824474660074165, val_acc: 0.5714285714285714, train_loss: 0.5812380254931738, val_loss: 1.274852195984037 (34 / 100)
train_acc: 0.8022249690976514, val_acc: 0.5763546798029556, train_loss: 0.5427249832117955, val_loss: 1.2605184205059934 (35 / 100)
train_acc: 0.7836835599505563, val_acc: 0.5665024630541872, train_loss: 0.5624109679011097, val_loss: 1.5574546353570347 (36 / 100)
overfit -> train_accuracy 0.8133498145859085, val_accuracy 0.5270935960591133
lr 0.003381617040598148, batch 8, decay 2.635937854463478e-06, gamma 0.00474259092069249, val accuracy 0.5960591133004927, val loss 1.0891152026471247 [37 / 50]
-------------------------------------
{'lr': 0.0036209982662466058, 'batch_size': 8, 'weight_decay': 3.841755034707538e-05, 'gamma': 0.009547088109964914}
train_acc: 0.17058096415327564, val_acc: 0.18226600985221675, train_loss: 1.7814499990017658, val_loss: 1.7660312482288905 (1 / 100)
train_acc: 0.23362175525339926, val_acc: 0.3251231527093596, train_loss: 1.7340875536903315, val_loss: 1.6261115361904275 (2 / 100)
train_acc: 0.25710754017305315, val_acc: 0.3448275862068966, train_loss: 1.6788690140279763, val_loss: 1.5549213146341259 (3 / 100)
train_acc: 0.3065512978986403, val_acc: 0.3251231527093596, train_loss: 1.636294986762576, val_loss: 1.574959670968831 (4 / 100)
train_acc: 0.3263288009888752, val_acc: 0.35467980295566504, train_loss: 1.5874691531154221, val_loss: 1.5350808433711236 (5 / 100)
train_acc: 0.3189122373300371, val_acc: 0.37438423645320196, train_loss: 1.51922829395761, val_loss: 1.460234415354987 (6 / 100)
train_acc: 0.3621755253399258, val_acc: 0.3694581280788177, train_loss: 1.4452233706475484, val_loss: 1.7679899396567509 (7 / 100)
train_acc: 0.34363411619283063, val_acc: 0.35467980295566504, train_loss: 1.500983863590085, val_loss: 1.3936425852658125 (8 / 100)
train_acc: 0.3794808405438813, val_acc: 0.4187192118226601, train_loss: 1.4207733471697135, val_loss: 1.357987403282391 (9 / 100)
train_acc: 0.380716934487021, val_acc: 0.3891625615763547, train_loss: 1.4261352844969157, val_loss: 1.279417521260642 (10 / 100)
train_acc: 0.3943139678615575, val_acc: 0.43349753694581283, train_loss: 1.3511168576583579, val_loss: 1.2395616711066861 (11 / 100)
train_acc: 0.39060568603213847, val_acc: 0.43842364532019706, train_loss: 1.341832638228927, val_loss: 1.286441600675066 (12 / 100)
train_acc: 0.4177997527812114, val_acc: 0.4187192118226601, train_loss: 1.3202546305945246, val_loss: 1.2505103161769549 (13 / 100)
train_acc: 0.4684796044499382, val_acc: 0.41379310344827586, train_loss: 1.2325632850672907, val_loss: 1.2654331112142854 (14 / 100)
train_acc: 0.4684796044499382, val_acc: 0.46798029556650245, train_loss: 1.2207534154059714, val_loss: 1.1854564016088476 (15 / 100)
train_acc: 0.5105067985166872, val_acc: 0.4876847290640394, train_loss: 1.148585665505955, val_loss: 1.1486015766125006 (16 / 100)
train_acc: 0.5339925834363412, val_acc: 0.5172413793103449, train_loss: 1.0891487501165629, val_loss: 1.2227273874094922 (17 / 100)
train_acc: 0.5129789864029666, val_acc: 0.43349753694581283, train_loss: 1.1132679368715792, val_loss: 1.4074401379806067 (18 / 100)
train_acc: 0.5475896168108776, val_acc: 0.5123152709359606, train_loss: 1.028285049832206, val_loss: 1.1295003039496285 (19 / 100)
train_acc: 0.5562422744128553, val_acc: 0.4729064039408867, train_loss: 1.049233151896775, val_loss: 1.4004037902860218 (20 / 100)
train_acc: 0.6205191594561187, val_acc: 0.5320197044334976, train_loss: 0.9258993635660933, val_loss: 1.2677584090843577 (21 / 100)
train_acc: 0.6365883807169345, val_acc: 0.541871921182266, train_loss: 0.9002243939082614, val_loss: 1.1266618156668 (22 / 100)
train_acc: 0.6823238566131026, val_acc: 0.5270935960591133, train_loss: 0.8031531526662216, val_loss: 1.4164214016768732 (23 / 100)
train_acc: 0.6786155747836835, val_acc: 0.5221674876847291, train_loss: 0.7971176641391146, val_loss: 1.491448061219577 (24 / 100)
train_acc: 0.7206427688504327, val_acc: 0.5960591133004927, train_loss: 0.6761371424819984, val_loss: 1.5883277719243993 (25 / 100)
train_acc: 0.7218788627935723, val_acc: 0.5024630541871922, train_loss: 0.6733884366392647, val_loss: 2.0699839961939843 (26 / 100)
train_acc: 0.7503090234857849, val_acc: 0.5467980295566502, train_loss: 0.6160349424454131, val_loss: 1.4504984681829443 (27 / 100)
overfit -> train_accuracy 0.7824474660074165, val_accuracy 0.5024630541871922
lr 0.0036209982662466058, batch 8, decay 3.841755034707538e-05, gamma 0.009547088109964914, val accuracy 0.5960591133004927, val loss 1.5883277719243993 [38 / 50]
-------------------------------------
{'lr': 0.0032140970022133406, 'batch_size': 8, 'weight_decay': 2.1193697240751113e-05, 'gamma': 0.031447258372514296}
train_acc: 0.19901112484548825, val_acc: 0.18226600985221675, train_loss: 1.768583147133826, val_loss: 1.7664390996171924 (1 / 100)
train_acc: 0.2200247218788628, val_acc: 0.19704433497536947, train_loss: 1.7120484056225225, val_loss: 1.7850799983358148 (2 / 100)
train_acc: 0.18541409147095178, val_acc: 0.2512315270935961, train_loss: 1.7791752588321605, val_loss: 1.7511737922142292 (3 / 100)
train_acc: 0.26823238566131025, val_acc: 0.28078817733990147, train_loss: 1.7295387772606095, val_loss: 1.6570506113503367 (4 / 100)
train_acc: 0.2583436341161928, val_acc: 0.29064039408866993, train_loss: 1.67936889910138, val_loss: 1.60342628086729 (5 / 100)
train_acc: 0.3016069221260816, val_acc: 0.270935960591133, train_loss: 1.6351528346022806, val_loss: 1.7028672331072428 (6 / 100)
train_acc: 0.33127317676143386, val_acc: 0.28078817733990147, train_loss: 1.612983372951173, val_loss: 1.699159500046904 (7 / 100)
train_acc: 0.2941903584672435, val_acc: 0.27586206896551724, train_loss: 1.62395131543776, val_loss: 1.518305095545764 (8 / 100)
train_acc: 0.36341161928306553, val_acc: 0.3497536945812808, train_loss: 1.4700999015341287, val_loss: 1.4606201672201673 (9 / 100)
train_acc: 0.3621755253399258, val_acc: 0.3891625615763547, train_loss: 1.4901227311533962, val_loss: 1.3458021943792333 (10 / 100)
train_acc: 0.37824474660074164, val_acc: 0.37438423645320196, train_loss: 1.440546887029971, val_loss: 1.3846393917581719 (11 / 100)
train_acc: 0.3831891223733004, val_acc: 0.39901477832512317, train_loss: 1.4088506938795224, val_loss: 1.3183791426015017 (12 / 100)
train_acc: 0.3856613102595797, val_acc: 0.3793103448275862, train_loss: 1.3625398395382724, val_loss: 1.5066125457509985 (13 / 100)
train_acc: 0.40173053152039556, val_acc: 0.42857142857142855, train_loss: 1.3415229722802218, val_loss: 1.2712083544049944 (14 / 100)
train_acc: 0.43139678615574784, val_acc: 0.3694581280788177, train_loss: 1.3068710225179552, val_loss: 1.4441675998894452 (15 / 100)
train_acc: 0.46600741656365885, val_acc: 0.42857142857142855, train_loss: 1.2586932656791507, val_loss: 1.3419574822111082 (16 / 100)
train_acc: 0.46971569839307786, val_acc: 0.4827586206896552, train_loss: 1.217586777560938, val_loss: 1.2851470820422244 (17 / 100)
train_acc: 0.4919653893695921, val_acc: 0.33497536945812806, train_loss: 1.1767797582081723, val_loss: 1.7119966017201615 (18 / 100)
train_acc: 0.5092707045735476, val_acc: 0.49261083743842365, train_loss: 1.1396883922100656, val_loss: 1.167473720212288 (19 / 100)
train_acc: 0.5401730531520396, val_acc: 0.4975369458128079, train_loss: 1.0756718405097612, val_loss: 1.2302863788722185 (20 / 100)
train_acc: 0.5525339925834364, val_acc: 0.4975369458128079, train_loss: 1.0421801425323203, val_loss: 1.1859088531268642 (21 / 100)
train_acc: 0.646477132262052, val_acc: 0.5517241379310345, train_loss: 0.9222819811039419, val_loss: 1.1767474354194303 (22 / 100)
train_acc: 0.681087762669963, val_acc: 0.5320197044334976, train_loss: 0.8321513528434838, val_loss: 1.2037691961368318 (23 / 100)
train_acc: 0.7144622991347342, val_acc: 0.5221674876847291, train_loss: 0.7216301170650902, val_loss: 1.3998625930306947 (24 / 100)
train_acc: 0.6514215080346106, val_acc: 0.5467980295566502, train_loss: 0.9415171119869123, val_loss: 1.1981558799743652 (25 / 100)
train_acc: 0.7255871446229913, val_acc: 0.5123152709359606, train_loss: 0.7140431881540343, val_loss: 1.519590237457764 (26 / 100)
train_acc: 0.7589616810877626, val_acc: 0.5123152709359606, train_loss: 0.6266253782880615, val_loss: 1.5356340064791036 (27 / 100)
train_acc: 0.7404202719406675, val_acc: 0.5467980295566502, train_loss: 0.662680520115441, val_loss: 1.4884026188568529 (28 / 100)
overfit -> train_accuracy 0.8182941903584673, val_accuracy 0.5221674876847291
lr 0.0032140970022133406, batch 8, decay 2.1193697240751113e-05, gamma 0.031447258372514296, val accuracy 0.5517241379310345, val loss 1.1767474354194303 [39 / 50]
-------------------------------------
{'lr': 0.0035774473800650494, 'batch_size': 8, 'weight_decay': 3.878205508067958e-06, 'gamma': 0.02806427725874454}
train_acc: 0.2027194066749073, val_acc: 0.18226600985221675, train_loss: 1.7782707308661951, val_loss: 1.7522904303273543 (1 / 100)
train_acc: 0.23485784919653893, val_acc: 0.2413793103448276, train_loss: 1.741410404848964, val_loss: 1.705061132684717 (2 / 100)
train_acc: 0.2595797280593325, val_acc: 0.2413793103448276, train_loss: 1.691459158442371, val_loss: 1.7411505289265674 (3 / 100)
train_acc: 0.27812113720642767, val_acc: 0.30049261083743845, train_loss: 1.7042394833747476, val_loss: 1.5939399978797424 (4 / 100)
train_acc: 0.2484548825710754, val_acc: 0.21674876847290642, train_loss: 1.712356589191187, val_loss: 1.750113820794768 (5 / 100)
train_acc: 0.2867737948084054, val_acc: 0.31527093596059114, train_loss: 1.6322229147252105, val_loss: 1.5423256229297282 (6 / 100)
train_acc: 0.3238566131025958, val_acc: 0.3448275862068966, train_loss: 1.5688844698173892, val_loss: 1.5026508963166787 (7 / 100)
train_acc: 0.3621755253399258, val_acc: 0.3645320197044335, train_loss: 1.5024547320508543, val_loss: 1.345052977500878 (8 / 100)
train_acc: 0.37082818294190356, val_acc: 0.37438423645320196, train_loss: 1.4573001661170988, val_loss: 1.3390788709001589 (9 / 100)
train_acc: 0.3646477132262052, val_acc: 0.4187192118226601, train_loss: 1.4280227416820077, val_loss: 1.2926307925449803 (10 / 100)
train_acc: 0.3967861557478368, val_acc: 0.3645320197044335, train_loss: 1.3751469321539729, val_loss: 1.3115206070134204 (11 / 100)
train_acc: 0.40914709517923364, val_acc: 0.4088669950738916, train_loss: 1.3019379225590615, val_loss: 1.3291129654851452 (12 / 100)
train_acc: 0.4252163164400494, val_acc: 0.3842364532019704, train_loss: 1.3240514617915207, val_loss: 1.3539861769511783 (13 / 100)
train_acc: 0.4622991347342398, val_acc: 0.41379310344827586, train_loss: 1.244354529787495, val_loss: 1.3117058136193036 (14 / 100)
train_acc: 0.46971569839307786, val_acc: 0.42857142857142855, train_loss: 1.2449182809799062, val_loss: 1.220314280152908 (15 / 100)
train_acc: 0.4635352286773795, val_acc: 0.5172413793103449, train_loss: 1.2663042355232097, val_loss: 1.19737257980948 (16 / 100)
train_acc: 0.4635352286773795, val_acc: 0.4876847290640394, train_loss: 1.221344554085372, val_loss: 1.190900552448968 (17 / 100)
train_acc: 0.5166872682323856, val_acc: 0.45320197044334976, train_loss: 1.1558262211430648, val_loss: 1.1385110963154308 (18 / 100)
train_acc: 0.5500618046971569, val_acc: 0.5073891625615764, train_loss: 1.1000027917371544, val_loss: 1.2134616947526415 (19 / 100)
train_acc: 0.5611866501854141, val_acc: 0.45320197044334976, train_loss: 1.0494152515279347, val_loss: 1.2694054495524891 (20 / 100)
train_acc: 0.5661310259579728, val_acc: 0.47783251231527096, train_loss: 1.038900995726049, val_loss: 1.2418992266866373 (21 / 100)
train_acc: 0.5562422744128553, val_acc: 0.4876847290640394, train_loss: 1.0648790316469736, val_loss: 1.1685838232486707 (22 / 100)
train_acc: 0.6551297898640297, val_acc: 0.5123152709359606, train_loss: 0.8548160445115475, val_loss: 1.3072416477015454 (23 / 100)
train_acc: 0.6254635352286774, val_acc: 0.47783251231527096, train_loss: 0.8788620771377431, val_loss: 1.5152595841825889 (24 / 100)
train_acc: 0.6501854140914709, val_acc: 0.6009852216748769, train_loss: 0.8688595875526682, val_loss: 1.0126189630607079 (25 / 100)
train_acc: 0.7292954264524104, val_acc: 0.541871921182266, train_loss: 0.7289494949453988, val_loss: 1.466686725616455 (26 / 100)
train_acc: 0.7317676143386898, val_acc: 0.541871921182266, train_loss: 0.6914603023505476, val_loss: 1.4031368103520623 (27 / 100)
train_acc: 0.7478368355995055, val_acc: 0.541871921182266, train_loss: 0.6541924006564656, val_loss: 1.601122275361874 (28 / 100)
train_acc: 0.7268232385661311, val_acc: 0.5862068965517241, train_loss: 0.6556644286449229, val_loss: 1.2967639158805604 (29 / 100)
overfit -> train_accuracy 0.8046971569839307, val_accuracy 0.5172413793103449
lr 0.0035774473800650494, batch 8, decay 3.878205508067958e-06, gamma 0.02806427725874454, val accuracy 0.6009852216748769, val loss 1.0126189630607079 [40 / 50]
-------------------------------------
{'lr': 0.0018913833038606016, 'batch_size': 8, 'weight_decay': 5.89668380011213e-05, 'gamma': 0.084637176326284}
train_acc: 0.16934487021013597, val_acc: 0.1921182266009852, train_loss: 1.7750151895622093, val_loss: 1.7448274507898416 (1 / 100)
train_acc: 0.24598269468479605, val_acc: 0.21674876847290642, train_loss: 1.7525599531838567, val_loss: 1.7359889170219158 (2 / 100)
train_acc: 0.23362175525339926, val_acc: 0.29064039408866993, train_loss: 1.714251025792842, val_loss: 1.6599389297034353 (3 / 100)
train_acc: 0.30284301606922126, val_acc: 0.2315270935960591, train_loss: 1.6574396696609384, val_loss: 1.6731210900057714 (4 / 100)
train_acc: 0.32138442521631644, val_acc: 0.2413793103448276, train_loss: 1.6089349603476129, val_loss: 1.7960807510784693 (5 / 100)
train_acc: 0.33498145859085293, val_acc: 0.35467980295566504, train_loss: 1.5788720265601857, val_loss: 1.4845088802534958 (6 / 100)
train_acc: 0.34981458590852904, val_acc: 0.4187192118226601, train_loss: 1.4894833303941932, val_loss: 1.3812574071837176 (7 / 100)
train_acc: 0.39184177997527814, val_acc: 0.3842364532019704, train_loss: 1.4614764895515773, val_loss: 1.5032990853774724 (8 / 100)
train_acc: 0.3794808405438813, val_acc: 0.41379310344827586, train_loss: 1.4352128679584366, val_loss: 1.3070632480635431 (9 / 100)
train_acc: 0.3856613102595797, val_acc: 0.4187192118226601, train_loss: 1.3924042450630179, val_loss: 1.4560814910921558 (10 / 100)
train_acc: 0.4042027194066749, val_acc: 0.4630541871921182, train_loss: 1.357879305062689, val_loss: 1.281590154605546 (11 / 100)
train_acc: 0.4042027194066749, val_acc: 0.46798029556650245, train_loss: 1.3202442321258658, val_loss: 1.2154713654753022 (12 / 100)
train_acc: 0.449938195302843, val_acc: 0.45320197044334976, train_loss: 1.2768664993668075, val_loss: 1.2691027378213817 (13 / 100)
train_acc: 0.453646477132262, val_acc: 0.5024630541871922, train_loss: 1.268438860276721, val_loss: 1.2045447541575127 (14 / 100)
train_acc: 0.4758961681087763, val_acc: 0.4433497536945813, train_loss: 1.2144055674485723, val_loss: 1.2072861846444642 (15 / 100)
train_acc: 0.48825710754017304, val_acc: 0.5123152709359606, train_loss: 1.197907016952194, val_loss: 1.207284825776011 (16 / 100)
train_acc: 0.4907292954264524, val_acc: 0.4729064039408867, train_loss: 1.2115538152687628, val_loss: 1.188744686507239 (17 / 100)
train_acc: 0.5352286773794809, val_acc: 0.4827586206896552, train_loss: 1.093220256609734, val_loss: 1.1445357734933863 (18 / 100)
train_acc: 0.5723114956736712, val_acc: 0.5024630541871922, train_loss: 1.0969654544174892, val_loss: 1.2552196086921128 (19 / 100)
train_acc: 0.5723114956736712, val_acc: 0.5221674876847291, train_loss: 1.0375261247821144, val_loss: 1.0789385288953781 (20 / 100)
train_acc: 0.6032138442521632, val_acc: 0.5123152709359606, train_loss: 0.9885733982246503, val_loss: 1.1774929847036089 (21 / 100)
train_acc: 0.6378244746600742, val_acc: 0.4876847290640394, train_loss: 0.9093972266236106, val_loss: 1.1224044032872016 (22 / 100)
train_acc: 0.6489493201483313, val_acc: 0.5467980295566502, train_loss: 0.8588588900854914, val_loss: 1.1827002443703525 (23 / 100)
train_acc: 0.6736711990111248, val_acc: 0.5270935960591133, train_loss: 0.8299402689609598, val_loss: 1.086197062372574 (24 / 100)
train_acc: 0.7206427688504327, val_acc: 0.5369458128078818, train_loss: 0.7514730154068126, val_loss: 1.0647451807125448 (25 / 100)
train_acc: 0.7218788627935723, val_acc: 0.5270935960591133, train_loss: 0.6868021134540383, val_loss: 1.5233832052335363 (26 / 100)
train_acc: 0.7342398022249691, val_acc: 0.5467980295566502, train_loss: 0.6659082323128567, val_loss: 1.2772936245490765 (27 / 100)
train_acc: 0.7948084054388134, val_acc: 0.5862068965517241, train_loss: 0.5487139830630553, val_loss: 1.3226844718303588 (28 / 100)
overfit -> train_accuracy 0.796044499381953, val_accuracy 0.49261083743842365
lr 0.0018913833038606016, batch 8, decay 5.89668380011213e-05, gamma 0.084637176326284, val accuracy 0.5862068965517241, val loss 1.3226844718303588 [41 / 50]
-------------------------------------
{'lr': 0.0017196302161729305, 'batch_size': 8, 'weight_decay': 1.520841430397437e-06, 'gamma': 0.007813055945219683}
train_acc: 0.19283065512978986, val_acc: 0.2019704433497537, train_loss: 1.7778617977653948, val_loss: 1.7488957684615563 (1 / 100)
train_acc: 0.20642768850432633, val_acc: 0.28078817733990147, train_loss: 1.759759808500264, val_loss: 1.7278851822679266 (2 / 100)
train_acc: 0.23362175525339926, val_acc: 0.26108374384236455, train_loss: 1.7296869243473294, val_loss: 1.6448027788124648 (3 / 100)
train_acc: 0.2880098887515451, val_acc: 0.18226600985221675, train_loss: 1.6695594208496904, val_loss: 1.8716397173886228 (4 / 100)
train_acc: 0.2657601977750309, val_acc: 0.2413793103448276, train_loss: 1.705977798245009, val_loss: 1.6565668315723026 (5 / 100)
train_acc: 0.33127317676143386, val_acc: 0.3054187192118227, train_loss: 1.5793713555200433, val_loss: 1.556811219365726 (6 / 100)
train_acc: 0.33250927070457353, val_acc: 0.3694581280788177, train_loss: 1.5276930662256532, val_loss: 1.447451802310098 (7 / 100)
train_acc: 0.37824474660074164, val_acc: 0.37438423645320196, train_loss: 1.473970123804986, val_loss: 1.3840591678478447 (8 / 100)
train_acc: 0.38442521631644005, val_acc: 0.39408866995073893, train_loss: 1.4452354801894707, val_loss: 1.388618007669308 (9 / 100)
train_acc: 0.3757725587144623, val_acc: 0.3793103448275862, train_loss: 1.4214538210549372, val_loss: 1.4476944369635558 (10 / 100)
train_acc: 0.36093943139678614, val_acc: 0.35960591133004927, train_loss: 1.3955191737789159, val_loss: 1.361029843391456 (11 / 100)
train_acc: 0.39555006180469715, val_acc: 0.41379310344827586, train_loss: 1.388540208413368, val_loss: 1.3265232335170503 (12 / 100)
train_acc: 0.4326328800988875, val_acc: 0.3793103448275862, train_loss: 1.333200746472598, val_loss: 1.2459805575497631 (13 / 100)
train_acc: 0.41409147095179233, val_acc: 0.458128078817734, train_loss: 1.31422890087288, val_loss: 1.2804467449047294 (14 / 100)
train_acc: 0.4573547589616811, val_acc: 0.42857142857142855, train_loss: 1.275692404569006, val_loss: 1.2622628623041614 (15 / 100)
train_acc: 0.4215080346106304, val_acc: 0.43349753694581283, train_loss: 1.3450723106987839, val_loss: 1.2163941748623777 (16 / 100)
train_acc: 0.4610630407911001, val_acc: 0.3694581280788177, train_loss: 1.280478576794543, val_loss: 1.4277637580345417 (17 / 100)
train_acc: 0.45982694684796044, val_acc: 0.4482758620689655, train_loss: 1.240461587611176, val_loss: 1.2382302163856957 (18 / 100)
train_acc: 0.4796044499381953, val_acc: 0.5073891625615764, train_loss: 1.1773296439898175, val_loss: 1.1555710942874402 (19 / 100)
train_acc: 0.47095179233621753, val_acc: 0.47783251231527096, train_loss: 1.193972338262683, val_loss: 1.1980718532806547 (20 / 100)
train_acc: 0.5179233621755254, val_acc: 0.4482758620689655, train_loss: 1.1130817063217258, val_loss: 1.26426541893353 (21 / 100)
train_acc: 0.5253399258343634, val_acc: 0.45320197044334976, train_loss: 1.1090218617093872, val_loss: 1.253679218257002 (22 / 100)
train_acc: 0.5587144622991347, val_acc: 0.43842364532019706, train_loss: 1.0836670354505995, val_loss: 1.325804776158826 (23 / 100)
train_acc: 0.5710754017305315, val_acc: 0.4975369458128079, train_loss: 1.031985114030402, val_loss: 1.0836396681264115 (24 / 100)
train_acc: 0.5686032138442522, val_acc: 0.541871921182266, train_loss: 1.0299331098610744, val_loss: 1.1002167275386492 (25 / 100)
train_acc: 0.6007416563658838, val_acc: 0.5369458128078818, train_loss: 0.9545909244433617, val_loss: 1.100414811390374 (26 / 100)
train_acc: 0.6328800988875154, val_acc: 0.5270935960591133, train_loss: 0.9091806063988005, val_loss: 1.1142224690009808 (27 / 100)
train_acc: 0.6440049443757726, val_acc: 0.5911330049261084, train_loss: 0.8221703421494869, val_loss: 1.1255948305717243 (28 / 100)
train_acc: 0.6996291718170581, val_acc: 0.5960591133004927, train_loss: 0.7951780198088977, val_loss: 1.106999770614314 (29 / 100)
train_acc: 0.6983930778739185, val_acc: 0.5812807881773399, train_loss: 0.7569373549872777, val_loss: 1.035593778629021 (30 / 100)
train_acc: 0.7503090234857849, val_acc: 0.541871921182266, train_loss: 0.6303633149386336, val_loss: 1.3227792273601289 (31 / 100)
train_acc: 0.7194066749072929, val_acc: 0.5862068965517241, train_loss: 0.7444827359155907, val_loss: 1.0307769035470897 (32 / 100)
train_acc: 0.7873918417799752, val_acc: 0.5911330049261084, train_loss: 0.5241009723418133, val_loss: 1.2117582201370465 (33 / 100)
train_acc: 0.799752781211372, val_acc: 0.5517241379310345, train_loss: 0.4932143288872887, val_loss: 1.6565620197451174 (34 / 100)
train_acc: 0.8220024721878862, val_acc: 0.5862068965517241, train_loss: 0.4636421405488098, val_loss: 1.2870619907754983 (35 / 100)
overfit -> train_accuracy 0.8380716934487021, val_accuracy 0.5073891625615764
lr 0.0017196302161729305, batch 8, decay 1.520841430397437e-06, gamma 0.007813055945219683, val accuracy 0.5960591133004927, val loss 1.106999770614314 [42 / 50]
-------------------------------------
{'lr': 0.0008610189679800624, 'batch_size': 8, 'weight_decay': 2.1413334573846994e-06, 'gamma': 0.07584554151330321}
train_acc: 0.18788627935723115, val_acc: 0.18719211822660098, train_loss: 1.7821749326472525, val_loss: 1.7609829550306197 (1 / 100)
train_acc: 0.21137206427688504, val_acc: 0.270935960591133, train_loss: 1.7572968695750195, val_loss: 1.7400180794335351 (2 / 100)
train_acc: 0.22249690976514216, val_acc: 0.2561576354679803, train_loss: 1.7436647117506294, val_loss: 1.7053562238298614 (3 / 100)
train_acc: 0.2954264524103832, val_acc: 0.2413793103448276, train_loss: 1.6883377554390133, val_loss: 1.65552770034433 (4 / 100)
train_acc: 0.315203955500618, val_acc: 0.39408866995073893, train_loss: 1.6281543757918444, val_loss: 1.5503258305817402 (5 / 100)
train_acc: 0.32014833127317677, val_acc: 0.26108374384236455, train_loss: 1.592192106694904, val_loss: 1.607965057706598 (6 / 100)
train_acc: 0.3362175525339926, val_acc: 0.3054187192118227, train_loss: 1.5785610628658526, val_loss: 1.5206122556930692 (7 / 100)
train_acc: 0.3646477132262052, val_acc: 0.4187192118226601, train_loss: 1.5054939510795773, val_loss: 1.4055118369938704 (8 / 100)
train_acc: 0.34857849196538937, val_acc: 0.30049261083743845, train_loss: 1.4976497872356136, val_loss: 1.5223224151310661 (9 / 100)
train_acc: 0.3683559950556242, val_acc: 0.3842364532019704, train_loss: 1.444095896554671, val_loss: 1.3605383663929154 (10 / 100)
train_acc: 0.37330037082818296, val_acc: 0.43842364532019706, train_loss: 1.4381498475599348, val_loss: 1.359854085692044 (11 / 100)
train_acc: 0.3967861557478368, val_acc: 0.4187192118226601, train_loss: 1.4088289616871235, val_loss: 1.3220230857726976 (12 / 100)
train_acc: 0.42027194066749074, val_acc: 0.4088669950738916, train_loss: 1.4076913517102028, val_loss: 1.4112942852997428 (13 / 100)
train_acc: 0.39555006180469715, val_acc: 0.42857142857142855, train_loss: 1.378036729484907, val_loss: 1.2708609767735297 (14 / 100)
train_acc: 0.4363411619283066, val_acc: 0.3891625615763547, train_loss: 1.3444506273281442, val_loss: 1.4452754728899802 (15 / 100)
train_acc: 0.44128553770086526, val_acc: 0.4482758620689655, train_loss: 1.30872468747963, val_loss: 1.4731505210763716 (16 / 100)
train_acc: 0.415327564894932, val_acc: 0.42857142857142855, train_loss: 1.3013673426931072, val_loss: 1.3463992003736824 (17 / 100)
train_acc: 0.453646477132262, val_acc: 0.4433497536945813, train_loss: 1.2609589922118687, val_loss: 1.2971125935098808 (18 / 100)
train_acc: 0.4684796044499382, val_acc: 0.458128078817734, train_loss: 1.2637361354379926, val_loss: 1.260881113888595 (19 / 100)
train_acc: 0.4684796044499382, val_acc: 0.46798029556650245, train_loss: 1.2558693205028293, val_loss: 1.2058877219707507 (20 / 100)
train_acc: 0.4969097651421508, val_acc: 0.5024630541871922, train_loss: 1.1862015482521764, val_loss: 1.1978131967225099 (21 / 100)
train_acc: 0.4894932014833127, val_acc: 0.4630541871921182, train_loss: 1.2039658286515509, val_loss: 1.3973325015288856 (22 / 100)
train_acc: 0.5055624227441285, val_acc: 0.4039408866995074, train_loss: 1.1729166805375786, val_loss: 1.5108995525707751 (23 / 100)
train_acc: 0.5302843016069221, val_acc: 0.4482758620689655, train_loss: 1.150629502733794, val_loss: 1.1955418830434676 (24 / 100)
train_acc: 0.5352286773794809, val_acc: 0.46798029556650245, train_loss: 1.107567374579544, val_loss: 1.2086637554497555 (25 / 100)
train_acc: 0.5475896168108776, val_acc: 0.5369458128078818, train_loss: 1.0903621875164093, val_loss: 1.0587356216801798 (26 / 100)
train_acc: 0.5784919653893696, val_acc: 0.5517241379310345, train_loss: 1.0330838163939335, val_loss: 1.1609301426140546 (27 / 100)
train_acc: 0.595797280593325, val_acc: 0.5566502463054187, train_loss: 1.0161366368401037, val_loss: 1.0846887348320684 (28 / 100)
train_acc: 0.584672435105068, val_acc: 0.5221674876847291, train_loss: 1.0329842384725036, val_loss: 1.1081622859527325 (29 / 100)
train_acc: 0.6341161928306551, val_acc: 0.5221674876847291, train_loss: 0.9411986384020453, val_loss: 1.192675194716806 (30 / 100)
train_acc: 0.6180469715698393, val_acc: 0.5270935960591133, train_loss: 0.9669855784426808, val_loss: 1.1558845065203793 (31 / 100)
train_acc: 0.6625463535228677, val_acc: 0.5763546798029556, train_loss: 0.87526017125369, val_loss: 1.024596460934343 (32 / 100)
train_acc: 0.6946847960444994, val_acc: 0.5665024630541872, train_loss: 0.777375674070917, val_loss: 1.128297331885164 (33 / 100)
train_acc: 0.7144622991347342, val_acc: 0.5911330049261084, train_loss: 0.7327776910053343, val_loss: 1.1081045309604682 (34 / 100)
train_acc: 0.7095179233621756, val_acc: 0.6059113300492611, train_loss: 0.7335100185738506, val_loss: 1.048625008813266 (35 / 100)
train_acc: 0.7255871446229913, val_acc: 0.5615763546798029, train_loss: 0.7026395175896115, val_loss: 1.0303097050178227 (36 / 100)
train_acc: 0.7750309023485785, val_acc: 0.5812807881773399, train_loss: 0.5992419343943649, val_loss: 1.4281821767684861 (37 / 100)
train_acc: 0.7713226205191595, val_acc: 0.5270935960591133, train_loss: 0.6064940265141547, val_loss: 1.8780059098022912 (38 / 100)
train_acc: 0.826946847960445, val_acc: 0.6305418719211823, train_loss: 0.47149875815336134, val_loss: 1.2015113267552089 (39 / 100)
train_acc: 0.8108776266996292, val_acc: 0.6650246305418719, train_loss: 0.4933553244481128, val_loss: 1.0629929791530366 (40 / 100)
train_acc: 0.8182941903584673, val_acc: 0.5714285714285714, train_loss: 0.46194235120332433, val_loss: 1.3595457000685442 (41 / 100)
train_acc: 0.8800988875154512, val_acc: 0.6403940886699507, train_loss: 0.35879145313989097, val_loss: 1.3561633428916555 (42 / 100)
overfit -> train_accuracy 0.8442521631644005, val_accuracy 0.5566502463054187
lr 0.0008610189679800624, batch 8, decay 2.1413334573846994e-06, gamma 0.07584554151330321, val accuracy 0.6650246305418719, val loss 1.0629929791530366 [43 / 50]
-------------------------------------
{'lr': 0.0013136319896644864, 'batch_size': 8, 'weight_decay': 7.335117679350525e-05, 'gamma': 0.033603889650815046}
train_acc: 0.20148331273176762, val_acc: 0.18226600985221675, train_loss: 1.7784024319336353, val_loss: 1.7589500837138135 (1 / 100)
train_acc: 0.18912237330037082, val_acc: 0.3399014778325123, train_loss: 1.758226152845602, val_loss: 1.7439550725110058 (2 / 100)
train_acc: 0.26823238566131025, val_acc: 0.26108374384236455, train_loss: 1.7219341738409695, val_loss: 1.6979063954846612 (3 / 100)
train_acc: 0.29171817058096416, val_acc: 0.2857142857142857, train_loss: 1.6426102161702179, val_loss: 1.6186570939172078 (4 / 100)
train_acc: 0.3263288009888752, val_acc: 0.35467980295566504, train_loss: 1.61431383217221, val_loss: 1.5198056821165413 (5 / 100)
train_acc: 0.3226205191594561, val_acc: 0.35467980295566504, train_loss: 1.5377367191173237, val_loss: 1.5417394567592977 (6 / 100)
train_acc: 0.3473423980222497, val_acc: 0.3645320197044335, train_loss: 1.5035075968953382, val_loss: 1.4272516930631816 (7 / 100)
train_acc: 0.3572311495673671, val_acc: 0.3694581280788177, train_loss: 1.4826716041682824, val_loss: 1.4484722490968376 (8 / 100)
train_acc: 0.3646477132262052, val_acc: 0.37438423645320196, train_loss: 1.4404230113377234, val_loss: 1.3645279883163903 (9 / 100)
train_acc: 0.3683559950556242, val_acc: 0.4039408866995074, train_loss: 1.4496048643208848, val_loss: 1.3057490687064937 (10 / 100)
train_acc: 0.3992583436341162, val_acc: 0.42857142857142855, train_loss: 1.3844177650727507, val_loss: 1.3516689638786128 (11 / 100)
train_acc: 0.3980222496909765, val_acc: 0.39408866995073893, train_loss: 1.3558233793645325, val_loss: 1.2805456944874354 (12 / 100)
train_acc: 0.3856613102595797, val_acc: 0.4433497536945813, train_loss: 1.3865808840882499, val_loss: 1.2586872483709175 (13 / 100)
train_acc: 0.4647713226205192, val_acc: 0.42857142857142855, train_loss: 1.2994266661489555, val_loss: 1.2405844659640872 (14 / 100)
train_acc: 0.42398022249690975, val_acc: 0.42857142857142855, train_loss: 1.3027233107865077, val_loss: 1.2187087788370443 (15 / 100)
train_acc: 0.4400494437577256, val_acc: 0.3842364532019704, train_loss: 1.2845627975699634, val_loss: 1.2841926889466535 (16 / 100)
train_acc: 0.4622991347342398, val_acc: 0.4630541871921182, train_loss: 1.246273084535587, val_loss: 1.2764884858883072 (17 / 100)
train_acc: 0.4672435105067985, val_acc: 0.3793103448275862, train_loss: 1.2466051020050521, val_loss: 1.2801182631201344 (18 / 100)
train_acc: 0.49443757725587145, val_acc: 0.42857142857142855, train_loss: 1.2130893147625352, val_loss: 1.2548282202241456 (19 / 100)
train_acc: 0.49938195302843014, val_acc: 0.4729064039408867, train_loss: 1.2124090091554431, val_loss: 1.2104052915948953 (20 / 100)
train_acc: 0.4796044499381953, val_acc: 0.4729064039408867, train_loss: 1.2166191441166976, val_loss: 1.2814519041277506 (21 / 100)
train_acc: 0.5092707045735476, val_acc: 0.5123152709359606, train_loss: 1.1570603375971245, val_loss: 1.133450795570618 (22 / 100)
train_acc: 0.5018541409147095, val_acc: 0.4630541871921182, train_loss: 1.2185369920082234, val_loss: 1.1537105103431664 (23 / 100)
train_acc: 0.5327564894932015, val_acc: 0.5172413793103449, train_loss: 1.1149969466389773, val_loss: 1.253949532661532 (24 / 100)
train_acc: 0.5216316440049443, val_acc: 0.46798029556650245, train_loss: 1.1078619570903048, val_loss: 1.1288728526073137 (25 / 100)
train_acc: 0.5661310259579728, val_acc: 0.5369458128078818, train_loss: 1.017489254400963, val_loss: 1.1515445063266847 (26 / 100)
train_acc: 0.5735475896168108, val_acc: 0.4975369458128079, train_loss: 1.0480427176165492, val_loss: 1.2323259309007617 (27 / 100)
train_acc: 0.619283065512979, val_acc: 0.5270935960591133, train_loss: 0.9065932013343085, val_loss: 1.204405016499787 (28 / 100)
train_acc: 0.6118665018541409, val_acc: 0.5172413793103449, train_loss: 0.9512589069173126, val_loss: 1.1160697314539567 (29 / 100)
train_acc: 0.6341161928306551, val_acc: 0.5517241379310345, train_loss: 0.8892134942289336, val_loss: 1.115690739871246 (30 / 100)
train_acc: 0.6378244746600742, val_acc: 0.5911330049261084, train_loss: 0.8638361590754411, val_loss: 1.0159901612497904 (31 / 100)
train_acc: 0.6909765142150803, val_acc: 0.6059113300492611, train_loss: 0.7637918667386577, val_loss: 1.047570731252285 (32 / 100)
train_acc: 0.7194066749072929, val_acc: 0.5467980295566502, train_loss: 0.7328761027681518, val_loss: 1.1989511260258152 (33 / 100)
train_acc: 0.7367119901112484, val_acc: 0.5123152709359606, train_loss: 0.6639482926379323, val_loss: 1.3052466987389062 (34 / 100)
train_acc: 0.7169344870210136, val_acc: 0.5615763546798029, train_loss: 0.6932140789161654, val_loss: 1.0825547361608796 (35 / 100)
train_acc: 0.7379480840543882, val_acc: 0.5320197044334976, train_loss: 0.6341016098064899, val_loss: 1.2217549144340853 (36 / 100)
train_acc: 0.7639060568603214, val_acc: 0.5960591133004927, train_loss: 0.6149079616933287, val_loss: 1.3259940866766304 (37 / 100)
train_acc: 0.757725587144623, val_acc: 0.5665024630541872, train_loss: 0.6262731082065144, val_loss: 1.4774858857610542 (38 / 100)
train_acc: 0.8121137206427689, val_acc: 0.5960591133004927, train_loss: 0.49505545432841674, val_loss: 1.2585605880603414 (39 / 100)
train_acc: 0.8405438813349815, val_acc: 0.6059113300492611, train_loss: 0.43004962731940194, val_loss: 1.1677084224564689 (40 / 100)
overfit -> train_accuracy 0.861557478368356, val_accuracy 0.5812807881773399
lr 0.0013136319896644864, batch 8, decay 7.335117679350525e-05, gamma 0.033603889650815046, val accuracy 0.6059113300492611, val loss 1.047570731252285 [44 / 50]
-------------------------------------
{'lr': 0.002418532167486425, 'batch_size': 8, 'weight_decay': 3.4418498736700045e-06, 'gamma': 0.029359326979210655}
train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7714778685893942, val_loss: 1.7712837146420783 (1 / 100)
train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.7668105521514477, val_loss: 1.7425868734350345 (2 / 100)
train_acc: 0.20395550061804696, val_acc: 0.29064039408866993, train_loss: 1.7447011585730705, val_loss: 1.716130348849179 (3 / 100)
train_acc: 0.2880098887515451, val_acc: 0.35960591133004927, train_loss: 1.6672502692756606, val_loss: 1.5122236154349566 (4 / 100)
train_acc: 0.3016069221260816, val_acc: 0.3497536945812808, train_loss: 1.6102423485188904, val_loss: 1.503253415887579 (5 / 100)
train_acc: 0.3300370828182942, val_acc: 0.3694581280788177, train_loss: 1.5389648696548122, val_loss: 1.4950246476187494 (6 / 100)
train_acc: 0.3337453646477132, val_acc: 0.35960591133004927, train_loss: 1.4988048447961417, val_loss: 1.3649654476513415 (7 / 100)
train_acc: 0.3683559950556242, val_acc: 0.33004926108374383, train_loss: 1.4530159121538122, val_loss: 1.5540369602259745 (8 / 100)
train_acc: 0.3819530284301607, val_acc: 0.3891625615763547, train_loss: 1.3796397625884256, val_loss: 1.3194807838336589 (9 / 100)
train_acc: 0.38442521631644005, val_acc: 0.3842364532019704, train_loss: 1.3927700587344554, val_loss: 1.3379435850481682 (10 / 100)
train_acc: 0.3868974042027194, val_acc: 0.3694581280788177, train_loss: 1.3369476976441808, val_loss: 1.4820163026819089 (11 / 100)
train_acc: 0.380716934487021, val_acc: 0.39408866995073893, train_loss: 1.3583537514041764, val_loss: 1.309588976681526 (12 / 100)
train_acc: 0.4042027194066749, val_acc: 0.43349753694581283, train_loss: 1.337569374089188, val_loss: 1.2825355418209958 (13 / 100)
train_acc: 0.4363411619283066, val_acc: 0.4039408866995074, train_loss: 1.2934986470803638, val_loss: 1.4290586181462104 (14 / 100)
train_acc: 0.43510506798516685, val_acc: 0.41379310344827586, train_loss: 1.2832749015173894, val_loss: 1.3259135666739177 (15 / 100)
train_acc: 0.4400494437577256, val_acc: 0.4729064039408867, train_loss: 1.2698144417610686, val_loss: 1.2004314097277637 (16 / 100)
train_acc: 0.46600741656365885, val_acc: 0.49261083743842365, train_loss: 1.1908840493601833, val_loss: 1.1716838578872493 (17 / 100)
train_acc: 0.453646477132262, val_acc: 0.46798029556650245, train_loss: 1.238415686250175, val_loss: 1.2539322270548403 (18 / 100)
train_acc: 0.5043263288009888, val_acc: 0.42857142857142855, train_loss: 1.1393637477393792, val_loss: 1.2031029310132482 (19 / 100)
train_acc: 0.5067985166872683, val_acc: 0.5024630541871922, train_loss: 1.0818719604548772, val_loss: 1.3772007916948479 (20 / 100)
train_acc: 0.5253399258343634, val_acc: 0.4876847290640394, train_loss: 1.0864358723384635, val_loss: 1.2160646938925306 (21 / 100)
train_acc: 0.5574783683559951, val_acc: 0.47783251231527096, train_loss: 1.060904576250885, val_loss: 1.2680296099244668 (22 / 100)
train_acc: 0.5896168108776267, val_acc: 0.458128078817734, train_loss: 0.9644801976212171, val_loss: 1.449108844907413 (23 / 100)
train_acc: 0.5500618046971569, val_acc: 0.46798029556650245, train_loss: 1.0895152077244592, val_loss: 1.429335335792579 (24 / 100)
train_acc: 0.5982694684796045, val_acc: 0.5320197044334976, train_loss: 0.9526834095953716, val_loss: 1.1798099981152952 (25 / 100)
train_acc: 0.61557478368356, val_acc: 0.5763546798029556, train_loss: 0.8825559209392156, val_loss: 1.0832900399057737 (26 / 100)
train_acc: 0.6328800988875154, val_acc: 0.46798029556650245, train_loss: 0.8865213927586088, val_loss: 1.5972417070360607 (27 / 100)
train_acc: 0.6847960444993819, val_acc: 0.5221674876847291, train_loss: 0.8012441029802093, val_loss: 1.2546839893157846 (28 / 100)
train_acc: 0.69221260815822, val_acc: 0.5566502463054187, train_loss: 0.7670496417801224, val_loss: 1.264289876216738 (29 / 100)
train_acc: 0.7354758961681088, val_acc: 0.541871921182266, train_loss: 0.6525855702434393, val_loss: 1.2319359168630515 (30 / 100)
train_acc: 0.7206427688504327, val_acc: 0.5714285714285714, train_loss: 0.6907697755415154, val_loss: 1.2455103782010195 (31 / 100)
train_acc: 0.7379480840543882, val_acc: 0.5369458128078818, train_loss: 0.6287451217731528, val_loss: 2.0110208213035694 (32 / 100)
train_acc: 0.7688504326328801, val_acc: 0.5221674876847291, train_loss: 0.5591974850785453, val_loss: 1.429034139135201 (33 / 100)
overfit -> train_accuracy 0.792336217552534, val_accuracy 0.5320197044334976
lr 0.002418532167486425, batch 8, decay 3.4418498736700045e-06, gamma 0.029359326979210655, val accuracy 0.5763546798029556, val loss 1.0832900399057737 [45 / 50]
-------------------------------------
{'lr': 0.0027401329528807215, 'batch_size': 8, 'weight_decay': 4.574089738170917e-06, 'gamma': 0.0969937699270124}
train_acc: 0.1508034610630408, val_acc: 0.18226600985221675, train_loss: 1.7796499359298252, val_loss: 1.7448198061271254 (1 / 100)
train_acc: 0.20395550061804696, val_acc: 0.33497536945812806, train_loss: 1.7436489824606256, val_loss: 1.6985228713510072 (2 / 100)
train_acc: 0.23980222496909764, val_acc: 0.31527093596059114, train_loss: 1.7389313489456555, val_loss: 1.6711364420763966 (3 / 100)
train_acc: 0.273176761433869, val_acc: 0.35467980295566504, train_loss: 1.6559082822245632, val_loss: 1.5633805561535463 (4 / 100)
train_acc: 0.32509270704573545, val_acc: 0.33497536945812806, train_loss: 1.597361697549431, val_loss: 1.55285084834827 (5 / 100)
train_acc: 0.31396786155747836, val_acc: 0.3399014778325123, train_loss: 1.6167088596723578, val_loss: 1.5144856184574182 (6 / 100)
train_acc: 0.3362175525339926, val_acc: 0.33497536945812806, train_loss: 1.548833646202559, val_loss: 1.5144456718942803 (7 / 100)
train_acc: 0.3646477132262052, val_acc: 0.37438423645320196, train_loss: 1.491122516625007, val_loss: 1.4645487269744497 (8 / 100)
train_acc: 0.380716934487021, val_acc: 0.3251231527093596, train_loss: 1.4387923972715702, val_loss: 1.7245884992806195 (9 / 100)
train_acc: 0.3831891223733004, val_acc: 0.3891625615763547, train_loss: 1.3996060523762839, val_loss: 1.3522099984690474 (10 / 100)
train_acc: 0.3856613102595797, val_acc: 0.4187192118226601, train_loss: 1.4019426061726914, val_loss: 1.268172109068321 (11 / 100)
train_acc: 0.3980222496909765, val_acc: 0.39901477832512317, train_loss: 1.3888667194156623, val_loss: 1.3554345815639777 (12 / 100)
train_acc: 0.3992583436341162, val_acc: 0.42857142857142855, train_loss: 1.3722609645504003, val_loss: 1.3032261896603212 (13 / 100)
train_acc: 0.44128553770086526, val_acc: 0.45320197044334976, train_loss: 1.2860107345250982, val_loss: 1.240556860204988 (14 / 100)
train_acc: 0.4635352286773795, val_acc: 0.46798029556650245, train_loss: 1.274883697589926, val_loss: 1.3181905787566612 (15 / 100)
train_acc: 0.4437577255871446, val_acc: 0.4827586206896552, train_loss: 1.2558777447241936, val_loss: 1.2679791330116723 (16 / 100)
train_acc: 0.5018541409147095, val_acc: 0.4482758620689655, train_loss: 1.2018839355157538, val_loss: 1.1869446943546165 (17 / 100)
train_acc: 0.5129789864029666, val_acc: 0.4975369458128079, train_loss: 1.121435906300586, val_loss: 1.3155757596927324 (18 / 100)
train_acc: 0.5451174289245982, val_acc: 0.5517241379310345, train_loss: 1.1024764238388194, val_loss: 1.1348706827375101 (19 / 100)
train_acc: 0.5661310259579728, val_acc: 0.5123152709359606, train_loss: 1.0821519835180935, val_loss: 1.085290311592553 (20 / 100)
train_acc: 0.5797280593325093, val_acc: 0.5172413793103449, train_loss: 1.0158889228834946, val_loss: 1.4774548264559855 (21 / 100)
train_acc: 0.5834363411619283, val_acc: 0.5073891625615764, train_loss: 0.994300257879665, val_loss: 1.4932628969840815 (22 / 100)
train_acc: 0.619283065512979, val_acc: 0.5320197044334976, train_loss: 0.9359382622321545, val_loss: 1.319920657303533 (23 / 100)
train_acc: 0.6341161928306551, val_acc: 0.5615763546798029, train_loss: 0.9166947150849294, val_loss: 1.2594706049106392 (24 / 100)
train_acc: 0.6637824474660075, val_acc: 0.4975369458128079, train_loss: 0.8501913017041899, val_loss: 1.912716381655538 (25 / 100)
train_acc: 0.6724351050679852, val_acc: 0.5221674876847291, train_loss: 0.8160555598467625, val_loss: 1.2699215359288483 (26 / 100)
train_acc: 0.6971569839307787, val_acc: 0.4975369458128079, train_loss: 0.7081464910684027, val_loss: 1.462568657151584 (27 / 100)
train_acc: 0.7268232385661311, val_acc: 0.5665024630541872, train_loss: 0.6961999506826602, val_loss: 1.8526587838609818 (28 / 100)
train_acc: 0.7564894932014833, val_acc: 0.5862068965517241, train_loss: 0.6462298952899551, val_loss: 1.405506047121997 (29 / 100)
train_acc: 0.7750309023485785, val_acc: 0.6108374384236454, train_loss: 0.5738927491368411, val_loss: 1.3450364400013326 (30 / 100)
train_acc: 0.8059332509270705, val_acc: 0.6305418719211823, train_loss: 0.5206958194008864, val_loss: 0.9770445293687248 (31 / 100)
train_acc: 0.8046971569839307, val_acc: 0.5812807881773399, train_loss: 0.5014817775106253, val_loss: 1.283647236565651 (32 / 100)
train_acc: 0.8430160692212608, val_acc: 0.6059113300492611, train_loss: 0.40764075362932845, val_loss: 1.1234416087419528 (33 / 100)
overfit -> train_accuracy 0.8640296662546354, val_accuracy 0.6108374384236454
lr 0.0027401329528807215, batch 8, decay 4.574089738170917e-06, gamma 0.0969937699270124, val accuracy 0.6305418719211823, val loss 0.9770445293687248 [46 / 50]
-------------------------------------
{'lr': 0.002626994292497959, 'batch_size': 8, 'weight_decay': 8.728449923604833e-05, 'gamma': 0.023460541493164233}
train_acc: 0.17058096415327564, val_acc: 0.18226600985221675, train_loss: 1.7810735027633875, val_loss: 1.7675186724498355 (1 / 100)
train_acc: 0.19901112484548825, val_acc: 0.18719211822660098, train_loss: 1.7512782722232663, val_loss: 1.6965514069120284 (2 / 100)
train_acc: 0.25339925834363414, val_acc: 0.18226600985221675, train_loss: 1.7154928930610307, val_loss: 1.7632878325842871 (3 / 100)
train_acc: 0.24721878862793573, val_acc: 0.270935960591133, train_loss: 1.742037029584642, val_loss: 1.7048901290141891 (4 / 100)
train_acc: 0.315203955500618, val_acc: 0.3793103448275862, train_loss: 1.6069437375911528, val_loss: 1.4667782613209315 (5 / 100)
train_acc: 0.3053152039555006, val_acc: 0.35960591133004927, train_loss: 1.6038084369065293, val_loss: 1.6063685428920051 (6 / 100)
train_acc: 0.32756489493201485, val_acc: 0.3645320197044335, train_loss: 1.5129605987015997, val_loss: 1.4697149005429497 (7 / 100)
train_acc: 0.3399258343634116, val_acc: 0.3399014778325123, train_loss: 1.5009341180987648, val_loss: 1.3668061371507316 (8 / 100)
train_acc: 0.3695920889987639, val_acc: 0.3645320197044335, train_loss: 1.4480472244644638, val_loss: 1.4735751868468787 (9 / 100)
train_acc: 0.35970333745364647, val_acc: 0.3842364532019704, train_loss: 1.4638287338839178, val_loss: 1.3387142701689245 (10 / 100)
train_acc: 0.3720642768850433, val_acc: 0.4039408866995074, train_loss: 1.403479098684266, val_loss: 1.400649491202068 (11 / 100)
train_acc: 0.3980222496909765, val_acc: 0.3891625615763547, train_loss: 1.3486257016437753, val_loss: 1.3190676241085446 (12 / 100)
train_acc: 0.40914709517923364, val_acc: 0.39901477832512317, train_loss: 1.348143470596766, val_loss: 1.330810279681765 (13 / 100)