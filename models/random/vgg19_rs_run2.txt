training set 809
validation set 203
-------------------------------------
train_acc: 0.19777503090234858, val_acc: 0.18226600985221675, train_loss: 1.7723115554404052, val_loss: 1.7440218244280135 (1 / 15)
train_acc: 0.2360939431396786, val_acc: 0.18719211822660098, train_loss: 1.7274242402301878, val_loss: 1.7180973103480974 (2 / 15)
train_acc: 0.2546353522867738, val_acc: 0.3103448275862069, train_loss: 1.7043239914147903, val_loss: 1.6208021129880632 (3 / 15)
train_acc: 0.29666254635352285, val_acc: 0.26108374384236455, train_loss: 1.6676862401926915, val_loss: 1.7623347995316454 (4 / 15)
train_acc: 0.33498145859085293, val_acc: 0.3054187192118227, train_loss: 1.5920371075053739, val_loss: 1.6719598470650283 (5 / 15)
train_acc: 0.3720642768850433, val_acc: 0.3103448275862069, train_loss: 1.5194394223032834, val_loss: 1.594145939267915 (6 / 15)
train_acc: 0.36711990111248455, val_acc: 0.3793103448275862, train_loss: 1.480719683638904, val_loss: 1.5093475873834394 (7 / 15)
train_acc: 0.3683559950556242, val_acc: 0.3793103448275862, train_loss: 1.4687112297204281, val_loss: 1.4731666113942714 (8 / 15)
train_acc: 0.38813349814585907, val_acc: 0.37438423645320196, train_loss: 1.4602812954462945, val_loss: 1.4162046374945805 (9 / 15)
train_acc: 0.3868974042027194, val_acc: 0.43842364532019706, train_loss: 1.4136538266840912, val_loss: 1.362737141219266 (10 / 15)
train_acc: 0.39184177997527814, val_acc: 0.4088669950738916, train_loss: 1.4156477999480899, val_loss: 1.532630872843888 (11 / 15)
train_acc: 0.3992583436341162, val_acc: 0.39901477832512317, train_loss: 1.3559425145350812, val_loss: 1.3708913763755648 (12 / 15)
train_acc: 0.453646477132262, val_acc: 0.39408866995073893, train_loss: 1.3147829822615729, val_loss: 1.2902919835057751 (13 / 15)
train_acc: 0.4437577255871446, val_acc: 0.41379310344827586, train_loss: 1.2939510021280447, val_loss: 1.307271401283189 (14 / 15)
train_acc: 0.453646477132262, val_acc: 0.42857142857142855, train_loss: 1.2855517089145883, val_loss: 1.2681833661248532 (15 / 15)



lr 0.0007883731073995129, batch 8, decay 0, gamma 1, val accuracy 0.43842364532019706, val loss 1.362737141219266 [1 / 20]
-------------------------------------
train_acc: 0.17552533992583436, val_acc: 0.18719211822660098, train_loss: 1.7872223392847295, val_loss: 1.7759410506986044 (1 / 15)
train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.7639759219031694, val_loss: 1.7439085844115083 (2 / 15)
train_acc: 0.2200247218788628, val_acc: 0.18226600985221675, train_loss: 1.7399227006178999, val_loss: 1.72476995402369 (3 / 15)
train_acc: 0.25092707045735474, val_acc: 0.3054187192118227, train_loss: 1.6916532637309674, val_loss: 1.6534834207572373 (4 / 15)
train_acc: 0.2941903584672435, val_acc: 0.2955665024630542, train_loss: 1.6381161013993404, val_loss: 1.587385995634671 (5 / 15)
train_acc: 0.34610630407911, val_acc: 0.3103448275862069, train_loss: 1.5399189322486944, val_loss: 1.5750798861968693 (6 / 15)
train_acc: 0.36093943139678614, val_acc: 0.3793103448275862, train_loss: 1.5107434212940438, val_loss: 1.4049450957716392 (7 / 15)
train_acc: 0.34981458590852904, val_acc: 0.35960591133004927, train_loss: 1.513283186701526, val_loss: 1.4309801685399022 (8 / 15)
train_acc: 0.35970333745364647, val_acc: 0.39901477832512317, train_loss: 1.4401004827214112, val_loss: 1.3856150593076433 (9 / 15)
train_acc: 0.37824474660074164, val_acc: 0.4088669950738916, train_loss: 1.4136750703983756, val_loss: 1.372639843395778 (10 / 15)
train_acc: 0.3794808405438813, val_acc: 0.3793103448275862, train_loss: 1.4165437918510955, val_loss: 1.3472238003913992 (11 / 15)
train_acc: 0.4103831891223733, val_acc: 0.4039408866995074, train_loss: 1.3444089270640067, val_loss: 1.3328622098039524 (12 / 15)
train_acc: 0.41409147095179233, val_acc: 0.3694581280788177, train_loss: 1.3259508736496066, val_loss: 1.4130982524655722 (13 / 15)
train_acc: 0.4054388133498146, val_acc: 0.39408866995073893, train_loss: 1.3498584345922482, val_loss: 1.298930946242046 (14 / 15)
train_acc: 0.4388133498145859, val_acc: 0.3694581280788177, train_loss: 1.318158031394072, val_loss: 1.398875721569719 (15 / 15)



lr 0.0005667922593213116, batch 8, decay 0, gamma 1, val accuracy 0.4088669950738916, val loss 1.372639843395778 [2 / 20]
-------------------------------------
train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.785928226813987, val_loss: 1.7765690287932974 (1 / 15)
train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7657802695839013, val_loss: 1.751198975910694 (2 / 15)
train_acc: 0.2138442521631644, val_acc: 0.18226600985221675, train_loss: 1.7536626735930094, val_loss: 1.7316069808499566 (3 / 15)
train_acc: 0.2249690976514215, val_acc: 0.35467980295566504, train_loss: 1.722386956951674, val_loss: 1.6971176616076766 (4 / 15)
train_acc: 0.2558714462299135, val_acc: 0.2955665024630542, train_loss: 1.6900094601662994, val_loss: 1.6723722301680466 (5 / 15)
train_acc: 0.32014833127317677, val_acc: 0.3694581280788177, train_loss: 1.6658838328678616, val_loss: 1.6459829020382735 (6 / 15)
train_acc: 0.34239802224969096, val_acc: 0.35467980295566504, train_loss: 1.5940092906668986, val_loss: 1.5932028452163847 (7 / 15)
train_acc: 0.3547589616810878, val_acc: 0.39901477832512317, train_loss: 1.5550283091914079, val_loss: 1.5314890480981085 (8 / 15)
train_acc: 0.380716934487021, val_acc: 0.3448275862068966, train_loss: 1.5256850018047432, val_loss: 1.4896454905054253 (9 / 15)
train_acc: 0.3572311495673671, val_acc: 0.2955665024630542, train_loss: 1.523291153725057, val_loss: 1.5627568854487002 (10 / 15)
train_acc: 0.37082818294190356, val_acc: 0.3448275862068966, train_loss: 1.4695889049466373, val_loss: 1.451044753854498 (11 / 15)
train_acc: 0.4054388133498146, val_acc: 0.3891625615763547, train_loss: 1.441672093346623, val_loss: 1.3641360710407127 (12 / 15)
train_acc: 0.4079110012360939, val_acc: 0.43349753694581283, train_loss: 1.3756786875141271, val_loss: 1.3287152556950235 (13 / 15)
train_acc: 0.4276885043263288, val_acc: 0.4433497536945813, train_loss: 1.3350088313424544, val_loss: 1.3379772707746533 (14 / 15)
train_acc: 0.4079110012360939, val_acc: 0.3399014778325123, train_loss: 1.345632586402563, val_loss: 1.5063065795475625 (15 / 15)



lr 0.0005590540157805591, batch 8, decay 0, gamma 1, val accuracy 0.4433497536945813, val loss 1.3379772707746533 [3 / 20]
-------------------------------------
train_acc: 0.17181705809641531, val_acc: 0.18226600985221675, train_loss: 1.783018684799798, val_loss: 1.7609128899174957 (1 / 15)
train_acc: 0.19777503090234858, val_acc: 0.2660098522167488, train_loss: 1.7547198108159126, val_loss: 1.7403342858911148 (2 / 15)
train_acc: 0.20395550061804696, val_acc: 0.21674876847290642, train_loss: 1.7443234291300638, val_loss: 1.7563565523166376 (3 / 15)
train_acc: 0.2323856613102596, val_acc: 0.2315270935960591, train_loss: 1.7200284496373388, val_loss: 1.7223846836043109 (4 / 15)
train_acc: 0.29295426452410384, val_acc: 0.2315270935960591, train_loss: 1.669281932862639, val_loss: 1.6701655716731632 (5 / 15)
train_acc: 0.3053152039555006, val_acc: 0.3448275862068966, train_loss: 1.6375703628926697, val_loss: 1.5614814358979023 (6 / 15)
train_acc: 0.34363411619283063, val_acc: 0.3645320197044335, train_loss: 1.5690897849051118, val_loss: 1.52539449665934 (7 / 15)
train_acc: 0.34610630407911, val_acc: 0.3793103448275862, train_loss: 1.5655572311250476, val_loss: 1.5160068273544312 (8 / 15)
train_acc: 0.3646477132262052, val_acc: 0.37438423645320196, train_loss: 1.497633520103945, val_loss: 1.409417387299937 (9 / 15)
train_acc: 0.37824474660074164, val_acc: 0.3448275862068966, train_loss: 1.462535589675526, val_loss: 1.4428818754374688 (10 / 15)
train_acc: 0.3980222496909765, val_acc: 0.4236453201970443, train_loss: 1.3948638616003124, val_loss: 1.3593352956724871 (11 / 15)
train_acc: 0.39555006180469715, val_acc: 0.3891625615763547, train_loss: 1.4046069455824617, val_loss: 1.370243338528525 (12 / 15)
train_acc: 0.41285537700865266, val_acc: 0.35467980295566504, train_loss: 1.35606192614151, val_loss: 1.3767446520293287 (13 / 15)
train_acc: 0.4054388133498146, val_acc: 0.3891625615763547, train_loss: 1.3137849986332162, val_loss: 1.3050316396017967 (14 / 15)
train_acc: 0.44128553770086526, val_acc: 0.4187192118226601, train_loss: 1.2850169929202613, val_loss: 1.2705018009458269 (15 / 15)



lr 0.0007018603437858261, batch 8, decay 0, gamma 1, val accuracy 0.4236453201970443, val loss 1.3593352956724871 [4 / 20]
-------------------------------------
train_acc: 0.1841779975278121, val_acc: 0.23645320197044334, train_loss: 1.7829351573998318, val_loss: 1.7638850893293108 (1 / 15)
train_acc: 0.17799752781211373, val_acc: 0.2955665024630542, train_loss: 1.758678478127799, val_loss: 1.739266017974891 (2 / 15)
train_acc: 0.22249690976514216, val_acc: 0.2857142857142857, train_loss: 1.7272539975763987, val_loss: 1.7355000755469787 (3 / 15)
train_acc: 0.2867737948084054, val_acc: 0.20689655172413793, train_loss: 1.6679405647391, val_loss: 1.6970165580364283 (4 / 15)
train_acc: 0.3164400494437577, val_acc: 0.31527093596059114, train_loss: 1.601715232444782, val_loss: 1.6185481659884524 (5 / 15)
train_acc: 0.35599505562422745, val_acc: 0.27586206896551724, train_loss: 1.5284018372133725, val_loss: 1.8402444975716727 (6 / 15)
train_acc: 0.311495673671199, val_acc: 0.3793103448275862, train_loss: 1.5849166753412913, val_loss: 1.4088172436934974 (7 / 15)
train_acc: 0.3695920889987639, val_acc: 0.3842364532019704, train_loss: 1.4671900480138356, val_loss: 1.4489888698596673 (8 / 15)
train_acc: 0.4054388133498146, val_acc: 0.3645320197044335, train_loss: 1.435838818255402, val_loss: 1.458258291183434 (9 / 15)
train_acc: 0.3967861557478368, val_acc: 0.39901477832512317, train_loss: 1.418953204331793, val_loss: 1.3684228141906813 (10 / 15)
train_acc: 0.4289245982694685, val_acc: 0.4236453201970443, train_loss: 1.3842830858360262, val_loss: 1.329023042923124 (11 / 15)
train_acc: 0.38442521631644005, val_acc: 0.3793103448275862, train_loss: 1.383668934016941, val_loss: 1.3466987134200599 (12 / 15)
train_acc: 0.40667490729295425, val_acc: 0.458128078817734, train_loss: 1.3296449490913798, val_loss: 1.3021103531269018 (13 / 15)
train_acc: 0.41656365883807167, val_acc: 0.4482758620689655, train_loss: 1.3182793571274125, val_loss: 1.292724975224199 (14 / 15)
train_acc: 0.44870210135970334, val_acc: 0.43842364532019706, train_loss: 1.2785819814585342, val_loss: 1.2796501238357845 (15 / 15)



lr 0.0005645697905230947, batch 8, decay 0, gamma 1, val accuracy 0.458128078817734, val loss 1.3021103531269018 [5 / 20]
-------------------------------------
train_acc: 0.1631644004944376, val_acc: 0.18226600985221675, train_loss: 1.7851602276706577, val_loss: 1.7627092682082077 (1 / 15)
train_acc: 0.18046971569839307, val_acc: 0.20689655172413793, train_loss: 1.760894620963758, val_loss: 1.7491168312251275 (2 / 15)
train_acc: 0.2088998763906057, val_acc: 0.20689655172413793, train_loss: 1.742868466783955, val_loss: 1.7326144573136504 (3 / 15)
train_acc: 0.23980222496909764, val_acc: 0.2561576354679803, train_loss: 1.7233617047886325, val_loss: 1.7112301188736714 (4 / 15)
train_acc: 0.27812113720642767, val_acc: 0.3251231527093596, train_loss: 1.6739260672933534, val_loss: 1.6030314726195312 (5 / 15)
train_acc: 0.33498145859085293, val_acc: 0.31527093596059114, train_loss: 1.6125227730118006, val_loss: 1.5943087480338336 (6 / 15)
train_acc: 0.3868974042027194, val_acc: 0.3251231527093596, train_loss: 1.5607334429902406, val_loss: 1.566444734634437 (7 / 15)
train_acc: 0.35970333745364647, val_acc: 0.3891625615763547, train_loss: 1.5372559141611728, val_loss: 1.5152777820972387 (8 / 15)
train_acc: 0.3621755253399258, val_acc: 0.33497536945812806, train_loss: 1.5254927592165537, val_loss: 1.5196905388620687 (9 / 15)
train_acc: 0.3473423980222497, val_acc: 0.3694581280788177, train_loss: 1.5045180532929334, val_loss: 1.4593308283190423 (10 / 15)
train_acc: 0.40173053152039556, val_acc: 0.3793103448275862, train_loss: 1.4142904738413242, val_loss: 1.3836218001220026 (11 / 15)
train_acc: 0.3658838071693449, val_acc: 0.3793103448275862, train_loss: 1.45230447022965, val_loss: 1.3848154668150277 (12 / 15)
train_acc: 0.4054388133498146, val_acc: 0.3399014778325123, train_loss: 1.417200385861108, val_loss: 1.4010407061412418 (13 / 15)
train_acc: 0.4042027194066749, val_acc: 0.4039408866995074, train_loss: 1.3720117372694358, val_loss: 1.3963550658061588 (14 / 15)
train_acc: 0.41285537700865266, val_acc: 0.43842364532019706, train_loss: 1.3624153885764745, val_loss: 1.3039865987054233 (15 / 15)



lr 0.0006501744795893987, batch 8, decay 0, gamma 1, val accuracy 0.43842364532019706, val loss 1.3039865987054233 [6 / 20]
-------------------------------------
train_acc: 0.16934487021013597, val_acc: 0.18226600985221675, train_loss: 1.7868369164956073, val_loss: 1.7759189388434875 (1 / 15)
train_acc: 0.17552533992583436, val_acc: 0.24630541871921183, train_loss: 1.7628055052055858, val_loss: 1.749100450811715 (2 / 15)
train_acc: 0.20395550061804696, val_acc: 0.19704433497536947, train_loss: 1.7558813907012656, val_loss: 1.738995816907272 (3 / 15)
train_acc: 0.2027194066749073, val_acc: 0.2512315270935961, train_loss: 1.7261790573523277, val_loss: 1.7241864034107752 (4 / 15)
train_acc: 0.27812113720642767, val_acc: 0.33497536945812806, train_loss: 1.6903665584451042, val_loss: 1.635393798057669 (5 / 15)
train_acc: 0.32014833127317677, val_acc: 0.3399014778325123, train_loss: 1.6366956622992517, val_loss: 1.625923830887367 (6 / 15)
train_acc: 0.31396786155747836, val_acc: 0.31527093596059114, train_loss: 1.5984681788716828, val_loss: 1.591998196000536 (7 / 15)
train_acc: 0.3584672435105068, val_acc: 0.37438423645320196, train_loss: 1.5173868626098255, val_loss: 1.4841505007203577 (8 / 15)
train_acc: 0.3757725587144623, val_acc: 0.3497536945812808, train_loss: 1.4765160262068948, val_loss: 1.6003303715748152 (9 / 15)
train_acc: 0.37330037082818296, val_acc: 0.35960591133004927, train_loss: 1.4764465716919586, val_loss: 1.446015412584314 (10 / 15)
train_acc: 0.380716934487021, val_acc: 0.3891625615763547, train_loss: 1.4568757403766268, val_loss: 1.398067012209023 (11 / 15)
train_acc: 0.40173053152039556, val_acc: 0.39901477832512317, train_loss: 1.426697410228668, val_loss: 1.4455702533862862 (12 / 15)
train_acc: 0.4252163164400494, val_acc: 0.4433497536945813, train_loss: 1.402342258336665, val_loss: 1.33488446562161 (13 / 15)
train_acc: 0.4252163164400494, val_acc: 0.3497536945812808, train_loss: 1.3577919897838635, val_loss: 1.5022828085669155 (14 / 15)
train_acc: 0.41285537700865266, val_acc: 0.4630541871921182, train_loss: 1.3902216983224611, val_loss: 1.368635171152688 (15 / 15)



lr 0.0004673254774493346, batch 8, decay 0, gamma 1, val accuracy 0.4630541871921182, val loss 1.368635171152688 [7 / 20]
-------------------------------------
train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7772950878544114, val_loss: 1.751576425993971 (1 / 15)
train_acc: 0.2088998763906057, val_acc: 0.1921182266009852, train_loss: 1.753631925406061, val_loss: 1.7434306162331492 (2 / 15)
train_acc: 0.207663782447466, val_acc: 0.28078817733990147, train_loss: 1.7273124138121256, val_loss: 1.7007574218834562 (3 / 15)
train_acc: 0.2583436341161928, val_acc: 0.21182266009852216, train_loss: 1.7003388496205598, val_loss: 1.6666811933658394 (4 / 15)
train_acc: 0.27564894932014833, val_acc: 0.2955665024630542, train_loss: 1.6604523449509931, val_loss: 1.6137102589818644 (5 / 15)
train_acc: 0.3337453646477132, val_acc: 0.28078817733990147, train_loss: 1.608356820638159, val_loss: 1.6154927790458566 (6 / 15)
train_acc: 0.30284301606922126, val_acc: 0.3891625615763547, train_loss: 1.5719200891381584, val_loss: 1.5055184634448273 (7 / 15)
train_acc: 0.3510506798516687, val_acc: 0.3251231527093596, train_loss: 1.5527559807627693, val_loss: 1.541359163857446 (8 / 15)
train_acc: 0.3658838071693449, val_acc: 0.3842364532019704, train_loss: 1.5183763711797291, val_loss: 1.4323733998049657 (9 / 15)
train_acc: 0.3535228677379481, val_acc: 0.37438423645320196, train_loss: 1.4908075132240324, val_loss: 1.3956280476941263 (10 / 15)
train_acc: 0.3967861557478368, val_acc: 0.31527093596059114, train_loss: 1.4090724583756644, val_loss: 1.4161803387656 (11 / 15)
train_acc: 0.39060568603213847, val_acc: 0.43842364532019706, train_loss: 1.4119869466175696, val_loss: 1.3395433795863185 (12 / 15)
train_acc: 0.4276885043263288, val_acc: 0.3793103448275862, train_loss: 1.3475304410248368, val_loss: 1.4075675063532562 (13 / 15)
train_acc: 0.44746600741656367, val_acc: 0.4187192118226601, train_loss: 1.325923975672209, val_loss: 1.2955671102542596 (14 / 15)
train_acc: 0.4215080346106304, val_acc: 0.4482758620689655, train_loss: 1.289425892941884, val_loss: 1.2621402062218765 (15 / 15)



lr 0.0005689641761267869, batch 8, decay 0, gamma 1, val accuracy 0.4482758620689655, val loss 1.2621402062218765 [8 / 20]
-------------------------------------
train_acc: 0.16934487021013597, val_acc: 0.2019704433497537, train_loss: 1.7784654921742689, val_loss: 1.752721408317829 (1 / 15)
train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.760082750296858, val_loss: 1.735048435004474 (2 / 15)
train_acc: 0.2323856613102596, val_acc: 0.3251231527093596, train_loss: 1.7365646845625415, val_loss: 1.7154229022011969 (3 / 15)
train_acc: 0.2830655129789864, val_acc: 0.27586206896551724, train_loss: 1.6827345922939267, val_loss: 1.6177113754995938 (4 / 15)
train_acc: 0.32014833127317677, val_acc: 0.3399014778325123, train_loss: 1.6157851018775968, val_loss: 1.5040989679656005 (5 / 15)
train_acc: 0.315203955500618, val_acc: 0.3103448275862069, train_loss: 1.5512743190722942, val_loss: 1.5565278230629531 (6 / 15)
train_acc: 0.3399258343634116, val_acc: 0.3694581280788177, train_loss: 1.4959429502487183, val_loss: 1.4376109016352687 (7 / 15)
train_acc: 0.37824474660074164, val_acc: 0.39408866995073893, train_loss: 1.4287913558804355, val_loss: 1.467154363693275 (8 / 15)
train_acc: 0.3584672435105068, val_acc: 0.41379310344827586, train_loss: 1.4382397951684864, val_loss: 1.365410893421455 (9 / 15)
train_acc: 0.38813349814585907, val_acc: 0.4187192118226601, train_loss: 1.4092920451288022, val_loss: 1.4065852464713486 (10 / 15)
train_acc: 0.3930778739184178, val_acc: 0.41379310344827586, train_loss: 1.3723139898443988, val_loss: 1.3736895569439591 (11 / 15)
train_acc: 0.40914709517923364, val_acc: 0.458128078817734, train_loss: 1.3734890537886744, val_loss: 1.3262786313230768 (12 / 15)
train_acc: 0.4177997527812114, val_acc: 0.43842364532019706, train_loss: 1.340527386541567, val_loss: 1.4036960038058277 (13 / 15)
train_acc: 0.3572311495673671, val_acc: 0.4088669950738916, train_loss: 1.4546554993345358, val_loss: 1.3342841948781694 (14 / 15)
train_acc: 0.4252163164400494, val_acc: 0.4039408866995074, train_loss: 1.3305520500034573, val_loss: 1.3582796395705838 (15 / 15)



lr 0.0005901783099933352, batch 8, decay 0, gamma 1, val accuracy 0.458128078817734, val loss 1.3262786313230768 [9 / 20]
-------------------------------------
train_acc: 0.1681087762669963, val_acc: 0.2019704433497537, train_loss: 1.7875059337639543, val_loss: 1.7752500678518135 (1 / 15)
train_acc: 0.18294190358467244, val_acc: 0.33004926108374383, train_loss: 1.7669511279010655, val_loss: 1.7502456879968127 (2 / 15)
train_acc: 0.20519159456118666, val_acc: 0.24630541871921183, train_loss: 1.7454660892191864, val_loss: 1.7203315966235007 (3 / 15)
train_acc: 0.22126081582200247, val_acc: 0.20689655172413793, train_loss: 1.6827243285067148, val_loss: 1.6665941153841066 (4 / 15)
train_acc: 0.32756489493201485, val_acc: 0.3251231527093596, train_loss: 1.6180603981607482, val_loss: 1.5924105056988194 (5 / 15)
train_acc: 0.34981458590852904, val_acc: 0.33497536945812806, train_loss: 1.5808468186221694, val_loss: 1.6177913150176626 (6 / 15)
train_acc: 0.3658838071693449, val_acc: 0.35960591133004927, train_loss: 1.5458935111650578, val_loss: 1.5383102206761026 (7 / 15)
train_acc: 0.37082818294190356, val_acc: 0.3497536945812808, train_loss: 1.5522746051050382, val_loss: 1.5636683385360417 (8 / 15)
train_acc: 0.3794808405438813, val_acc: 0.3842364532019704, train_loss: 1.5391664938225291, val_loss: 1.5128252441659937 (9 / 15)
train_acc: 0.3819530284301607, val_acc: 0.3793103448275862, train_loss: 1.5263263415936634, val_loss: 1.5101581330369847 (10 / 15)
train_acc: 0.3868974042027194, val_acc: 0.37438423645320196, train_loss: 1.518807722110536, val_loss: 1.5028588754202932 (11 / 15)
train_acc: 0.3831891223733004, val_acc: 0.3842364532019704, train_loss: 1.4827766804524198, val_loss: 1.491914479603321 (12 / 15)
train_acc: 0.39555006180469715, val_acc: 0.3842364532019704, train_loss: 1.4558693129288398, val_loss: 1.4584480048400428 (13 / 15)
train_acc: 0.38936959208899874, val_acc: 0.35960591133004927, train_loss: 1.4476057982120585, val_loss: 1.4664111131517759 (14 / 15)
train_acc: 0.3980222496909765, val_acc: 0.4039408866995074, train_loss: 1.4294556134710794, val_loss: 1.4149395779435858 (15 / 15)



lr 0.0006537664238508854, batch 8, decay 0, gamma 1, val accuracy 0.4039408866995074, val loss 1.4149395779435858 [10 / 20]
-------------------------------------
train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7848889733421789, val_loss: 1.7695753163304822 (1 / 15)
train_acc: 0.17799752781211373, val_acc: 0.2413793103448276, train_loss: 1.7573581729153325, val_loss: 1.7359964324922985 (2 / 15)
train_acc: 0.22373300370828184, val_acc: 0.2413793103448276, train_loss: 1.726892603047846, val_loss: 1.6932243760583436 (3 / 15)
train_acc: 0.26452410383189123, val_acc: 0.27586206896551724, train_loss: 1.6835230690587142, val_loss: 1.632401563263879 (4 / 15)
train_acc: 0.3226205191594561, val_acc: 0.29064039408866993, train_loss: 1.6140393476993102, val_loss: 1.5869143478976095 (5 / 15)
train_acc: 0.3399258343634116, val_acc: 0.33004926108374383, train_loss: 1.5995724753779446, val_loss: 1.601962369063805 (6 / 15)
train_acc: 0.33127317676143386, val_acc: 0.3497536945812808, train_loss: 1.5839095260068423, val_loss: 1.5524353787229566 (7 / 15)
train_acc: 0.3757725587144623, val_acc: 0.2857142857142857, train_loss: 1.5357424614603352, val_loss: 1.8037089655552003 (8 / 15)
train_acc: 0.3757725587144623, val_acc: 0.3694581280788177, train_loss: 1.5241668651660971, val_loss: 1.5377124377659388 (9 / 15)
train_acc: 0.38936959208899874, val_acc: 0.35467980295566504, train_loss: 1.50321174552031, val_loss: 1.5022980021725734 (10 / 15)
train_acc: 0.36341161928306553, val_acc: 0.35960591133004927, train_loss: 1.5215403232055777, val_loss: 1.5309437771736107 (11 / 15)
train_acc: 0.39184177997527814, val_acc: 0.37438423645320196, train_loss: 1.4860604099643835, val_loss: 1.4551899051431365 (12 / 15)
train_acc: 0.38936959208899874, val_acc: 0.39408866995073893, train_loss: 1.4232063070952672, val_loss: 1.4956345164717124 (13 / 15)
train_acc: 0.3856613102595797, val_acc: 0.3497536945812808, train_loss: 1.4389379776010407, val_loss: 1.4158080000008268 (14 / 15)
train_acc: 0.4004944375772559, val_acc: 0.3842364532019704, train_loss: 1.4167814461056176, val_loss: 1.3783286868645053 (15 / 15)



lr 0.0006023707740961776, batch 8, decay 0, gamma 1, val accuracy 0.39408866995073893, val loss 1.4956345164717124 [11 / 20]
-------------------------------------
train_acc: 0.15945611866501855, val_acc: 0.18226600985221675, train_loss: 1.7865983896114033, val_loss: 1.7724247360464387 (1 / 15)
train_acc: 0.207663782447466, val_acc: 0.18226600985221675, train_loss: 1.7609477137458338, val_loss: 1.7446881491562416 (2 / 15)
train_acc: 0.19283065512978986, val_acc: 0.21182266009852216, train_loss: 1.7406672989334842, val_loss: 1.73678974506303 (3 / 15)
train_acc: 0.2892459826946848, val_acc: 0.3694581280788177, train_loss: 1.7015035771321603, val_loss: 1.6343034576312663 (4 / 15)
train_acc: 0.32014833127317677, val_acc: 0.3054187192118227, train_loss: 1.647950967545857, val_loss: 1.5768161501203264 (5 / 15)
train_acc: 0.3053152039555006, val_acc: 0.2955665024630542, train_loss: 1.5893051983841564, val_loss: 1.6043902659063856 (6 / 15)
train_acc: 0.3473423980222497, val_acc: 0.3645320197044335, train_loss: 1.5498675471920018, val_loss: 1.5785587391829843 (7 / 15)
train_acc: 0.3547589616810878, val_acc: 0.3448275862068966, train_loss: 1.505563428139657, val_loss: 1.5220557509971957 (8 / 15)
train_acc: 0.3757725587144623, val_acc: 0.35960591133004927, train_loss: 1.4480336767486648, val_loss: 1.4585253869371462 (9 / 15)
train_acc: 0.3535228677379481, val_acc: 0.39901477832512317, train_loss: 1.4527172072708827, val_loss: 1.3643965392277158 (10 / 15)
train_acc: 0.39060568603213847, val_acc: 0.4088669950738916, train_loss: 1.3977097471800664, val_loss: 1.3807472460375632 (11 / 15)
train_acc: 0.38813349814585907, val_acc: 0.33497536945812806, train_loss: 1.384051739064371, val_loss: 1.518304578482811 (12 / 15)
train_acc: 0.39060568603213847, val_acc: 0.42857142857142855, train_loss: 1.3599693919583804, val_loss: 1.4220621192396568 (13 / 15)
train_acc: 0.4215080346106304, val_acc: 0.4236453201970443, train_loss: 1.355435414426259, val_loss: 1.2909797312590876 (14 / 15)
train_acc: 0.4326328800988875, val_acc: 0.43842364532019706, train_loss: 1.319074787227126, val_loss: 1.2912250222830937 (15 / 15)



lr 0.0006166743886480918, batch 8, decay 0, gamma 1, val accuracy 0.43842364532019706, val loss 1.2912250222830937 [12 / 20]
-------------------------------------
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
<ipython-input-4-7166a226b630> in <module>()
     41   net = vgg19()
     42   net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
---> 43   current_net, val_accuracy, val_loss = train_network(net, net.parameters(), LR, NUM_EPOCHS, BATCH_SIZE, WEIGHT_DECAY, STEP_SIZE, GAMMA, train_dataset, val_dataset=val_dataset, verbosity=True, plot=True)
     44 
     45   val_accuracies.append(val_accuracy)

<ipython-input-2-c446edcbb070> in train_network(net, parameters_to_optimize, learning_rate, num_epochs, batch_size, weight_decay, step_size, gamma, train_dataset, val_dataset, verbosity, plot)
     82 
     83         for images, labels in train_dataloader:
---> 84             images = images.to(DEVICE)
     85             labels = labels.to(DEVICE)
     86             net.train()

KeyboardInterrupt: 