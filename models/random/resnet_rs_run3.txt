

training set 809
validation set 203
---------------------------------------------
train_acc: 0.25092707045735474, val_acc: 0.21182266009852216, train_loss: 2.8360635070187787, val_loss: 1.754578704023596 (1 / 30)
train_acc: 0.2843016069221261, val_acc: 0.2413793103448276, train_loss: 2.033004798759489, val_loss: 2.1232859439450533 (2 / 30)
train_acc: 0.33250927070457353, val_acc: 0.3251231527093596, train_loss: 1.7142444093088873, val_loss: 1.8651411962039366 (3 / 30)
train_acc: 0.33868974042027195, val_acc: 0.3694581280788177, train_loss: 1.6663536601072484, val_loss: 1.597207959649598 (4 / 30)
train_acc: 0.3572311495673671, val_acc: 0.35960591133004927, train_loss: 1.5455494893643411, val_loss: 1.7473113372408111 (5 / 30)
train_acc: 0.3510506798516687, val_acc: 0.3645320197044335, train_loss: 1.6272054904765634, val_loss: 2.1481774284921844 (6 / 30)
train_acc: 0.3868974042027194, val_acc: 0.41379310344827586, train_loss: 1.4329661263523643, val_loss: 1.3475333234946716 (7 / 30)
train_acc: 0.3658838071693449, val_acc: 0.4088669950738916, train_loss: 1.4656733685577166, val_loss: 3.8709751291228045 (8 / 30)
train_acc: 0.3868974042027194, val_acc: 0.37438423645320196, train_loss: 1.4163368841626294, val_loss: 1.3421443806493223 (9 / 30)
train_acc: 0.37330037082818296, val_acc: 0.4088669950738916, train_loss: 1.4631188496376293, val_loss: 1.6240168833380262 (10 / 30)
train_acc: 0.3967861557478368, val_acc: 0.3645320197044335, train_loss: 1.4020321887856804, val_loss: 1.7378997042261322 (11 / 30)
train_acc: 0.43139678615574784, val_acc: 0.4039408866995074, train_loss: 1.3766784190542176, val_loss: 1.3718663225033012 (12 / 30)
train_acc: 0.411619283065513, val_acc: 0.47783251231527096, train_loss: 1.3256885967089602, val_loss: 1.238147769949119 (13 / 30)
train_acc: 0.4511742892459827, val_acc: 0.30049261083743845, train_loss: 1.3034514582201342, val_loss: 2.0590338178456125 (14 / 30)
train_acc: 0.44128553770086526, val_acc: 0.39408866995073893, train_loss: 1.338950586407382, val_loss: 1.4595678941956882 (15 / 30)
train_acc: 0.46600741656365885, val_acc: 0.41379310344827586, train_loss: 1.2450997608406436, val_loss: 1.5035659532828871 (16 / 30)
train_acc: 0.5315203955500618, val_acc: 0.3842364532019704, train_loss: 1.1085148591488343, val_loss: 1.9614692803086906 (17 / 30)
train_acc: 0.5525339925834364, val_acc: 0.4630541871921182, train_loss: 1.118932532733686, val_loss: 1.2938097562695958 (18 / 30)
train_acc: 0.5747836835599506, val_acc: 0.4482758620689655, train_loss: 1.0567019980386987, val_loss: 1.3825959325423969 (19 / 30)
train_acc: 0.5550061804697157, val_acc: 0.4236453201970443, train_loss: 1.1227751444237783, val_loss: 1.6115801029017407 (20 / 30)
train_acc: 0.5525339925834364, val_acc: 0.43349753694581283, train_loss: 1.0848221660986523, val_loss: 1.4823967124440987 (21 / 30)
train_acc: 0.5896168108776267, val_acc: 0.4187192118226601, train_loss: 1.0254931535355387, val_loss: 2.0521925317830054 (22 / 30)
train_acc: 0.61557478368356, val_acc: 0.4876847290640394, train_loss: 0.996654800785192, val_loss: 1.3172906378807105 (23 / 30)
train_acc: 0.6093943139678616, val_acc: 0.3694581280788177, train_loss: 0.9993457652729728, val_loss: 2.0388935023340684 (24 / 30)
train_acc: 0.6279357231149567, val_acc: 0.4975369458128079, train_loss: 0.9231668897848047, val_loss: 1.467078797922933 (25 / 30)
train_acc: 0.6687268232385661, val_acc: 0.4975369458128079, train_loss: 0.8510560579736094, val_loss: 1.5601803722052738 (26 / 30)
train_acc: 0.6662546353522868, val_acc: 0.4827586206896552, train_loss: 0.8920477108548687, val_loss: 1.4787192456240725 (27 / 30)
train_acc: 0.7132262051915945, val_acc: 0.4827586206896552, train_loss: 0.7386050115262327, val_loss: 1.807316624472294 (28 / 30)
train_acc: 0.7194066749072929, val_acc: 0.5073891625615764, train_loss: 0.7506845701757556, val_loss: 1.6825030854182879 (29 / 30)
train_acc: 0.7354758961681088, val_acc: 0.42857142857142855, train_loss: 0.7037792488729998, val_loss: 1.9807272321484946 (30 / 30)
lr 0.003149050913917944, batch 8, decay 6.256843284483036e-05, gamma 0.8254205519719666, val accuracy 0.5073891625615764, val loss 1.6825030854182879 [1 / 20]
---------------------------------------------
train_acc: 0.24474660074165636, val_acc: 0.2561576354679803, train_loss: 2.8685443716673973, val_loss: 1.9297507408217256 (1 / 30)
train_acc: 0.28059332509270707, val_acc: 0.3694581280788177, train_loss: 2.013303842179118, val_loss: 1.495247532581461 (2 / 30)
train_acc: 0.3263288009888752, val_acc: 0.35467980295566504, train_loss: 1.7221828111169954, val_loss: 1.8678043117664132 (3 / 30)
train_acc: 0.35599505562422745, val_acc: 0.3842364532019704, train_loss: 1.5758639659516154, val_loss: 1.6371720395064706 (4 / 30)
train_acc: 0.36711990111248455, val_acc: 0.3497536945812808, train_loss: 1.5381405297551667, val_loss: 1.8802092104709793 (5 / 30)
train_acc: 0.32509270704573545, val_acc: 0.3842364532019704, train_loss: 1.698171909718932, val_loss: 2.572512246705041 (6 / 30)
train_acc: 0.34610630407911, val_acc: 0.3694581280788177, train_loss: 1.5067775258322433, val_loss: 2.0522822347180596 (7 / 30)
train_acc: 0.3943139678615575, val_acc: 0.3793103448275862, train_loss: 1.4523010648814652, val_loss: 1.6066090191526365 (8 / 30)
train_acc: 0.37453646477132263, val_acc: 0.4187192118226601, train_loss: 1.5193330444423172, val_loss: 1.8264997028952161 (9 / 30)
train_acc: 0.3856613102595797, val_acc: 0.3842364532019704, train_loss: 1.4015581522353353, val_loss: 1.4126031351794164 (10 / 30)
train_acc: 0.4289245982694685, val_acc: 0.3694581280788177, train_loss: 1.325780755362493, val_loss: 1.6211837923585488 (11 / 30)
train_acc: 0.37453646477132263, val_acc: 0.33497536945812806, train_loss: 1.3760827952173937, val_loss: 2.0302872246709365 (12 / 30)
train_acc: 0.4252163164400494, val_acc: 0.3399014778325123, train_loss: 1.3165253287340124, val_loss: 2.0071932235962064 (13 / 30)
train_acc: 0.43139678615574784, val_acc: 0.39408866995073893, train_loss: 1.3543832010332821, val_loss: 2.335305664926914 (14 / 30)
train_acc: 0.43016069221260816, val_acc: 0.39408866995073893, train_loss: 1.3316046605151428, val_loss: 1.6745726436817001 (15 / 30)
train_acc: 0.4610630407911001, val_acc: 0.39408866995073893, train_loss: 1.285591822766845, val_loss: 1.731057950428554 (16 / 30)
train_acc: 0.45488257107540175, val_acc: 0.3842364532019704, train_loss: 1.3110818076045316, val_loss: 13.669993465757136 (17 / 30)
train_acc: 0.446229913473424, val_acc: 0.5024630541871922, train_loss: 1.2986018038798026, val_loss: 1.3981136606244617 (18 / 30)
train_acc: 0.5451174289245982, val_acc: 0.458128078817734, train_loss: 1.0439601843377715, val_loss: 1.61112817872334 (19 / 30)
train_acc: 0.5970333745364648, val_acc: 0.39901477832512317, train_loss: 0.9112972809446167, val_loss: 2.109013621443011 (20 / 30)
train_acc: 0.595797280593325, val_acc: 0.49261083743842365, train_loss: 0.9217348738270725, val_loss: 3.8246926403985233 (21 / 30)
train_acc: 0.6440049443757726, val_acc: 0.43349753694581283, train_loss: 0.8669087176564597, val_loss: 2.4061799031760307 (22 / 30)
train_acc: 0.6427688504326329, val_acc: 0.5024630541871922, train_loss: 0.8802706165278356, val_loss: 1.686250461439781 (23 / 30)
train_acc: 0.6798516687268232, val_acc: 0.47783251231527096, train_loss: 0.7937303203882187, val_loss: 2.117304970478189 (24 / 30)
train_acc: 0.695920889987639, val_acc: 0.5221674876847291, train_loss: 0.7690501599140898, val_loss: 1.7654033238664637 (25 / 30)
train_acc: 0.6798516687268232, val_acc: 0.43349753694581283, train_loss: 0.786761885373053, val_loss: 1.5122680452656863 (26 / 30)
train_acc: 0.7070457354758962, val_acc: 0.47783251231527096, train_loss: 0.7449938496494175, val_loss: 1.8900578559325834 (27 / 30)
train_acc: 0.6909765142150803, val_acc: 0.5024630541871922, train_loss: 0.7584310434067942, val_loss: 1.5827578617434197 (28 / 30)
train_acc: 0.6823238566131026, val_acc: 0.5862068965517241, train_loss: 0.7622708207449895, val_loss: 1.4801913344214115 (29 / 30)
train_acc: 0.7330037082818294, val_acc: 0.5320197044334976, train_loss: 0.6845831907871185, val_loss: 1.2990717864388903 (30 / 30)
lr 0.004370054390760577, batch 8, decay 2.4983989017395743e-05, gamma 0.2790351363752022, val accuracy 0.5862068965517241, val loss 1.4801913344214115 [2 / 20]
---------------------------------------------
train_acc: 0.26452410383189123, val_acc: 0.27586206896551724, train_loss: 2.379960071318524, val_loss: 3.4735443395346843 (1 / 30)
train_acc: 0.2719406674907293, val_acc: 0.27586206896551724, train_loss: 1.9287659747638868, val_loss: 2.0293556023113832 (2 / 30)
train_acc: 0.30778739184178, val_acc: 0.30049261083743845, train_loss: 1.7349921787625042, val_loss: 1.752160960523953 (3 / 30)
train_acc: 0.31396786155747836, val_acc: 0.39408866995073893, train_loss: 1.8306671697807548, val_loss: 1.4968807010227823 (4 / 30)
train_acc: 0.37824474660074164, val_acc: 0.30049261083743845, train_loss: 1.7009433096801985, val_loss: 1.8680763773142999 (5 / 30)
train_acc: 0.3943139678615575, val_acc: 0.4088669950738916, train_loss: 1.5316855264977265, val_loss: 1.6823801269084948 (6 / 30)
train_acc: 0.3757725587144623, val_acc: 0.41379310344827586, train_loss: 1.5724365322197913, val_loss: 1.3666302003883963 (7 / 30)
train_acc: 0.4647713226205192, val_acc: 0.42857142857142855, train_loss: 1.4045648191709013, val_loss: 1.4607994160041433 (8 / 30)
train_acc: 0.40173053152039556, val_acc: 0.4088669950738916, train_loss: 1.5076674881618015, val_loss: 1.4511264935502866 (9 / 30)
train_acc: 0.44252163164400493, val_acc: 0.39408866995073893, train_loss: 1.394355600048202, val_loss: 1.5135358137450194 (10 / 30)
train_acc: 0.4758961681087763, val_acc: 0.3793103448275862, train_loss: 1.3230627487852193, val_loss: 1.6124192476272583 (11 / 30)
train_acc: 0.4672435105067985, val_acc: 0.31527093596059114, train_loss: 1.3092745443799145, val_loss: 1.8547120945794242 (12 / 30)
train_acc: 0.47095179233621753, val_acc: 0.4236453201970443, train_loss: 1.3268449169448928, val_loss: 1.4362547186207888 (13 / 30)
train_acc: 0.519159456118665, val_acc: 0.45320197044334976, train_loss: 1.1995004752952472, val_loss: 1.5822372653801453 (14 / 30)
train_acc: 0.4746600741656366, val_acc: 0.39408866995073893, train_loss: 1.2940581571333782, val_loss: 1.607819471453211 (15 / 30)
train_acc: 0.5302843016069221, val_acc: 0.47783251231527096, train_loss: 1.1957342110399263, val_loss: 1.309878223048055 (16 / 30)
train_acc: 0.5364647713226205, val_acc: 0.45320197044334976, train_loss: 1.1239817864520587, val_loss: 1.3681411655078382 (17 / 30)
train_acc: 0.5475896168108776, val_acc: 0.4482758620689655, train_loss: 1.2202039973255436, val_loss: 1.593776431576959 (18 / 30)
train_acc: 0.65389369592089, val_acc: 0.5467980295566502, train_loss: 0.8655260144117, val_loss: 1.2584704642225368 (19 / 30)
train_acc: 0.7082818294190358, val_acc: 0.5369458128078818, train_loss: 0.7328225418427966, val_loss: 1.1866967413813023 (20 / 30)
train_acc: 0.761433868974042, val_acc: 0.5467980295566502, train_loss: 0.6131746546447056, val_loss: 1.2354558400919873 (21 / 30)
train_acc: 0.7601977750309024, val_acc: 0.5172413793103449, train_loss: 0.6165230350824458, val_loss: 1.2592922132003483 (22 / 30)
train_acc: 0.7762669962917181, val_acc: 0.5320197044334976, train_loss: 0.5760405156462095, val_loss: 1.328961276655714 (23 / 30)
train_acc: 0.7775030902348579, val_acc: 0.5221674876847291, train_loss: 0.5646831401346346, val_loss: 1.3678467579075855 (24 / 30)
train_acc: 0.8158220024721878, val_acc: 0.5172413793103449, train_loss: 0.4829329023844527, val_loss: 1.4185307131612241 (25 / 30)
train_acc: 0.8603213844252163, val_acc: 0.5566502463054187, train_loss: 0.41356047976886384, val_loss: 1.4002945951640313 (26 / 30)
train_acc: 0.8405438813349815, val_acc: 0.5566502463054187, train_loss: 0.4503410672964655, val_loss: 1.362995246361042 (27 / 30)
train_acc: 0.8640296662546354, val_acc: 0.5073891625615764, train_loss: 0.3623174915207626, val_loss: 1.516213278465083 (28 / 30)
train_acc: 0.8516687268232386, val_acc: 0.5615763546798029, train_loss: 0.4247641539102137, val_loss: 1.4249550990870434 (29 / 30)
train_acc: 0.8862793572311496, val_acc: 0.541871921182266, train_loss: 0.3381342014069905, val_loss: 1.4302119586268083 (30 / 30)
lr 0.0013956457803157216, batch 8, decay 1.761025272281164e-05, gamma 0.13405878555049575, val accuracy 0.5615763546798029, val loss 1.4249550990870434 [3 / 20]
---------------------------------------------
train_acc: 0.23856613102595797, val_acc: 0.3645320197044335, train_loss: 2.202292610894617, val_loss: 2.503738297030256 (1 / 30)
train_acc: 0.29295426452410384, val_acc: 0.3103448275862069, train_loss: 2.1736790500258927, val_loss: 1.5890989468015473 (2 / 30)
train_acc: 0.26081582200247216, val_acc: 0.29064039408866993, train_loss: 1.8896538990242373, val_loss: 1.7266455921046253 (3 / 30)
train_acc: 0.34363411619283063, val_acc: 0.3251231527093596, train_loss: 1.5852659535496432, val_loss: 2.3532201732907976 (4 / 30)
train_acc: 0.35599505562422745, val_acc: 0.35467980295566504, train_loss: 1.7069188481650335, val_loss: 1.504488161338374 (5 / 30)
train_acc: 0.3362175525339926, val_acc: 0.3448275862068966, train_loss: 1.6561241810047729, val_loss: 1.8380166961641735 (6 / 30)
train_acc: 0.3522867737948084, val_acc: 0.39408866995073893, train_loss: 1.5516244436813373, val_loss: 1.4277430514396705 (7 / 30)
train_acc: 0.37082818294190356, val_acc: 0.4039408866995074, train_loss: 1.4633394428177728, val_loss: 1.381696382766874 (8 / 30)
train_acc: 0.38936959208899874, val_acc: 0.3694581280788177, train_loss: 1.4281557366638749, val_loss: 1.5463191728873793 (9 / 30)
train_acc: 0.42398022249690975, val_acc: 0.458128078817734, train_loss: 1.4169380252234574, val_loss: 1.3182456370057731 (10 / 30)
train_acc: 0.4103831891223733, val_acc: 0.39408866995073893, train_loss: 1.3977239500312193, val_loss: 1.6695986622072794 (11 / 30)
train_acc: 0.4622991347342398, val_acc: 0.28078817733990147, train_loss: 1.2769437851510914, val_loss: 1.698418150394421 (12 / 30)
train_acc: 0.44252163164400493, val_acc: 0.4039408866995074, train_loss: 1.401681269644512, val_loss: 1.848349982294543 (13 / 30)
train_acc: 0.45241038318912236, val_acc: 0.43349753694581283, train_loss: 1.349188883608145, val_loss: 1.8362520944896004 (14 / 30)
train_acc: 0.4746600741656366, val_acc: 0.35960591133004927, train_loss: 1.2879323157745475, val_loss: 1.594687531734335 (15 / 30)
train_acc: 0.5278121137206427, val_acc: 0.47783251231527096, train_loss: 1.1936218149140385, val_loss: 1.515521199245171 (16 / 30)
train_acc: 0.4919653893695921, val_acc: 0.39408866995073893, train_loss: 1.2091782561633437, val_loss: 1.4867986558106145 (17 / 30)
train_acc: 0.5080346106304079, val_acc: 0.4482758620689655, train_loss: 1.2674015098802829, val_loss: 1.4762640833267437 (18 / 30)
train_acc: 0.5995055624227441, val_acc: 0.4729064039408867, train_loss: 1.0097844191033702, val_loss: 1.4552835924872036 (19 / 30)
train_acc: 0.6205191594561187, val_acc: 0.4088669950738916, train_loss: 0.9161286984739551, val_loss: 1.6380927586203138 (20 / 30)
train_acc: 0.6143386897404203, val_acc: 0.4482758620689655, train_loss: 0.9703742588406293, val_loss: 1.5883428439718161 (21 / 30)
train_acc: 0.6279357231149567, val_acc: 0.458128078817734, train_loss: 0.9381296979482153, val_loss: 1.5590111786508796 (22 / 30)
train_acc: 0.6501854140914709, val_acc: 0.4975369458128079, train_loss: 0.8794338602954879, val_loss: 1.3783097132086166 (23 / 30)
train_acc: 0.6328800988875154, val_acc: 0.458128078817734, train_loss: 0.8950573015861369, val_loss: 1.6609094048955757 (24 / 30)
train_acc: 0.6526576019777504, val_acc: 0.4827586206896552, train_loss: 0.8718130448545307, val_loss: 1.3310130159255906 (25 / 30)
train_acc: 0.6996291718170581, val_acc: 0.458128078817734, train_loss: 0.7622620579044074, val_loss: 1.8043875001334204 (26 / 30)
train_acc: 0.6860321384425216, val_acc: 0.458128078817734, train_loss: 0.8085437621115459, val_loss: 1.484706421791039 (27 / 30)
train_acc: 0.7255871446229913, val_acc: 0.5320197044334976, train_loss: 0.7316534186175786, val_loss: 1.405789856840237 (28 / 30)
train_acc: 0.7008652657601978, val_acc: 0.5073891625615764, train_loss: 0.7949476405628237, val_loss: 1.443992102087425 (29 / 30)
train_acc: 0.7206427688504327, val_acc: 0.5172413793103449, train_loss: 0.7311069526247984, val_loss: 1.5246700988027262 (30 / 30)
lr 0.0017283386367491806, batch 8, decay 5.075974197515026e-06, gamma 0.6430214592694454, val accuracy 0.5320197044334976, val loss 1.405789856840237 [4 / 20]
---------------------------------------------
train_acc: 0.2311495673671199, val_acc: 0.3103448275862069, train_loss: 3.623693150259803, val_loss: 1.766727109260747 (1 / 30)
train_acc: 0.2620519159456119, val_acc: 0.3497536945812808, train_loss: 1.8953618732164759, val_loss: 2.890737066715222 (2 / 30)
train_acc: 0.3189122373300371, val_acc: 0.32019704433497537, train_loss: 1.6521292858571734, val_loss: 1.6774609523453736 (3 / 30)
train_acc: 0.3226205191594561, val_acc: 0.35467980295566504, train_loss: 1.6528849501544967, val_loss: 4.008367947169712 (4 / 30)
train_acc: 0.36711990111248455, val_acc: 0.3891625615763547, train_loss: 1.590443319973751, val_loss: 1.5383561092057252 (5 / 30)
train_acc: 0.3547589616810878, val_acc: 0.3793103448275862, train_loss: 1.4788653353972843, val_loss: 1.6074710168274753 (6 / 30)
train_acc: 0.3695920889987639, val_acc: 0.3694581280788177, train_loss: 1.4592372917274314, val_loss: 1.9503452067304714 (7 / 30)
train_acc: 0.3720642768850433, val_acc: 0.37438423645320196, train_loss: 1.4238485653409263, val_loss: 1.971620636033307 (8 / 30)
train_acc: 0.4042027194066749, val_acc: 0.3448275862068966, train_loss: 1.3388653538283075, val_loss: 2.101247714070851 (9 / 30)
train_acc: 0.3980222496909765, val_acc: 0.3891625615763547, train_loss: 1.3879089508716786, val_loss: 4.552794780637243 (10 / 30)
train_acc: 0.42398022249690975, val_acc: 0.31527093596059114, train_loss: 1.3637582501315952, val_loss: 2.2601670989849296 (11 / 30)
train_acc: 0.4054388133498146, val_acc: 0.3645320197044335, train_loss: 1.348661007780964, val_loss: 2.421360769882578 (12 / 30)
train_acc: 0.4684796044499382, val_acc: 0.3448275862068966, train_loss: 1.2495622471324888, val_loss: 3.6582962224636173 (13 / 30)
train_acc: 0.4388133498145859, val_acc: 0.4433497536945813, train_loss: 1.2895489726284672, val_loss: 2.4337001181588382 (14 / 30)
train_acc: 0.4573547589616811, val_acc: 0.35960591133004927, train_loss: 1.279659264314602, val_loss: 1.9032828875363166 (15 / 30)
train_acc: 0.484548825710754, val_acc: 0.3793103448275862, train_loss: 1.2337195678753377, val_loss: 2.1649352488259375 (16 / 30)
train_acc: 0.4746600741656366, val_acc: 0.39901477832512317, train_loss: 1.2441203694408403, val_loss: 1.5131806942629697 (17 / 30)
train_acc: 0.5278121137206427, val_acc: 0.4039408866995074, train_loss: 1.1081589809601622, val_loss: 3.074167041355753 (18 / 30)
train_acc: 0.5574783683559951, val_acc: 0.4630541871921182, train_loss: 1.0468264896289086, val_loss: 1.7107549224581038 (19 / 30)
train_acc: 0.695920889987639, val_acc: 0.5024630541871922, train_loss: 0.7558876153712514, val_loss: 1.9380622538439747 (20 / 30)
train_acc: 0.7280593325092707, val_acc: 0.4729064039408867, train_loss: 0.7149382400866344, val_loss: 2.138863542984272 (21 / 30)
train_acc: 0.7404202719406675, val_acc: 0.41379310344827586, train_loss: 0.6462438952348141, val_loss: 4.2074077881028495 (22 / 30)
train_acc: 0.7490729295426453, val_acc: 0.5123152709359606, train_loss: 0.6629470896219883, val_loss: 1.4900284571013427 (23 / 30)
train_acc: 0.7564894932014833, val_acc: 0.458128078817734, train_loss: 0.6088269334493668, val_loss: 1.8894476044941417 (24 / 30)
train_acc: 0.7836835599505563, val_acc: 0.541871921182266, train_loss: 0.5585414078385927, val_loss: 1.645122166337638 (25 / 30)
train_acc: 0.8121137206427689, val_acc: 0.4433497536945813, train_loss: 0.5158992621158934, val_loss: 3.8133967416039827 (26 / 30)
train_acc: 0.7515451174289246, val_acc: 0.4876847290640394, train_loss: 0.6500891562003288, val_loss: 2.2453354661687843 (27 / 30)
train_acc: 0.7787391841779975, val_acc: 0.4876847290640394, train_loss: 0.5945704908689257, val_loss: 2.373055372332117 (28 / 30)
train_acc: 0.8034610630407911, val_acc: 0.45320197044334976, train_loss: 0.5278291112854985, val_loss: 2.286399771427286 (29 / 30)
train_acc: 0.8133498145859085, val_acc: 0.5270935960591133, train_loss: 0.48618767612207364, val_loss: 2.253510808709807 (30 / 30)
lr 0.005039312748392438, batch 8, decay 2.190527344808628e-05, gamma 0.21616510432563296, val accuracy 0.541871921182266, val loss 1.645122166337638 [5 / 20]
---------------------------------------------
train_acc: 0.2880098887515451, val_acc: 0.30049261083743845, train_loss: 2.101609855411374, val_loss: 2.0904341533853503 (1 / 30)
train_acc: 0.28059332509270707, val_acc: 0.31527093596059114, train_loss: 2.090916737637797, val_loss: 1.742014374932632 (2 / 30)
train_acc: 0.311495673671199, val_acc: 0.37438423645320196, train_loss: 1.8707995178967678, val_loss: 2.0989065516758436 (3 / 30)
train_acc: 0.34487021013597036, val_acc: 0.35467980295566504, train_loss: 1.810747036975158, val_loss: 2.0008586592274935 (4 / 30)
train_acc: 0.3226205191594561, val_acc: 0.3645320197044335, train_loss: 1.6201173254232324, val_loss: 1.6845503282077208 (5 / 30)
train_acc: 0.37453646477132263, val_acc: 0.33497536945812806, train_loss: 1.522403873530837, val_loss: 1.6752061773403524 (6 / 30)
train_acc: 0.4054388133498146, val_acc: 0.33004926108374383, train_loss: 1.5136939739562083, val_loss: 1.7422964514182706 (7 / 30)
train_acc: 0.3658838071693449, val_acc: 0.3694581280788177, train_loss: 1.506275430450628, val_loss: 1.7787154883586715 (8 / 30)
train_acc: 0.4177997527812114, val_acc: 0.35960591133004927, train_loss: 1.3975348808562063, val_loss: 1.9203100092892575 (9 / 30)
train_acc: 0.3992583436341162, val_acc: 0.37438423645320196, train_loss: 1.4166324306624782, val_loss: 1.5346483497196817 (10 / 30)
train_acc: 0.42027194066749074, val_acc: 0.3842364532019704, train_loss: 1.3618618006759284, val_loss: 1.3893971742667588 (11 / 30)
train_acc: 0.37330037082818296, val_acc: 0.41379310344827586, train_loss: 1.4769654586377043, val_loss: 1.4421426128284098 (12 / 30)
train_acc: 0.40667490729295425, val_acc: 0.35960591133004927, train_loss: 1.4224645835065428, val_loss: 1.7033604994196023 (13 / 30)
train_acc: 0.49938195302843014, val_acc: 0.3842364532019704, train_loss: 1.1790333620844724, val_loss: 1.5319721176119274 (14 / 30)
train_acc: 0.4857849196538937, val_acc: 0.3399014778325123, train_loss: 1.2229747912202984, val_loss: 1.8398736850381485 (15 / 30)
train_acc: 0.4672435105067985, val_acc: 0.41379310344827586, train_loss: 1.2464697464140149, val_loss: 1.5272146551479846 (16 / 30)
train_acc: 0.5364647713226205, val_acc: 0.43349753694581283, train_loss: 1.158093377303134, val_loss: 1.550428618351227 (17 / 30)
train_acc: 0.5129789864029666, val_acc: 0.39408866995073893, train_loss: 1.168432737015676, val_loss: 1.6421483649408877 (18 / 30)
train_acc: 0.6242274412855378, val_acc: 0.47783251231527096, train_loss: 0.9210748828680465, val_loss: 1.4616909996042111 (19 / 30)
train_acc: 0.7008652657601978, val_acc: 0.5369458128078818, train_loss: 0.7711914911847769, val_loss: 1.392506042137522 (20 / 30)
train_acc: 0.7391841779975278, val_acc: 0.5615763546798029, train_loss: 0.6681382801977754, val_loss: 1.387393525668553 (21 / 30)
train_acc: 0.7503090234857849, val_acc: 0.5172413793103449, train_loss: 0.6316563394956742, val_loss: 1.5370148173693954 (22 / 30)
train_acc: 0.7725587144622992, val_acc: 0.5073891625615764, train_loss: 0.5844680417158695, val_loss: 1.4761953982226368 (23 / 30)
train_acc: 0.7700865265760197, val_acc: 0.4729064039408867, train_loss: 0.5938472240905385, val_loss: 1.8089679755600803 (24 / 30)
train_acc: 0.796044499381953, val_acc: 0.5123152709359606, train_loss: 0.5254001947504333, val_loss: 1.6112019372691075 (25 / 30)
train_acc: 0.7453646477132262, val_acc: 0.5862068965517241, train_loss: 0.6534416553263905, val_loss: 1.3540217230472658 (26 / 30)
train_acc: 0.7515451174289246, val_acc: 0.541871921182266, train_loss: 0.6389233861187628, val_loss: 1.4622476840841359 (27 / 30)
train_acc: 0.8145859085290482, val_acc: 0.5123152709359606, train_loss: 0.5424767617684212, val_loss: 1.6249350961205995 (28 / 30)
train_acc: 0.7861557478368356, val_acc: 0.4827586206896552, train_loss: 0.5497741545970714, val_loss: 1.729927503416691 (29 / 30)
train_acc: 0.8009888751545118, val_acc: 0.5320197044334976, train_loss: 0.5117958508551637, val_loss: 1.5962805677517293 (30 / 30)
lr 0.002079668637156788, batch 8, decay 9.35265161244351e-05, gamma 0.27285577635295216, val accuracy 0.5862068965517241, val loss 1.3540217230472658 [6 / 20]
---------------------------------------------
train_acc: 0.242274412855377, val_acc: 0.3054187192118227, train_loss: 2.630358144880373, val_loss: 2.097042988086569 (1 / 30)
train_acc: 0.27441285537700866, val_acc: 0.2660098522167488, train_loss: 2.1675953773691865, val_loss: 4.152619633181342 (2 / 30)
train_acc: 0.311495673671199, val_acc: 0.31527093596059114, train_loss: 1.7977134229520932, val_loss: 1.674081032792923 (3 / 30)
train_acc: 0.3572311495673671, val_acc: 0.3497536945812808, train_loss: 1.6106464774411158, val_loss: 1.7684840333872829 (4 / 30)
train_acc: 0.36711990111248455, val_acc: 0.3497536945812808, train_loss: 1.5727529183895832, val_loss: 1.4578596488595597 (5 / 30)
train_acc: 0.31396786155747836, val_acc: 0.31527093596059114, train_loss: 1.7177225196612043, val_loss: 1.5476314187637104 (6 / 30)
train_acc: 0.36711990111248455, val_acc: 0.3645320197044335, train_loss: 1.4370268095556384, val_loss: 1.502152350735782 (7 / 30)
train_acc: 0.3992583436341162, val_acc: 0.4039408866995074, train_loss: 1.4661091305269447, val_loss: 1.4729475566906294 (8 / 30)
train_acc: 0.38442521631644005, val_acc: 0.39408866995073893, train_loss: 1.421413505033156, val_loss: 1.5378884552734826 (9 / 30)
train_acc: 0.3695920889987639, val_acc: 0.3694581280788177, train_loss: 1.624547552266728, val_loss: 1.6094682416305166 (10 / 30)
train_acc: 0.39184177997527814, val_acc: 0.3497536945812808, train_loss: 1.429640866622642, val_loss: 1.4072235238375923 (11 / 30)
train_acc: 0.4227441285537701, val_acc: 0.3497536945812808, train_loss: 1.3590433270439082, val_loss: 1.6245218027988677 (12 / 30)
train_acc: 0.4103831891223733, val_acc: 0.37438423645320196, train_loss: 1.306594993629031, val_loss: 1.6387838747701033 (13 / 30)
train_acc: 0.4338689740420272, val_acc: 0.41379310344827586, train_loss: 1.3323362074027987, val_loss: 1.3787524612079114 (14 / 30)
train_acc: 0.4672435105067985, val_acc: 0.45320197044334976, train_loss: 1.2654090384470371, val_loss: 1.43546462059021 (15 / 30)
train_acc: 0.4276885043263288, val_acc: 0.45320197044334976, train_loss: 1.3766369442415178, val_loss: 1.386273781067045 (16 / 30)
train_acc: 0.4783683559950556, val_acc: 0.3891625615763547, train_loss: 1.1777972509009287, val_loss: 1.4579296763894594 (17 / 30)
train_acc: 0.5080346106304079, val_acc: 0.43842364532019706, train_loss: 1.1654305447754076, val_loss: 1.5443142165104156 (18 / 30)
train_acc: 0.6106304079110012, val_acc: 0.5517241379310345, train_loss: 0.9041813644402107, val_loss: 1.2079971506090588 (19 / 30)
train_acc: 0.6588380716934487, val_acc: 0.5763546798029556, train_loss: 0.8341651708440226, val_loss: 1.1761996575764246 (20 / 30)
train_acc: 0.7132262051915945, val_acc: 0.541871921182266, train_loss: 0.7362804784173577, val_loss: 1.202874554789125 (21 / 30)
train_acc: 0.7181705809641533, val_acc: 0.5270935960591133, train_loss: 0.7392651421767378, val_loss: 1.2512758688386438 (22 / 30)
train_acc: 0.7342398022249691, val_acc: 0.5024630541871922, train_loss: 0.6762938464085753, val_loss: 1.479696458783643 (23 / 30)
train_acc: 0.7935723114956736, val_acc: 0.5911330049261084, train_loss: 0.5266482077069277, val_loss: 1.2441625970925017 (24 / 30)
train_acc: 0.830655129789864, val_acc: 0.5615763546798029, train_loss: 0.48401681248130846, val_loss: 1.355937771609264 (25 / 30)
train_acc: 0.8207663782447466, val_acc: 0.5270935960591133, train_loss: 0.47293275602079, val_loss: 1.5287877943715438 (26 / 30)
train_acc: 0.857849196538937, val_acc: 0.5320197044334976, train_loss: 0.44219333928064597, val_loss: 1.4982121736545282 (27 / 30)
train_acc: 0.8603213844252163, val_acc: 0.5763546798029556, train_loss: 0.41220784408347716, val_loss: 1.3429665518511693 (28 / 30)
train_acc: 0.861557478368356, val_acc: 0.5073891625615764, train_loss: 0.38012398364959454, val_loss: 1.5705638331145488 (29 / 30)
train_acc: 0.8479604449938195, val_acc: 0.5615763546798029, train_loss: 0.43865249257152544, val_loss: 1.4605240839455516 (30 / 30)
lr 0.0024294902368032162, batch 8, decay 2.4613300639705193e-06, gamma 0.1162206247261106, val accuracy 0.5911330049261084, val loss 1.2441625970925017 [7 / 20]
---------------------------------------------
train_acc: 0.28553770086526575, val_acc: 0.28078817733990147, train_loss: 2.091251953865306, val_loss: 3.168967931141407 (1 / 30)
train_acc: 0.29295426452410384, val_acc: 0.3103448275862069, train_loss: 2.258145470849074, val_loss: 2.4493237093751654 (2 / 30)
train_acc: 0.311495673671199, val_acc: 0.33497536945812806, train_loss: 2.04367767364634, val_loss: 1.6666993865825859 (3 / 30)
train_acc: 0.30902348578491967, val_acc: 0.3054187192118227, train_loss: 1.800914296997787, val_loss: 1.5155420547048446 (4 / 30)
train_acc: 0.3683559950556242, val_acc: 0.4187192118226601, train_loss: 1.6198753980535217, val_loss: 1.4934572621519342 (5 / 30)
train_acc: 0.35970333745364647, val_acc: 0.3891625615763547, train_loss: 1.5831995981438052, val_loss: 1.4838425555252677 (6 / 30)
train_acc: 0.4079110012360939, val_acc: 0.3842364532019704, train_loss: 1.4640838691713784, val_loss: 1.6320801427211669 (7 / 30)
train_acc: 0.43016069221260816, val_acc: 0.3891625615763547, train_loss: 1.4423847398887606, val_loss: 1.380030456141298 (8 / 30)
train_acc: 0.4289245982694685, val_acc: 0.4088669950738916, train_loss: 1.457191835669858, val_loss: 1.4859273145938743 (9 / 30)
train_acc: 0.4338689740420272, val_acc: 0.41379310344827586, train_loss: 1.4273199366108595, val_loss: 1.4532915066028465 (10 / 30)
train_acc: 0.4635352286773795, val_acc: 0.35960591133004927, train_loss: 1.3830653769418837, val_loss: 1.6858699028127886 (11 / 30)
train_acc: 0.48825710754017304, val_acc: 0.43349753694581283, train_loss: 1.254291044030113, val_loss: 1.411989762278026 (12 / 30)
train_acc: 0.5067985166872683, val_acc: 0.39901477832512317, train_loss: 1.2581368353222444, val_loss: 1.52221079001873 (13 / 30)
train_acc: 0.5624227441285538, val_acc: 0.43842364532019706, train_loss: 1.119211435612701, val_loss: 1.9175543456242001 (14 / 30)
train_acc: 0.5179233621755254, val_acc: 0.4975369458128079, train_loss: 1.1836958793538757, val_loss: 1.3190421570698028 (15 / 30)
train_acc: 0.5364647713226205, val_acc: 0.4088669950738916, train_loss: 1.1447644174762062, val_loss: 1.717902729076705 (16 / 30)
train_acc: 0.5389369592088998, val_acc: 0.42857142857142855, train_loss: 1.1494562543366837, val_loss: 1.5655327947268933 (17 / 30)
train_acc: 0.5908529048207664, val_acc: 0.5172413793103449, train_loss: 1.051364780651182, val_loss: 1.2989413714761218 (18 / 30)
train_acc: 0.6687268232385661, val_acc: 0.4975369458128079, train_loss: 0.8361167901820689, val_loss: 1.5263413349395902 (19 / 30)
train_acc: 0.7330037082818294, val_acc: 0.4827586206896552, train_loss: 0.7273710093775255, val_loss: 1.5489496022022415 (20 / 30)
train_acc: 0.7008652657601978, val_acc: 0.4630541871921182, train_loss: 0.7794522655614374, val_loss: 1.5382557890098083 (21 / 30)
train_acc: 0.7194066749072929, val_acc: 0.49261083743842365, train_loss: 0.741596229438876, val_loss: 1.4397308767722745 (22 / 30)
train_acc: 0.7218788627935723, val_acc: 0.4876847290640394, train_loss: 0.6698513237301292, val_loss: 1.6677440968640331 (23 / 30)
train_acc: 0.7428924598269468, val_acc: 0.5221674876847291, train_loss: 0.6295456258563972, val_loss: 1.6100457301868007 (24 / 30)
train_acc: 0.6897404202719407, val_acc: 0.5270935960591133, train_loss: 0.751825457008277, val_loss: 1.4133488579923883 (25 / 30)
train_acc: 0.7428924598269468, val_acc: 0.5024630541871922, train_loss: 0.6947767778733752, val_loss: 1.580092074248591 (26 / 30)
train_acc: 0.7676143386897404, val_acc: 0.4729064039408867, train_loss: 0.5968139202544657, val_loss: 1.5058563060948413 (27 / 30)
train_acc: 0.7898640296662547, val_acc: 0.4876847290640394, train_loss: 0.5922372927624452, val_loss: 1.6899546983794038 (28 / 30)

---------------------------------------------------------------------------

KeyboardInterrupt                         Traceback (most recent call last)

<ipython-input-5-7c6ee9e79dc3> in <module>()
     39 
     40   net = resnet152(num_classes=NUM_CLASSES)
---> 41   current_net, val_accuracy, val_loss = train_network(net, net.parameters(), LR, NUM_EPOCHS, BATCH_SIZE, WEIGHT_DECAY, STEP_SIZE, GAMMA, train_dataset, val_dataset=val_dataset, verbosity=True)
     42   val_accuracies.append(val_accuracy)
     43   val_losses.append(val_loss)

2 frames

/usr/local/lib/python3.6/dist-packages/torch/optim/sgd.py in step(self, closure)
     98                     else:
     99                         buf = param_state['momentum_buffer']
--> 100                         buf.mul_(momentum).add_(1 - dampening, d_p)
    101                     if nesterov:
    102                         d_p = d_p.add(momentum, buf)

KeyboardInterrupt: 

