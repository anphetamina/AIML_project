training set 809
validation set 203
-------------------------------------
train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.7818401351994726, val_loss: 1.7612010035021552 (1 / 30)
train_acc: 0.20024721878862795, val_acc: 0.18719211822660098, train_loss: 1.7477612429113116, val_loss: 1.772157180485467 (2 / 30)
train_acc: 0.2546353522867738, val_acc: 0.32019704433497537, train_loss: 1.6983599371020077, val_loss: 1.689666474981261 (3 / 30)
train_acc: 0.34981458590852904, val_acc: 0.3497536945812808, train_loss: 1.5961497305939607, val_loss: 1.5796566320757561 (4 / 30)
train_acc: 0.32756489493201485, val_acc: 0.33497536945812806, train_loss: 1.6019731659824386, val_loss: 1.569179624759505 (5 / 30)
train_acc: 0.35599505562422745, val_acc: 0.3448275862068966, train_loss: 1.5383074131825356, val_loss: 1.5531182077717898 (6 / 30)
train_acc: 0.3522867737948084, val_acc: 0.3645320197044335, train_loss: 1.5473612530711849, val_loss: 1.545136965554336 (7 / 30)
train_acc: 0.37453646477132263, val_acc: 0.3448275862068966, train_loss: 1.520615420618517, val_loss: 1.5492324095054213 (8 / 30)
train_acc: 0.3856613102595797, val_acc: 0.3054187192118227, train_loss: 1.4727512216391168, val_loss: 1.5580859184265137 (9 / 30)
train_acc: 0.315203955500618, val_acc: 0.2857142857142857, train_loss: 1.5884102877639281, val_loss: 1.6450343032188603 (10 / 30)
train_acc: 0.3584672435105068, val_acc: 0.37438423645320196, train_loss: 1.500381412258549, val_loss: 1.4938071578594263 (11 / 30)
train_acc: 0.4042027194066749, val_acc: 0.3497536945812808, train_loss: 1.4628629451629112, val_loss: 1.5016439213541342 (12 / 30)
train_acc: 0.37082818294190356, val_acc: 0.3399014778325123, train_loss: 1.5037374956204068, val_loss: 1.6073696155266222 (13 / 30)
train_acc: 0.40296662546353523, val_acc: 0.3694581280788177, train_loss: 1.465084793099072, val_loss: 1.4645504886880885 (14 / 30)
train_acc: 0.4079110012360939, val_acc: 0.3448275862068966, train_loss: 1.4300822155142596, val_loss: 1.4659470578132592 (15 / 30)
train_acc: 0.41656365883807167, val_acc: 0.3645320197044335, train_loss: 1.4446415845189606, val_loss: 1.4615041710473047 (16 / 30)
train_acc: 0.42027194066749074, val_acc: 0.3891625615763547, train_loss: 1.3978710660828353, val_loss: 1.4204869881052102 (17 / 30)
train_acc: 0.41409147095179233, val_acc: 0.4433497536945813, train_loss: 1.4002893256905347, val_loss: 1.415363531981783 (18 / 30)
train_acc: 0.446229913473424, val_acc: 0.39901477832512317, train_loss: 1.3486487128678595, val_loss: 1.4334271864350794 (19 / 30)
train_acc: 0.4227441285537701, val_acc: 0.4433497536945813, train_loss: 1.3594496107219323, val_loss: 1.3731705684379991 (20 / 30)
train_acc: 0.41285537700865266, val_acc: 0.41379310344827586, train_loss: 1.3693282698524012, val_loss: 1.4186115006507911 (21 / 30)
train_acc: 0.3621755253399258, val_acc: 0.39408866995073893, train_loss: 1.567086497402309, val_loss: 1.6103903524981344 (22 / 30)
train_acc: 0.44252163164400493, val_acc: 0.45320197044334976, train_loss: 1.335751688819291, val_loss: 1.3546489066091076 (23 / 30)
train_acc: 0.4758961681087763, val_acc: 0.43349753694581283, train_loss: 1.2780493690882095, val_loss: 1.3343780105337133 (24 / 30)
train_acc: 0.43757725587144625, val_acc: 0.458128078817734, train_loss: 1.3230057416946543, val_loss: 1.3592036445739821 (25 / 30)
train_acc: 0.47342398022249693, val_acc: 0.43842364532019706, train_loss: 1.2485577657874052, val_loss: 1.3848158778815434 (26 / 30)
train_acc: 0.48825710754017304, val_acc: 0.4630541871921182, train_loss: 1.2223651559450128, val_loss: 1.3341484369315537 (27 / 30)
train_acc: 0.5092707045735476, val_acc: 0.46798029556650245, train_loss: 1.1794312495383699, val_loss: 1.2481374441109268 (28 / 30)
train_acc: 0.5253399258343634, val_acc: 0.4630541871921182, train_loss: 1.136108255062174, val_loss: 1.2968917715138402 (29 / 30)
train_acc: 0.5290482076637825, val_acc: 0.49261083743842365, train_loss: 1.1226755186123372, val_loss: 1.2368561722374902 (30 / 30)
lr 0.0009998869991768619, batch 8, decay 1.642703159576562e-06, gamma 0.9539020293887379, val accuracy 0.49261083743842365, val loss 1.2368561722374902 [1 / 20]
-------------------------------------
train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.7823306812786202, val_loss: 1.7577090527623744 (1 / 30)
train_acc: 0.1965389369592089, val_acc: 0.23645320197044334, train_loss: 1.7558516297853184, val_loss: 1.740254361641231 (2 / 30)
train_acc: 0.19406674907292953, val_acc: 0.29064039408866993, train_loss: 1.734920016620009, val_loss: 1.697434333157657 (3 / 30)
train_acc: 0.26328800988875156, val_acc: 0.2315270935960591, train_loss: 1.6966977840006279, val_loss: 1.7051008270291859 (4 / 30)
train_acc: 0.30284301606922126, val_acc: 0.270935960591133, train_loss: 1.613750980427887, val_loss: 1.592370848350337 (5 / 30)
train_acc: 0.3189122373300371, val_acc: 0.3448275862068966, train_loss: 1.5890529369394328, val_loss: 1.5633656984479556 (6 / 30)
train_acc: 0.36341161928306553, val_acc: 0.3645320197044335, train_loss: 1.4630724583626973, val_loss: 1.4129578303820982 (7 / 30)
train_acc: 0.4004944375772559, val_acc: 0.3891625615763547, train_loss: 1.4458350940746785, val_loss: 1.471329766541279 (8 / 30)
train_acc: 0.3757725587144623, val_acc: 0.43349753694581283, train_loss: 1.4272687066765444, val_loss: 1.4533537925757798 (9 / 30)
train_acc: 0.3584672435105068, val_acc: 0.41379310344827586, train_loss: 1.4059220704218955, val_loss: 1.4430945923762957 (10 / 30)
train_acc: 0.40914709517923364, val_acc: 0.43842364532019706, train_loss: 1.3945481361653218, val_loss: 1.3042121702814338 (11 / 30)
train_acc: 0.41656365883807167, val_acc: 0.39901477832512317, train_loss: 1.3252049709574696, val_loss: 1.39903964021523 (12 / 30)
train_acc: 0.41409147095179233, val_acc: 0.41379310344827586, train_loss: 1.3382461552861595, val_loss: 1.3984302461440927 (13 / 30)
train_acc: 0.45488257107540175, val_acc: 0.43349753694581283, train_loss: 1.2802823807017323, val_loss: 1.269118714508752 (14 / 30)
train_acc: 0.453646477132262, val_acc: 0.4433497536945813, train_loss: 1.277837485406543, val_loss: 1.2461545819719437 (15 / 30)
train_acc: 0.48702101359703337, val_acc: 0.43349753694581283, train_loss: 1.2036604674991187, val_loss: 1.3550532093189034 (16 / 30)
train_acc: 0.44128553770086526, val_acc: 0.4975369458128079, train_loss: 1.332327880287642, val_loss: 1.2360909836632865 (17 / 30)
train_acc: 0.4932014833127318, val_acc: 0.4187192118226601, train_loss: 1.1806986234686137, val_loss: 1.3343725192723015 (18 / 30)
train_acc: 0.553770086526576, val_acc: 0.4827586206896552, train_loss: 1.0441597492350048, val_loss: 1.1909484223191962 (19 / 30)
train_acc: 0.5760197775030902, val_acc: 0.49261083743842365, train_loss: 1.014183905569673, val_loss: 1.161918526799808 (20 / 30)
train_acc: 0.5896168108776267, val_acc: 0.4729064039408867, train_loss: 0.9638558250422531, val_loss: 1.1604289256880436 (21 / 30)
train_acc: 0.630407911001236, val_acc: 0.5073891625615764, train_loss: 0.8998455520908087, val_loss: 1.1972156199328419 (22 / 30)
train_acc: 0.622991347342398, val_acc: 0.49261083743842365, train_loss: 0.8950701882724267, val_loss: 1.3442561285836356 (23 / 30)
train_acc: 0.6390605686032138, val_acc: 0.46798029556650245, train_loss: 0.8705451896665123, val_loss: 1.1726953501771824 (24 / 30)
train_acc: 0.65389369592089, val_acc: 0.5123152709359606, train_loss: 0.8407677310358903, val_loss: 1.1507761073229936 (25 / 30)
train_acc: 0.6909765142150803, val_acc: 0.5123152709359606, train_loss: 0.7817542874179458, val_loss: 1.1697591860306087 (26 / 30)
train_acc: 0.6588380716934487, val_acc: 0.5172413793103449, train_loss: 0.7699966150691689, val_loss: 1.1131742705265288 (27 / 30)
train_acc: 0.7428924598269468, val_acc: 0.5566502463054187, train_loss: 0.6859301375812299, val_loss: 1.1566629932431751 (28 / 30)
train_acc: 0.7317676143386898, val_acc: 0.5517241379310345, train_loss: 0.7004101856971995, val_loss: 1.080320247288408 (29 / 30)
train_acc: 0.7342398022249691, val_acc: 0.5812807881773399, train_loss: 0.6322625302266426, val_loss: 1.0857965740664253 (30 / 30)
lr 0.00092919697788403, batch 8, decay 8.68332005024181e-06, gamma 0.34884675617814215, val accuracy 0.5812807881773399, val loss 1.0857965740664253 [2 / 20]
-------------------------------------
train_acc: 0.1681087762669963, val_acc: 0.19704433497536947, train_loss: 1.783683272168427, val_loss: 1.761778186107504 (1 / 30)
train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7657182117917187, val_loss: 1.7506738155346198 (2 / 30)
train_acc: 0.22373300370828184, val_acc: 0.3251231527093596, train_loss: 1.7324827931572686, val_loss: 1.6791294848390401 (3 / 30)
train_acc: 0.2954264524103832, val_acc: 0.35467980295566504, train_loss: 1.6576674408906764, val_loss: 1.546392140717342 (4 / 30)
train_acc: 0.34239802224969096, val_acc: 0.32019704433497537, train_loss: 1.5521826950375024, val_loss: 1.5822042684836928 (5 / 30)
train_acc: 0.33868974042027195, val_acc: 0.3793103448275862, train_loss: 1.511374686645489, val_loss: 1.4675638881222954 (6 / 30)
train_acc: 0.37330037082818296, val_acc: 0.4039408866995074, train_loss: 1.4463481278295718, val_loss: 1.3482009078481514 (7 / 30)
train_acc: 0.35599505562422745, val_acc: 0.35960591133004927, train_loss: 1.430817723126877, val_loss: 1.4638309854592009 (8 / 30)
train_acc: 0.39060568603213847, val_acc: 0.42857142857142855, train_loss: 1.3929649551660375, val_loss: 1.4445017940305136 (9 / 30)
train_acc: 0.3992583436341162, val_acc: 0.458128078817734, train_loss: 1.3924180983907655, val_loss: 1.3384859098002242 (10 / 30)
train_acc: 0.23362175525339926, val_acc: 0.2561576354679803, train_loss: 1.717654306308006, val_loss: 1.7318805914207045 (11 / 30)
train_acc: 0.3176761433868974, val_acc: 0.3054187192118227, train_loss: 1.5955615081952146, val_loss: 1.5506075021668608 (12 / 30)
train_acc: 0.3127317676143387, val_acc: 0.3251231527093596, train_loss: 1.5693203781090208, val_loss: 1.5180772865934324 (13 / 30)
train_acc: 0.3967861557478368, val_acc: 0.42857142857142855, train_loss: 1.419660070770603, val_loss: 1.3761645496772428 (14 / 30)
train_acc: 0.38936959208899874, val_acc: 0.4088669950738916, train_loss: 1.399273796340886, val_loss: 1.4124661965910437 (15 / 30)
train_acc: 0.4004944375772559, val_acc: 0.4236453201970443, train_loss: 1.4318496908334042, val_loss: 1.3467013289775756 (16 / 30)
train_acc: 0.3856613102595797, val_acc: 0.4039408866995074, train_loss: 1.3878712223840437, val_loss: 1.417256148577911 (17 / 30)
train_acc: 0.4227441285537701, val_acc: 0.4630541871921182, train_loss: 1.3019572878060737, val_loss: 1.27554635754947 (18 / 30)
train_acc: 0.4573547589616811, val_acc: 0.46798029556650245, train_loss: 1.2302825639510184, val_loss: 1.2644607368948424 (19 / 30)
train_acc: 0.4969097651421508, val_acc: 0.4827586206896552, train_loss: 1.2014744829925235, val_loss: 1.2329746860588713 (20 / 30)
train_acc: 0.484548825710754, val_acc: 0.4729064039408867, train_loss: 1.1992093833032142, val_loss: 1.1925663768951529 (21 / 30)
train_acc: 0.4907292954264524, val_acc: 0.4827586206896552, train_loss: 1.1818888237508767, val_loss: 1.2098578766648993 (22 / 30)
train_acc: 0.5142150803461063, val_acc: 0.4876847290640394, train_loss: 1.1470820464663511, val_loss: 1.2005009536672695 (23 / 30)
train_acc: 0.5377008652657602, val_acc: 0.4975369458128079, train_loss: 1.1113719278566623, val_loss: 1.1967531201874682 (24 / 30)
train_acc: 0.546353522867738, val_acc: 0.4975369458128079, train_loss: 1.1084090406431992, val_loss: 1.1710508873897234 (25 / 30)
train_acc: 0.5512978986402967, val_acc: 0.5172413793103449, train_loss: 1.075697383715579, val_loss: 1.152786671821707 (26 / 30)
train_acc: 0.5834363411619283, val_acc: 0.5517241379310345, train_loss: 1.0420807249319126, val_loss: 1.1534988040407304 (27 / 30)
train_acc: 0.5933250927070457, val_acc: 0.5517241379310345, train_loss: 1.0269690352700402, val_loss: 1.113041044456031 (28 / 30)
train_acc: 0.6019777503090235, val_acc: 0.5566502463054187, train_loss: 0.9923628260679681, val_loss: 1.1054482013721185 (29 / 30)
train_acc: 0.5945611866501854, val_acc: 0.5467980295566502, train_loss: 0.9877538209497855, val_loss: 1.1337685173955456 (30 / 30)
lr 0.0009047073363434544, batch 8, decay 3.578929287034246e-07, gamma 0.32103276895312094, val accuracy 0.5566502463054187, val loss 1.1054482013721185 [3 / 20]
-------------------------------------
train_acc: 0.17799752781211373, val_acc: 0.2857142857142857, train_loss: 1.783260633977883, val_loss: 1.7661672507600832 (1 / 30)
train_acc: 0.1903584672435105, val_acc: 0.27586206896551724, train_loss: 1.7580336373580254, val_loss: 1.7251296207822602 (2 / 30)
train_acc: 0.22249690976514216, val_acc: 0.1921182266009852, train_loss: 1.7256512568229798, val_loss: 1.6980238583287581 (3 / 30)
train_acc: 0.2620519159456119, val_acc: 0.3251231527093596, train_loss: 1.6724713940555587, val_loss: 1.6184053473871918 (4 / 30)
train_acc: 0.3288009888751545, val_acc: 0.3645320197044335, train_loss: 1.6206301804378684, val_loss: 1.5127086410381523 (5 / 30)
train_acc: 0.36093943139678614, val_acc: 0.3842364532019704, train_loss: 1.5255983686270318, val_loss: 1.4948887866118858 (6 / 30)
train_acc: 0.3374536464771323, val_acc: 0.31527093596059114, train_loss: 1.5806829336989205, val_loss: 1.5626100959448979 (7 / 30)
train_acc: 0.3547589616810878, val_acc: 0.3793103448275862, train_loss: 1.4866110911622772, val_loss: 1.5192361283184859 (8 / 30)
train_acc: 0.3683559950556242, val_acc: 0.39901477832512317, train_loss: 1.4422196525578446, val_loss: 1.3598022654726 (9 / 30)
train_acc: 0.3868974042027194, val_acc: 0.3694581280788177, train_loss: 1.4161675885522322, val_loss: 1.5067920567366877 (10 / 30)
train_acc: 0.4054388133498146, val_acc: 0.4433497536945813, train_loss: 1.3846923077504332, val_loss: 1.3532866245420108 (11 / 30)
train_acc: 0.3980222496909765, val_acc: 0.37438423645320196, train_loss: 1.3681364949465682, val_loss: 1.4242657481743197 (12 / 30)
train_acc: 0.4177997527812114, val_acc: 0.3645320197044335, train_loss: 1.3194501422096978, val_loss: 1.4246121316120541 (13 / 30)
train_acc: 0.43139678615574784, val_acc: 0.4975369458128079, train_loss: 1.3152199324924954, val_loss: 1.2386913604924243 (14 / 30)
train_acc: 0.40173053152039556, val_acc: 0.43842364532019706, train_loss: 1.280347308358097, val_loss: 1.27351565842558 (15 / 30)
train_acc: 0.4363411619283066, val_acc: 0.4236453201970443, train_loss: 1.2492186106621703, val_loss: 1.3392088548303238 (16 / 30)
train_acc: 0.4561186650185414, val_acc: 0.4236453201970443, train_loss: 1.2189047961358823, val_loss: 1.258571300306931 (17 / 30)
train_acc: 0.4610630407911001, val_acc: 0.4482758620689655, train_loss: 1.2227744967740015, val_loss: 1.2319729674625866 (18 / 30)
train_acc: 0.5129789864029666, val_acc: 0.5123152709359606, train_loss: 1.118487098161016, val_loss: 1.1628887527094687 (19 / 30)
train_acc: 0.5253399258343634, val_acc: 0.4729064039408867, train_loss: 1.0929142406166557, val_loss: 1.2259820012623452 (20 / 30)
train_acc: 0.5290482076637825, val_acc: 0.5123152709359606, train_loss: 1.0730873001520653, val_loss: 1.1464340974544656 (21 / 30)
train_acc: 0.5512978986402967, val_acc: 0.43842364532019706, train_loss: 1.0224499454899094, val_loss: 1.2289224589986754 (22 / 30)
train_acc: 0.5710754017305315, val_acc: 0.5123152709359606, train_loss: 0.987169469979549, val_loss: 1.1560348222408388 (23 / 30)
train_acc: 0.5834363411619283, val_acc: 0.46798029556650245, train_loss: 0.9617691423159153, val_loss: 1.3039606812552278 (24 / 30)
train_acc: 0.5859085290482077, val_acc: 0.5221674876847291, train_loss: 0.9666849962713102, val_loss: 1.1487291678419254 (25 / 30)
train_acc: 0.5982694684796045, val_acc: 0.5320197044334976, train_loss: 0.952233731230935, val_loss: 1.1848222269800497 (26 / 30)
train_acc: 0.6168108776266996, val_acc: 0.5123152709359606, train_loss: 0.8860475580830804, val_loss: 1.1147833663254536 (27 / 30)
train_acc: 0.6180469715698393, val_acc: 0.5172413793103449, train_loss: 0.8841873775748593, val_loss: 1.2884316485503624 (28 / 30)
train_acc: 0.65389369592089, val_acc: 0.541871921182266, train_loss: 0.8443545903204692, val_loss: 1.1317234600118815 (29 / 30)
train_acc: 0.6588380716934487, val_acc: 0.43842364532019706, train_loss: 0.830773461292347, val_loss: 1.2454901848520552 (30 / 30)
lr 0.0008596317972511528, batch 8, decay 2.592255798399596e-07, gamma 0.4074959629351274, val accuracy 0.541871921182266, val loss 1.1317234600118815 [4 / 20]
-------------------------------------
train_acc: 0.18046971569839307, val_acc: 0.2413793103448276, train_loss: 1.7777597132070988, val_loss: 1.7552077764360776 (1 / 30)
train_acc: 0.21508034610630408, val_acc: 0.31527093596059114, train_loss: 1.7543365053547033, val_loss: 1.732384214260308 (2 / 30)
train_acc: 0.2880098887515451, val_acc: 0.32019704433497537, train_loss: 1.6955875973176897, val_loss: 1.6666061983907163 (3 / 30)
train_acc: 0.3016069221260816, val_acc: 0.3054187192118227, train_loss: 1.5982984445887827, val_loss: 1.7049897462863641 (4 / 30)
train_acc: 0.33250927070457353, val_acc: 0.28078817733990147, train_loss: 1.5291749414319014, val_loss: 1.6649733599770833 (5 / 30)
train_acc: 0.3572311495673671, val_acc: 0.37438423645320196, train_loss: 1.5039475344314859, val_loss: 1.4898846161189339 (6 / 30)
train_acc: 0.3584672435105068, val_acc: 0.3891625615763547, train_loss: 1.4452190570100423, val_loss: 1.5072563992345274 (7 / 30)
train_acc: 0.380716934487021, val_acc: 0.4187192118226601, train_loss: 1.4547959328876585, val_loss: 1.4090573681986391 (8 / 30)
train_acc: 0.3794808405438813, val_acc: 0.43842364532019706, train_loss: 1.3988323628681405, val_loss: 1.3239022058806396 (9 / 30)
train_acc: 0.4004944375772559, val_acc: 0.3694581280788177, train_loss: 1.3955798809254272, val_loss: 1.4738967894333337 (10 / 30)
train_acc: 0.3992583436341162, val_acc: 0.4630541871921182, train_loss: 1.3647461223366528, val_loss: 1.341117696809064 (11 / 30)
train_acc: 0.4103831891223733, val_acc: 0.4236453201970443, train_loss: 1.3501080563395516, val_loss: 1.3190592374707677 (12 / 30)
train_acc: 0.4388133498145859, val_acc: 0.3793103448275862, train_loss: 1.2902846575078033, val_loss: 1.4634083227571009 (13 / 30)
train_acc: 0.43510506798516685, val_acc: 0.42857142857142855, train_loss: 1.292258612747098, val_loss: 1.3409334890948141 (14 / 30)
train_acc: 0.45982694684796044, val_acc: 0.45320197044334976, train_loss: 1.223018073328346, val_loss: 1.2542055445938862 (15 / 30)
train_acc: 0.4783683559950556, val_acc: 0.43349753694581283, train_loss: 1.2120925546134504, val_loss: 1.2723744297262483 (16 / 30)
train_acc: 0.5166872682323856, val_acc: 0.3793103448275862, train_loss: 1.1792921065105348, val_loss: 1.3182333326104827 (17 / 30)
train_acc: 0.4969097651421508, val_acc: 0.458128078817734, train_loss: 1.1933335457802998, val_loss: 1.197759152633216 (18 / 30)
train_acc: 0.5735475896168108, val_acc: 0.5172413793103449, train_loss: 1.0205223050488825, val_loss: 1.1546252965927124 (19 / 30)
train_acc: 0.6019777503090235, val_acc: 0.5073891625615764, train_loss: 0.9704311026630944, val_loss: 1.1783693711745915 (20 / 30)
train_acc: 0.6106304079110012, val_acc: 0.4975369458128079, train_loss: 0.9447476519053003, val_loss: 1.1560541485330742 (21 / 30)
train_acc: 0.6032138442521632, val_acc: 0.5024630541871922, train_loss: 0.9274330249675566, val_loss: 1.2167506382383149 (22 / 30)
train_acc: 0.6477132262051916, val_acc: 0.5024630541871922, train_loss: 0.9176164958326719, val_loss: 1.161302787329763 (23 / 30)
train_acc: 0.6514215080346106, val_acc: 0.5123152709359606, train_loss: 0.8884235616078041, val_loss: 1.1820622518144805 (24 / 30)
train_acc: 0.6514215080346106, val_acc: 0.5073891625615764, train_loss: 0.8629170301081371, val_loss: 1.1300688253834916 (25 / 30)
train_acc: 0.6637824474660075, val_acc: 0.5024630541871922, train_loss: 0.8430265941195494, val_loss: 1.185939683702779 (26 / 30)
train_acc: 0.6625463535228677, val_acc: 0.5221674876847291, train_loss: 0.811659138193826, val_loss: 1.1620961063601114 (27 / 30)
train_acc: 0.6736711990111248, val_acc: 0.5123152709359606, train_loss: 0.7910607037939159, val_loss: 1.1534328255160102 (28 / 30)
train_acc: 0.6736711990111248, val_acc: 0.5073891625615764, train_loss: 0.7909573181008527, val_loss: 1.192962500262143 (29 / 30)
train_acc: 0.7021013597033374, val_acc: 0.5073891625615764, train_loss: 0.7767812359907719, val_loss: 1.1626372437171748 (30 / 30)
lr 0.0009747945249084196, batch 8, decay 4.516544849061572e-07, gamma 0.12543747662707058, val accuracy 0.5221674876847291, val loss 1.1620961063601114 [5 / 20]
-------------------------------------
train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.783662517816381, val_loss: 1.7635863761009254 (1 / 30)
train_acc: 0.21013597033374537, val_acc: 0.18226600985221675, train_loss: 1.7614175457005448, val_loss: 1.753829363531667 (2 / 30)
train_acc: 0.23856613102595797, val_acc: 0.22660098522167488, train_loss: 1.7359996334141943, val_loss: 1.7008131037791963 (3 / 30)
train_acc: 0.31025957972805934, val_acc: 0.2857142857142857, train_loss: 1.642523660353294, val_loss: 1.6397675986360447 (4 / 30)
train_acc: 0.30284301606922126, val_acc: 0.35960591133004927, train_loss: 1.5694130778460036, val_loss: 1.4376328079571277 (5 / 30)
train_acc: 0.34981458590852904, val_acc: 0.3448275862068966, train_loss: 1.5087573448718699, val_loss: 1.5547756232651584 (6 / 30)
train_acc: 0.34981458590852904, val_acc: 0.37438423645320196, train_loss: 1.5006027846460142, val_loss: 1.4372619554914277 (7 / 30)
train_acc: 0.37330037082818296, val_acc: 0.3694581280788177, train_loss: 1.414410609986785, val_loss: 1.3769332387764466 (8 / 30)
train_acc: 0.3695920889987639, val_acc: 0.39408866995073893, train_loss: 1.4640743776658556, val_loss: 1.3883416406039535 (9 / 30)
train_acc: 0.380716934487021, val_acc: 0.3793103448275862, train_loss: 1.388237617219187, val_loss: 1.348741230706276 (10 / 30)
train_acc: 0.411619283065513, val_acc: 0.4039408866995074, train_loss: 1.3551865256171587, val_loss: 1.3420154537473405 (11 / 30)
train_acc: 0.4326328800988875, val_acc: 0.4088669950738916, train_loss: 1.335114485695866, val_loss: 1.449099751528848 (12 / 30)
train_acc: 0.44128553770086526, val_acc: 0.4088669950738916, train_loss: 1.3219523291357957, val_loss: 1.4139057967463151 (13 / 30)
train_acc: 0.41903584672435107, val_acc: 0.42857142857142855, train_loss: 1.3389224248704568, val_loss: 1.2817907967590934 (14 / 30)
train_acc: 0.4511742892459827, val_acc: 0.47783251231527096, train_loss: 1.301855667559856, val_loss: 1.2671695612921503 (15 / 30)
train_acc: 0.4561186650185414, val_acc: 0.4729064039408867, train_loss: 1.2511726719487288, val_loss: 1.2066884839476035 (16 / 30)
train_acc: 0.4820766378244747, val_acc: 0.42857142857142855, train_loss: 1.2065046758970017, val_loss: 1.311119872948219 (17 / 30)
train_acc: 0.5030902348578492, val_acc: 0.4187192118226601, train_loss: 1.1863064491851958, val_loss: 1.2805807144183832 (18 / 30)
train_acc: 0.5364647713226205, val_acc: 0.5123152709359606, train_loss: 1.1068048630420888, val_loss: 1.214203021796466 (19 / 30)
train_acc: 0.546353522867738, val_acc: 0.5024630541871922, train_loss: 1.0671234170644923, val_loss: 1.165660832903068 (20 / 30)
train_acc: 0.5772558714462299, val_acc: 0.4433497536945813, train_loss: 1.0134739171441907, val_loss: 1.2961442558636218 (21 / 30)
train_acc: 0.5896168108776267, val_acc: 0.5123152709359606, train_loss: 1.0050270466044158, val_loss: 1.174111457293844 (22 / 30)
train_acc: 0.5735475896168108, val_acc: 0.4975369458128079, train_loss: 0.9969561615744686, val_loss: 1.2804812697941446 (23 / 30)
train_acc: 0.61557478368356, val_acc: 0.4876847290640394, train_loss: 0.9214245950335772, val_loss: 1.2457835040068979 (24 / 30)
train_acc: 0.6341161928306551, val_acc: 0.5221674876847291, train_loss: 0.8972187316314546, val_loss: 1.1992514139325747 (25 / 30)
train_acc: 0.6353522867737948, val_acc: 0.4827586206896552, train_loss: 0.8870323594332625, val_loss: 1.2257586065771544 (26 / 30)
train_acc: 0.6514215080346106, val_acc: 0.4876847290640394, train_loss: 0.8423588632505815, val_loss: 1.2282412918330414 (27 / 30)
train_acc: 0.6452410383189122, val_acc: 0.5024630541871922, train_loss: 0.8307518738014589, val_loss: 1.3082299496739955 (28 / 30)
train_acc: 0.65389369592089, val_acc: 0.5024630541871922, train_loss: 0.8376876351565159, val_loss: 1.218301007900332 (29 / 30)
train_acc: 0.6786155747836835, val_acc: 0.5024630541871922, train_loss: 0.7571765813603537, val_loss: 1.3384363069910135 (30 / 30)
lr 0.0008357561765034875, batch 8, decay 8.10591224064222e-07, gamma 0.5425851948804319, val accuracy 0.5221674876847291, val loss 1.1992514139325747 [6 / 20]
-------------------------------------
train_acc: 0.19283065512978986, val_acc: 0.19704433497536947, train_loss: 1.7810424995068714, val_loss: 1.75749447134328 (1 / 30)
train_acc: 0.19901112484548825, val_acc: 0.18226600985221675, train_loss: 1.7662816937685897, val_loss: 1.7444173778806413 (2 / 30)
train_acc: 0.2088998763906057, val_acc: 0.31527093596059114, train_loss: 1.736319503913851, val_loss: 1.7163571841611063 (3 / 30)
train_acc: 0.2484548825710754, val_acc: 0.270935960591133, train_loss: 1.70410107534807, val_loss: 1.6566537571658055 (4 / 30)
train_acc: 0.29295426452410384, val_acc: 0.2561576354679803, train_loss: 1.673643476883472, val_loss: 1.6251786700610458 (5 / 30)
train_acc: 0.32014833127317677, val_acc: 0.3103448275862069, train_loss: 1.6071767093814053, val_loss: 1.650457472049544 (6 / 30)
train_acc: 0.33498145859085293, val_acc: 0.3497536945812808, train_loss: 1.5909448860602267, val_loss: 1.5568194324747096 (7 / 30)
train_acc: 0.3226205191594561, val_acc: 0.32019704433497537, train_loss: 1.5548590384838166, val_loss: 1.5683210095748525 (8 / 30)
train_acc: 0.37453646477132263, val_acc: 0.4088669950738916, train_loss: 1.5329438679297864, val_loss: 1.4962412805980063 (9 / 30)
train_acc: 0.3856613102595797, val_acc: 0.37438423645320196, train_loss: 1.4878415281899338, val_loss: 1.5073221346427654 (10 / 30)
train_acc: 0.39184177997527814, val_acc: 0.3891625615763547, train_loss: 1.4653523784632736, val_loss: 1.4480430058070592 (11 / 30)
train_acc: 0.40914709517923364, val_acc: 0.37438423645320196, train_loss: 1.428623098967544, val_loss: 1.4887189771154243 (12 / 30)
train_acc: 0.411619283065513, val_acc: 0.3842364532019704, train_loss: 1.372952952667868, val_loss: 1.3018701681362583 (13 / 30)
train_acc: 0.4326328800988875, val_acc: 0.42857142857142855, train_loss: 1.3219546417076007, val_loss: 1.3157347158845423 (14 / 30)
train_acc: 0.41903584672435107, val_acc: 0.4236453201970443, train_loss: 1.3265216705678273, val_loss: 1.350921577420728 (15 / 30)
train_acc: 0.4363411619283066, val_acc: 0.4630541871921182, train_loss: 1.2669063528034978, val_loss: 1.2487771898654882 (16 / 30)
train_acc: 0.45488257107540175, val_acc: 0.43842364532019706, train_loss: 1.2515076837964052, val_loss: 1.2753901240860888 (17 / 30)
train_acc: 0.4758961681087763, val_acc: 0.4236453201970443, train_loss: 1.2026081250241425, val_loss: 1.3523094243016736 (18 / 30)
train_acc: 0.519159456118665, val_acc: 0.4876847290640394, train_loss: 1.1218485785061114, val_loss: 1.1867768834964396 (19 / 30)
train_acc: 0.511742892459827, val_acc: 0.5221674876847291, train_loss: 1.0951700204677133, val_loss: 1.1655866882483947 (20 / 30)
train_acc: 0.5377008652657602, val_acc: 0.5172413793103449, train_loss: 1.0771527390544877, val_loss: 1.1732866311895436 (21 / 30)
train_acc: 0.5401730531520396, val_acc: 0.5123152709359606, train_loss: 1.0351224252113749, val_loss: 1.1661923325120522 (22 / 30)
train_acc: 0.5735475896168108, val_acc: 0.5566502463054187, train_loss: 1.0141490378102795, val_loss: 1.167614181934319 (23 / 30)
train_acc: 0.5587144622991347, val_acc: 0.5467980295566502, train_loss: 1.0160559134371348, val_loss: 1.1471483020359659 (24 / 30)
train_acc: 0.5859085290482077, val_acc: 0.5270935960591133, train_loss: 0.9986546048422532, val_loss: 1.1642629413181924 (25 / 30)
train_acc: 0.5982694684796045, val_acc: 0.5566502463054187, train_loss: 0.9680562031136455, val_loss: 1.1156161556690198 (26 / 30)
train_acc: 0.5871446229913473, val_acc: 0.5369458128078818, train_loss: 0.9599208065546929, val_loss: 1.1413320408666074 (27 / 30)
train_acc: 0.5970333745364648, val_acc: 0.5123152709359606, train_loss: 0.9474262103751504, val_loss: 1.1429417089288458 (28 / 30)
train_acc: 0.6056860321384425, val_acc: 0.5270935960591133, train_loss: 0.94718977046396, val_loss: 1.1414581560736219 (29 / 30)
train_acc: 0.6180469715698393, val_acc: 0.5763546798029556, train_loss: 0.8998845543053889, val_loss: 1.0834094691159102 (30 / 30)
lr 0.0008164608671537016, batch 8, decay 9.827933937733401e-06, gamma 0.1810540714504256, val accuracy 0.5763546798029556, val loss 1.0834094691159102 [7 / 20]
-------------------------------------
train_acc: 0.17058096415327564, val_acc: 0.23645320197044334, train_loss: 1.7878385966434172, val_loss: 1.7763948276125152 (1 / 30)
train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7624035006842595, val_loss: 1.7514542987193968 (2 / 30)
train_acc: 0.20395550061804696, val_acc: 0.2955665024630542, train_loss: 1.7295331990320986, val_loss: 1.6890185836500722 (3 / 30)
train_acc: 0.2521631644004944, val_acc: 0.270935960591133, train_loss: 1.724170629409394, val_loss: 1.6560957731284531 (4 / 30)
train_acc: 0.28059332509270707, val_acc: 0.2561576354679803, train_loss: 1.6632957051799382, val_loss: 1.7585689316829438 (5 / 30)
train_acc: 0.311495673671199, val_acc: 0.3399014778325123, train_loss: 1.5948053408317424, val_loss: 1.563238291904844 (6 / 30)
train_acc: 0.3189122373300371, val_acc: 0.3497536945812808, train_loss: 1.636860082853857, val_loss: 1.5465300100777537 (7 / 30)
train_acc: 0.33868974042027195, val_acc: 0.35467980295566504, train_loss: 1.5855715040518121, val_loss: 1.5802232027053833 (8 / 30)
train_acc: 0.34610630407911, val_acc: 0.37438423645320196, train_loss: 1.5782520035730747, val_loss: 1.5547336046331621 (9 / 30)
train_acc: 0.3535228677379481, val_acc: 0.3793103448275862, train_loss: 1.5217376732266288, val_loss: 1.497047532368176 (10 / 30)
train_acc: 0.37330037082818296, val_acc: 0.3842364532019704, train_loss: 1.5186046836402713, val_loss: 1.503393532020118 (11 / 30)
train_acc: 0.38813349814585907, val_acc: 0.39901477832512317, train_loss: 1.4724740186344707, val_loss: 1.4455905688807296 (12 / 30)
train_acc: 0.4042027194066749, val_acc: 0.3891625615763547, train_loss: 1.3974617414627735, val_loss: 1.5264483326174356 (13 / 30)
train_acc: 0.39184177997527814, val_acc: 0.35960591133004927, train_loss: 1.4368466384920702, val_loss: 1.5386551260360943 (14 / 30)
train_acc: 0.40667490729295425, val_acc: 0.4187192118226601, train_loss: 1.3795814508266, val_loss: 1.3404837553137041 (15 / 30)
train_acc: 0.41285537700865266, val_acc: 0.4236453201970443, train_loss: 1.3612770774897893, val_loss: 1.335005989802882 (16 / 30)
train_acc: 0.4363411619283066, val_acc: 0.4039408866995074, train_loss: 1.310281911945461, val_loss: 1.32651466219296 (17 / 30)
train_acc: 0.4326328800988875, val_acc: 0.4630541871921182, train_loss: 1.3303397205763017, val_loss: 1.2800979649492086 (18 / 30)
train_acc: 0.4635352286773795, val_acc: 0.458128078817734, train_loss: 1.208329501051838, val_loss: 1.253822952068498 (19 / 30)
train_acc: 0.48702101359703337, val_acc: 0.47783251231527096, train_loss: 1.1838183037577512, val_loss: 1.2287347310869565 (20 / 30)
train_acc: 0.5067985166872683, val_acc: 0.4630541871921182, train_loss: 1.159604726232616, val_loss: 1.248197814513897 (21 / 30)
train_acc: 0.4932014833127318, val_acc: 0.47783251231527096, train_loss: 1.1398252415273924, val_loss: 1.2368445954299325 (22 / 30)
train_acc: 0.5105067985166872, val_acc: 0.49261083743842365, train_loss: 1.1258776189370268, val_loss: 1.2153769778500636 (23 / 30)
train_acc: 0.5080346106304079, val_acc: 0.4876847290640394, train_loss: 1.1340415772460448, val_loss: 1.2030695089565708 (24 / 30)
train_acc: 0.5253399258343634, val_acc: 0.4876847290640394, train_loss: 1.1238473985339568, val_loss: 1.2073819443510083 (25 / 30)
train_acc: 0.5265760197775031, val_acc: 0.4729064039408867, train_loss: 1.1074205763702487, val_loss: 1.208118922017478 (26 / 30)
train_acc: 0.5377008652657602, val_acc: 0.4827586206896552, train_loss: 1.0988287371669623, val_loss: 1.2504599558308793 (27 / 30)
train_acc: 0.5302843016069221, val_acc: 0.5024630541871922, train_loss: 1.1124621589929418, val_loss: 1.231636628728782 (28 / 30)
train_acc: 0.5352286773794809, val_acc: 0.5024630541871922, train_loss: 1.065551184311196, val_loss: 1.2272583557467156 (29 / 30)
train_acc: 0.5327564894932015, val_acc: 0.5073891625615764, train_loss: 1.0723876932495457, val_loss: 1.1839757153553327 (30 / 30)
lr 0.0008696351651005292, batch 8, decay 3.1856882920801124e-07, gamma 0.10862119199595698, val accuracy 0.5073891625615764, val loss 1.1839757153553327 [8 / 20]
-------------------------------------
train_acc: 0.15945611866501855, val_acc: 0.18226600985221675, train_loss: 1.7839305686714917, val_loss: 1.7607248851231165 (1 / 30)
train_acc: 0.21631644004944375, val_acc: 0.18226600985221675, train_loss: 1.754799725098722, val_loss: 1.751951564121716 (2 / 30)
train_acc: 0.19777503090234858, val_acc: 0.20689655172413793, train_loss: 1.7487823689087065, val_loss: 1.6943215994999326 (3 / 30)
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
<ipython-input-4-8f6a1a2578d9> in <module>()
     42   net = vgg19()
     43   net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
---> 44   current_net, val_accuracy, val_loss = train_network(net, net.parameters(), LR, NUM_EPOCHS, BATCH_SIZE, WEIGHT_DECAY, STEP_SIZE, GAMMA, train_dataset, val_dataset=val_dataset, verbosity=True)
     45 
     46   val_accuracies.append(val_accuracy)

<ipython-input-2-c446edcbb070> in train_network(net, parameters_to_optimize, learning_rate, num_epochs, batch_size, weight_decay, step_size, gamma, train_dataset, val_dataset, verbosity, plot)
     82 
     83         for images, labels in train_dataloader:
---> 84             images = images.to(DEVICE)
     85             labels = labels.to(DEVICE)
     86             net.train()

KeyboardInterrupt: 