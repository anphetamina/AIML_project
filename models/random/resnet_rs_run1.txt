

training set 809
validation set 203
---------------------------------------------
lr 0.007143816333267795, batch 10, decay 7.97847132986919e-06, gamma 0.011397752872062666 [1 / 20]
train_acc: 0.2546353522867738, val_acc: 0.28078817733990147, train_loss: 2.8318154803017896, val_loss: 3.1886843375384513 (1 / 15)
train_acc: 0.32509270704573545, val_acc: 0.31527093596059114, train_loss: 1.9173051559438812, val_loss: 1.7124730982803946 (2 / 15)
train_acc: 0.3300370828182942, val_acc: 0.3793103448275862, train_loss: 1.7023786829487502, val_loss: 1.4275859418173729 (3 / 15)
train_acc: 0.3473423980222497, val_acc: 0.33004926108374383, train_loss: 1.6203729767439834, val_loss: 1.8795477116636454 (4 / 15)
train_acc: 0.3683559950556242, val_acc: 0.29064039408866993, train_loss: 1.4918087233129627, val_loss: 1.5947262283616466 (5 / 15)
train_acc: 0.3794808405438813, val_acc: 0.39901477832512317, train_loss: 1.4685382455476872, val_loss: 1.6990068808858618 (6 / 15)
train_acc: 0.41285537700865266, val_acc: 0.41379310344827586, train_loss: 1.4570874986601405, val_loss: 1.717403342424355 (7 / 15)
train_acc: 0.43510506798516685, val_acc: 0.4236453201970443, train_loss: 1.304637505922683, val_loss: 1.3400913121665052 (8 / 15)
train_acc: 0.4684796044499382, val_acc: 0.4236453201970443, train_loss: 1.3025612869427732, val_loss: 1.3015230412553684 (9 / 15)
train_acc: 0.453646477132262, val_acc: 0.4482758620689655, train_loss: 1.2884161513579644, val_loss: 1.3414706147363034 (10 / 15)
train_acc: 0.45488257107540175, val_acc: 0.43842364532019706, train_loss: 1.234747644334258, val_loss: 1.3016409230936925 (11 / 15)
train_acc: 0.453646477132262, val_acc: 0.42857142857142855, train_loss: 1.2585104003972265, val_loss: 1.2954033771759184 (12 / 15)
train_acc: 0.4573547589616811, val_acc: 0.43842364532019706, train_loss: 1.2225568446299644, val_loss: 1.2923717604482114 (13 / 15)
train_acc: 0.45488257107540175, val_acc: 0.43349753694581283, train_loss: 1.2399392842657044, val_loss: 1.3108692421701742 (14 / 15)
train_acc: 0.44499381953028433, val_acc: 0.43349753694581283, train_loss: 1.2657464530470934, val_loss: 1.2893914630260375 (15 / 15)

val accuracy 0.4482758620689655
val loss 1.3414706147363034
---------------------------------------------
lr 0.006271728035054039, batch 13, decay 5.028667061409382e-06, gamma 0.016614264516159452 [2 / 20]
train_acc: 0.24969097651421507, val_acc: 0.3103448275862069, train_loss: 3.2325158680619945, val_loss: 1.9774267233064022 (1 / 15)
train_acc: 0.2595797280593325, val_acc: 0.3399014778325123, train_loss: 1.8080612054124603, val_loss: 1.655421627859764 (2 / 15)
train_acc: 0.32756489493201485, val_acc: 0.3842364532019704, train_loss: 1.787009585625751, val_loss: 1.5109686516775873 (3 / 15)
train_acc: 0.34363411619283063, val_acc: 0.39408866995073893, train_loss: 1.5388924516323028, val_loss: 1.4826167909969836 (4 / 15)
train_acc: 0.38442521631644005, val_acc: 0.3448275862068966, train_loss: 1.4633318987411386, val_loss: 1.4682114249086144 (5 / 15)
train_acc: 0.36341161928306553, val_acc: 0.39901477832512317, train_loss: 1.4750780768241223, val_loss: 1.6050760054529594 (6 / 15)
train_acc: 0.3930778739184178, val_acc: 0.4088669950738916, train_loss: 1.5715998800929603, val_loss: 1.6395651414770211 (7 / 15)
train_acc: 0.40914709517923364, val_acc: 0.39901477832512317, train_loss: 1.3879084578992704, val_loss: 1.3497819122422505 (8 / 15)
train_acc: 0.4511742892459827, val_acc: 0.41379310344827586, train_loss: 1.2567425958158354, val_loss: 1.345319721792719 (9 / 15)
train_acc: 0.4388133498145859, val_acc: 0.3793103448275862, train_loss: 1.2661148058469276, val_loss: 1.3314830461159128 (10 / 15)
train_acc: 0.4561186650185414, val_acc: 0.39901477832512317, train_loss: 1.2884461883414071, val_loss: 1.3401329646556837 (11 / 15)
train_acc: 0.4338689740420272, val_acc: 0.39408866995073893, train_loss: 1.2637473237971293, val_loss: 1.3304798051054254 (12 / 15)
train_acc: 0.45241038318912236, val_acc: 0.39408866995073893, train_loss: 1.2453625250805735, val_loss: 1.340242964880807 (13 / 15)
train_acc: 0.46600741656365885, val_acc: 0.4039408866995074, train_loss: 1.2461549398778249, val_loss: 1.3301176687179528 (14 / 15)
train_acc: 0.4684796044499382, val_acc: 0.4039408866995074, train_loss: 1.2434890920063768, val_loss: 1.3248857409496027 (15 / 15)

val accuracy 0.41379310344827586
val loss 1.345319721792719
---------------------------------------------
lr 0.0029161799479118155, batch 6, decay 3.4310285126909607e-06, gamma 0.8642333733535363 [3 / 20]
train_acc: 0.21755253399258342, val_acc: 0.28078817733990147, train_loss: 2.8446740057913424, val_loss: 2.2448040857690894 (1 / 15)
train_acc: 0.29295426452410384, val_acc: 0.37438423645320196, train_loss: 2.0463126122730477, val_loss: 2.6429220289725976 (2 / 15)
train_acc: 0.30902348578491967, val_acc: 0.3497536945812808, train_loss: 1.761345378990079, val_loss: 1.568134371870257 (3 / 15)
train_acc: 0.3621755253399258, val_acc: 0.35960591133004927, train_loss: 1.6427048807238471, val_loss: 1.6345808228542065 (4 / 15)
train_acc: 0.3411619283065513, val_acc: 0.3645320197044335, train_loss: 1.563635950919577, val_loss: 1.6032283893336767 (5 / 15)
train_acc: 0.34363411619283063, val_acc: 0.3251231527093596, train_loss: 1.5987979055777173, val_loss: 1.6740490045453527 (6 / 15)
train_acc: 0.37082818294190356, val_acc: 0.3694581280788177, train_loss: 1.4867988493298128, val_loss: 1.6929130030181168 (7 / 15)
train_acc: 0.3720642768850433, val_acc: 0.3891625615763547, train_loss: 1.4891342148645257, val_loss: 1.406206064036327 (8 / 15)
train_acc: 0.3695920889987639, val_acc: 0.43842364532019706, train_loss: 1.4779808260454383, val_loss: 1.402059168137353 (9 / 15)
train_acc: 0.4079110012360939, val_acc: 0.4088669950738916, train_loss: 1.4053483234053048, val_loss: 1.3366819687664802 (10 / 15)
train_acc: 0.4388133498145859, val_acc: 0.3891625615763547, train_loss: 1.3176198662137808, val_loss: 1.4443342545129396 (11 / 15)
train_acc: 0.44252163164400493, val_acc: 0.3399014778325123, train_loss: 1.3459725052818232, val_loss: 1.61237313359829 (12 / 15)
train_acc: 0.44128553770086526, val_acc: 0.4482758620689655, train_loss: 1.3156114521221118, val_loss: 1.3080236924252486 (13 / 15)
train_acc: 0.45488257107540175, val_acc: 0.4827586206896552, train_loss: 1.2732379717202063, val_loss: 1.334148696637506 (14 / 15)
train_acc: 0.5006180469715699, val_acc: 0.45320197044334976, train_loss: 1.1609490426126015, val_loss: 1.6232119634233673 (15 / 15)

val accuracy 0.4827586206896552
val loss 1.334148696637506
---------------------------------------------
lr 0.0016262102148232815, batch 5, decay 3.397802813431981e-06, gamma 0.2752929819051954 [4 / 20]
train_acc: 0.25092707045735474, val_acc: 0.270935960591133, train_loss: 2.506141657808655, val_loss: 1.9084218887272726 (1 / 15)
train_acc: 0.27070457354758964, val_acc: 0.32019704433497537, train_loss: 2.0467546336141003, val_loss: 1.6243849910538772 (2 / 15)
train_acc: 0.30407911001236093, val_acc: 0.3399014778325123, train_loss: 1.908548507245421, val_loss: 1.9257445940243199 (3 / 15)
train_acc: 0.31025957972805934, val_acc: 0.35960591133004927, train_loss: 1.771213925444741, val_loss: 1.4518236324117688 (4 / 15)
train_acc: 0.3337453646477132, val_acc: 0.39901477832512317, train_loss: 1.7543231058032316, val_loss: 1.7166054464619735 (5 / 15)
train_acc: 0.35599505562422745, val_acc: 0.3793103448275862, train_loss: 1.642358398850091, val_loss: 1.6423666941121293 (6 / 15)
train_acc: 0.38813349814585907, val_acc: 0.4088669950738916, train_loss: 1.4208714509629201, val_loss: 1.338478318869774 (7 / 15)
train_acc: 0.44499381953028433, val_acc: 0.4433497536945813, train_loss: 1.327383906422793, val_loss: 1.3689977570707574 (8 / 15)
train_acc: 0.4252163164400494, val_acc: 0.37438423645320196, train_loss: 1.3772755333755455, val_loss: 2.0339414003520764 (9 / 15)
train_acc: 0.4363411619283066, val_acc: 0.4433497536945813, train_loss: 1.2697264250040938, val_loss: 1.4509903219679894 (10 / 15)
train_acc: 0.4573547589616811, val_acc: 0.35467980295566504, train_loss: 1.2985201815444842, val_loss: 1.5592983301930827 (11 / 15)
train_acc: 0.4573547589616811, val_acc: 0.43349753694581283, train_loss: 1.2801783230749726, val_loss: 1.3615145527670536 (12 / 15)
train_acc: 0.4956736711990111, val_acc: 0.4482758620689655, train_loss: 1.165919422513917, val_loss: 1.3163884763647182 (13 / 15)
train_acc: 0.5377008652657602, val_acc: 0.4433497536945813, train_loss: 1.1127021614052308, val_loss: 1.3064721020424894 (14 / 15)
train_acc: 0.511742892459827, val_acc: 0.4236453201970443, train_loss: 1.1655401239362724, val_loss: 1.3957326776288412 (15 / 15)

val accuracy 0.4482758620689655
val loss 1.3163884763647182
---------------------------------------------
lr 0.008261445723244346, batch 7, decay 2.579223294368155e-06, gamma 0.11223997509753997 [5 / 20]
train_acc: 0.28182941903584674, val_acc: 0.3103448275862069, train_loss: 3.0984683981342576, val_loss: 1.9252880873351261 (1 / 15)
train_acc: 0.26452410383189123, val_acc: 0.3103448275862069, train_loss: 2.008828196006888, val_loss: 1.6243135682467758 (2 / 15)
train_acc: 0.33127317676143386, val_acc: 0.29064039408866993, train_loss: 1.7051501016239596, val_loss: 3.8768797414056184 (3 / 15)
train_acc: 0.35599505562422745, val_acc: 0.3891625615763547, train_loss: 1.60402856275384, val_loss: 1.526283387480111 (4 / 15)
train_acc: 0.34610630407911, val_acc: 0.3793103448275862, train_loss: 1.5612943947536246, val_loss: 2.0978439648089737 (5 / 15)
train_acc: 0.3646477132262052, val_acc: 0.37438423645320196, train_loss: 1.4977228377009795, val_loss: 1.5996990645753926 (6 / 15)
train_acc: 0.411619283065513, val_acc: 0.3842364532019704, train_loss: 1.4060135109462903, val_loss: 1.3843392616715924 (7 / 15)
train_acc: 0.42027194066749074, val_acc: 0.4088669950738916, train_loss: 1.3084750029595733, val_loss: 1.3477353648892765 (8 / 15)
train_acc: 0.4103831891223733, val_acc: 0.3793103448275862, train_loss: 1.3131461227485364, val_loss: 1.4253848123139348 (9 / 15)
train_acc: 0.4400494437577256, val_acc: 0.39901477832512317, train_loss: 1.273196044973154, val_loss: 1.3577241322089886 (10 / 15)
train_acc: 0.44128553770086526, val_acc: 0.4088669950738916, train_loss: 1.2645502366005859, val_loss: 1.3699562652357693 (11 / 15)
train_acc: 0.4758961681087763, val_acc: 0.39901477832512317, train_loss: 1.2327344891314749, val_loss: 1.4181728183195508 (12 / 15)
train_acc: 0.4437577255871446, val_acc: 0.4039408866995074, train_loss: 1.2236231444645282, val_loss: 1.3593794477396999 (13 / 15)
train_acc: 0.4907292954264524, val_acc: 0.4236453201970443, train_loss: 1.1833704103645495, val_loss: 1.3493862624826103 (14 / 15)
train_acc: 0.48825710754017304, val_acc: 0.41379310344827586, train_loss: 1.171676071303147, val_loss: 1.3421822878821144 (15 / 15)

val accuracy 0.4236453201970443
val loss 1.3493862624826103
---------------------------------------------
lr 0.002615118706238351, batch 12, decay 1.5959268080934959e-07, gamma 0.066664377022231 [6 / 20]
train_acc: 0.2830655129789864, val_acc: 0.3103448275862069, train_loss: 2.055126201089734, val_loss: 2.223417716366904 (1 / 15)
train_acc: 0.3238566131025958, val_acc: 0.2955665024630542, train_loss: 1.9419339142270082, val_loss: 1.637261509308087 (2 / 15)
train_acc: 0.3337453646477132, val_acc: 0.3448275862068966, train_loss: 1.6321128089584143, val_loss: 1.5575512207200375 (3 / 15)
train_acc: 0.35970333745364647, val_acc: 0.35467980295566504, train_loss: 1.683694696249567, val_loss: 1.5353009753626556 (4 / 15)
train_acc: 0.3720642768850433, val_acc: 0.32019704433497537, train_loss: 1.569995502016895, val_loss: 1.6288414265721889 (5 / 15)
train_acc: 0.38813349814585907, val_acc: 0.3694581280788177, train_loss: 1.4961718617027857, val_loss: 1.4705784491130285 (6 / 15)
train_acc: 0.44870210135970334, val_acc: 0.46798029556650245, train_loss: 1.329682293869509, val_loss: 1.2750799321188715 (7 / 15)
train_acc: 0.4919653893695921, val_acc: 0.41379310344827586, train_loss: 1.1871439482284565, val_loss: 1.295596411075498 (8 / 15)
train_acc: 0.5080346106304079, val_acc: 0.43842364532019706, train_loss: 1.1174971827470475, val_loss: 1.2846451527966654 (9 / 15)
train_acc: 0.5092707045735476, val_acc: 0.43349753694581283, train_loss: 1.1250764629602137, val_loss: 1.280832285951511 (10 / 15)
train_acc: 0.5401730531520396, val_acc: 0.4482758620689655, train_loss: 1.1032525818191736, val_loss: 1.299446580445238 (11 / 15)
train_acc: 0.5438813349814586, val_acc: 0.458128078817734, train_loss: 1.0647312670909284, val_loss: 1.3079545421553362 (12 / 15)
train_acc: 0.5661310259579728, val_acc: 0.4729064039408867, train_loss: 1.0411279432853162, val_loss: 1.2986611057384847 (13 / 15)
train_acc: 0.5710754017305315, val_acc: 0.47783251231527096, train_loss: 1.037421010038614, val_loss: 1.2758896979205128 (14 / 15)
train_acc: 0.5500618046971569, val_acc: 0.49261083743842365, train_loss: 1.0355430072258665, val_loss: 1.262218946893814 (15 / 15)

val accuracy 0.49261083743842365
val loss 1.262218946893814
---------------------------------------------
lr 0.0012532712335450772, batch 15, decay 2.7797304747930537e-06, gamma 0.17298702655745737 [7 / 20]
train_acc: 0.25092707045735474, val_acc: 0.3103448275862069, train_loss: 1.9792051667483392, val_loss: 3.3820862448567826 (1 / 15)
train_acc: 0.3189122373300371, val_acc: 0.3399014778325123, train_loss: 1.7128573891848362, val_loss: 1.6275927392132763 (2 / 15)
train_acc: 0.2904820766378245, val_acc: 0.2512315270935961, train_loss: 1.785834220637497, val_loss: 2.1185624295561185 (3 / 15)
train_acc: 0.3337453646477132, val_acc: 0.32019704433497537, train_loss: 1.8216652450219661, val_loss: 1.9021401842826693 (4 / 15)
train_acc: 0.3288009888751545, val_acc: 0.30049261083743845, train_loss: 1.626353914569718, val_loss: 1.5775891289922404 (5 / 15)
train_acc: 0.380716934487021, val_acc: 0.32019704433497537, train_loss: 1.4891193284387199, val_loss: 1.5986673679257848 (6 / 15)
train_acc: 0.42027194066749074, val_acc: 0.3448275862068966, train_loss: 1.3701727938445742, val_loss: 1.402177119489961 (7 / 15)
train_acc: 0.4252163164400494, val_acc: 0.35467980295566504, train_loss: 1.3049757100448325, val_loss: 1.4249773548154407 (8 / 15)
train_acc: 0.4388133498145859, val_acc: 0.3694581280788177, train_loss: 1.289461947636787, val_loss: 1.39724348301958 (9 / 15)
train_acc: 0.47095179233621753, val_acc: 0.37438423645320196, train_loss: 1.257412269840429, val_loss: 1.4042294800575144 (10 / 15)
train_acc: 0.4758961681087763, val_acc: 0.35467980295566504, train_loss: 1.2785703072736525, val_loss: 1.3988378259348753 (11 / 15)
train_acc: 0.4561186650185414, val_acc: 0.37438423645320196, train_loss: 1.2445596569990198, val_loss: 1.4107298692458956 (12 / 15)
train_acc: 0.5216316440049443, val_acc: 0.3891625615763547, train_loss: 1.185232029046647, val_loss: 1.374021423274073 (13 / 15)
train_acc: 0.5278121137206427, val_acc: 0.37438423645320196, train_loss: 1.185356871454029, val_loss: 1.3791786984269843 (14 / 15)
train_acc: 0.5129789864029666, val_acc: 0.3694581280788177, train_loss: 1.1677145382529872, val_loss: 1.3723329831226705 (15 / 15)

val accuracy 0.3891625615763547
val loss 1.374021423274073
---------------------------------------------
lr 0.004959134720403403, batch 9, decay 1.7007017000532863e-08, gamma 0.023408593092240505 [8 / 20]
train_acc: 0.2657601977750309, val_acc: 0.27586206896551724, train_loss: 3.020256882692297, val_loss: 1.775287043872138 (1 / 15)
train_acc: 0.2954264524103832, val_acc: 0.2561576354679803, train_loss: 1.8277368002533176, val_loss: 1.6267751901607794 (2 / 15)
train_acc: 0.3374536464771323, val_acc: 0.3497536945812808, train_loss: 1.647358333740305, val_loss: 1.8474899951019899 (3 / 15)
train_acc: 0.3053152039555006, val_acc: 0.3497536945812808, train_loss: 1.6417062440230907, val_loss: 1.4334245156772032 (4 / 15)
train_acc: 0.37330037082818296, val_acc: 0.3793103448275862, train_loss: 1.6025206547585051, val_loss: 1.4767551448544844 (5 / 15)
train_acc: 0.3473423980222497, val_acc: 0.3793103448275862, train_loss: 1.5214413859788214, val_loss: 1.3895564340605524 (6 / 15)
train_acc: 0.4338689740420272, val_acc: 0.4187192118226601, train_loss: 1.3510754775942035, val_loss: 1.348820058803253 (7 / 15)
train_acc: 0.43510506798516685, val_acc: 0.4433497536945813, train_loss: 1.2977882995741035, val_loss: 1.3698402820843194 (8 / 15)
train_acc: 0.4511742892459827, val_acc: 0.43842364532019706, train_loss: 1.2646178765998342, val_loss: 1.336350054356265 (9 / 15)
train_acc: 0.4326328800988875, val_acc: 0.42857142857142855, train_loss: 1.2513390640982591, val_loss: 1.3273813011023798 (10 / 15)
train_acc: 0.4400494437577256, val_acc: 0.42857142857142855, train_loss: 1.2612200933422235, val_loss: 1.3855388306558425 (11 / 15)
train_acc: 0.4326328800988875, val_acc: 0.43842364532019706, train_loss: 1.2663776434690903, val_loss: 1.3608910420845295 (12 / 15)
train_acc: 0.446229913473424, val_acc: 0.4482758620689655, train_loss: 1.2464763516990747, val_loss: 1.3455937176649206 (13 / 15)
train_acc: 0.4857849196538937, val_acc: 0.43842364532019706, train_loss: 1.216265064971556, val_loss: 1.339454277909448 (14 / 15)
train_acc: 0.4635352286773795, val_acc: 0.458128078817734, train_loss: 1.2156193354368505, val_loss: 1.4053471045834678 (15 / 15)

val accuracy 0.458128078817734
val loss 1.4053471045834678
---------------------------------------------
lr 0.006755009534924762, batch 6, decay 2.156827486954534e-06, gamma 0.01394773907605446 [9 / 20]
train_acc: 0.20642768850432633, val_acc: 0.21674876847290642, train_loss: 3.2343709315593516, val_loss: 1.9646621590177413 (1 / 15)
train_acc: 0.2867737948084054, val_acc: 0.28078817733990147, train_loss: 1.8001639698580258, val_loss: 1.7988857653340682 (2 / 15)
train_acc: 0.3053152039555006, val_acc: 0.3399014778325123, train_loss: 1.6741077587542634, val_loss: 1.5445585803225124 (3 / 15)
train_acc: 0.32509270704573545, val_acc: 0.29064039408866993, train_loss: 1.6344644065545721, val_loss: 2.165250555635086 (4 / 15)
train_acc: 0.3535228677379481, val_acc: 0.31527093596059114, train_loss: 1.5241162614857753, val_loss: 2.7223336132876392 (5 / 15)
train_acc: 0.35599505562422745, val_acc: 0.33497536945812806, train_loss: 1.5312455248626407, val_loss: 1.7842825363605774 (6 / 15)
train_acc: 0.3374536464771323, val_acc: 0.35467980295566504, train_loss: 1.4371467397003739, val_loss: 1.625526119326356 (7 / 15)
train_acc: 0.37082818294190356, val_acc: 0.37438423645320196, train_loss: 1.3866569387308598, val_loss: 1.3979255671938653 (8 / 15)
train_acc: 0.3621755253399258, val_acc: 0.3842364532019704, train_loss: 1.398427887811649, val_loss: 1.3510370929840163 (9 / 15)
train_acc: 0.38813349814585907, val_acc: 0.3694581280788177, train_loss: 1.3584813473398518, val_loss: 1.3662319564687206 (10 / 15)
train_acc: 0.3646477132262052, val_acc: 0.39408866995073893, train_loss: 1.3986573559686781, val_loss: 1.3290090505125487 (11 / 15)
train_acc: 0.40173053152039556, val_acc: 0.41379310344827586, train_loss: 1.3524677482317347, val_loss: 1.3271375392458122 (12 / 15)
train_acc: 0.3794808405438813, val_acc: 0.41379310344827586, train_loss: 1.344184421933036, val_loss: 1.3244682321407524 (13 / 15)
train_acc: 0.38936959208899874, val_acc: 0.4088669950738916, train_loss: 1.3403660123810928, val_loss: 1.38096894419252 (14 / 15)
train_acc: 0.39555006180469715, val_acc: 0.37438423645320196, train_loss: 1.3306266823864101, val_loss: 1.4152162896992244 (15 / 15)

val accuracy 0.41379310344827586
val loss 1.3271375392458122
---------------------------------------------
lr 0.0017096187024898822, batch 5, decay 6.613975594785559e-08, gamma 0.02206726159273092 [10 / 20]
train_acc: 0.22126081582200247, val_acc: 0.2019704433497537, train_loss: 2.722738390652594, val_loss: 5.725194618032484 (1 / 15)
train_acc: 0.2830655129789864, val_acc: 0.3251231527093596, train_loss: 1.9895111144104758, val_loss: 1.7076688978472367 (2 / 15)
train_acc: 0.2978986402966625, val_acc: 0.33004926108374383, train_loss: 1.823728432098337, val_loss: 1.9815630327010978 (3 / 15)
train_acc: 0.30902348578491967, val_acc: 0.3448275862068966, train_loss: 1.7281974575428203, val_loss: 1.9262496337955222 (4 / 15)
train_acc: 0.36341161928306553, val_acc: 0.3399014778325123, train_loss: 1.6353878602846001, val_loss: 1.6180519642207423 (5 / 15)
train_acc: 0.32756489493201485, val_acc: 0.30049261083743845, train_loss: 1.6551976925952472, val_loss: 2.772278022296323 (6 / 15)
train_acc: 0.3819530284301607, val_acc: 0.3842364532019704, train_loss: 1.9781503899135755, val_loss: 1.6022793542722176 (7 / 15)
train_acc: 0.38813349814585907, val_acc: 0.4039408866995074, train_loss: 1.4337131384718698, val_loss: 1.4106241322209683 (8 / 15)
train_acc: 0.4103831891223733, val_acc: 0.3891625615763547, train_loss: 1.3506557001319008, val_loss: 1.4033779118008214 (9 / 15)
train_acc: 0.45488257107540175, val_acc: 0.3694581280788177, train_loss: 1.286420578125528, val_loss: 1.4089033707902936 (10 / 15)
train_acc: 0.4276885043263288, val_acc: 0.35960591133004927, train_loss: 1.3256621710744276, val_loss: 1.369578494300396 (11 / 15)
train_acc: 0.4227441285537701, val_acc: 0.3497536945812808, train_loss: 1.343291054697355, val_loss: 1.3938621739154966 (12 / 15)
train_acc: 0.4326328800988875, val_acc: 0.3694581280788177, train_loss: 1.3059543782317888, val_loss: 1.371141544850589 (13 / 15)
train_acc: 0.446229913473424, val_acc: 0.3645320197044335, train_loss: 1.2622860003313412, val_loss: 1.3561128420489175 (14 / 15)
train_acc: 0.4388133498145859, val_acc: 0.39901477832512317, train_loss: 1.3068562003384414, val_loss: 1.369123844604187 (15 / 15)

val accuracy 0.4039408866995074
val loss 1.4106241322209683
---------------------------------------------
lr 0.002056563063422829, batch 13, decay 1.3763840950971783e-09, gamma 0.13148950347751343 [11 / 20]
train_acc: 0.28182941903584674, val_acc: 0.2512315270935961, train_loss: 2.0541237603011915, val_loss: 2.9083482352970855 (1 / 15)
train_acc: 0.3065512978986403, val_acc: 0.33497536945812806, train_loss: 2.133756543561466, val_loss: 1.6498237495939132 (2 / 15)
train_acc: 0.30902348578491967, val_acc: 0.3694581280788177, train_loss: 1.7203338523435652, val_loss: 1.8067108539524923 (3 / 15)
train_acc: 0.34857849196538937, val_acc: 0.32019704433497537, train_loss: 1.7871615038666648, val_loss: 1.553668826084419 (4 / 15)
train_acc: 0.33498145859085293, val_acc: 0.37438423645320196, train_loss: 1.5347461607606507, val_loss: 1.5946262608020765 (5 / 15)
train_acc: 0.3794808405438813, val_acc: 0.35960591133004927, train_loss: 1.513080778758517, val_loss: 1.3845897249400323 (6 / 15)
train_acc: 0.44746600741656367, val_acc: 0.42857142857142855, train_loss: 1.27796240259602, val_loss: 1.3378818419766543 (7 / 15)
train_acc: 0.4746600741656366, val_acc: 0.4236453201970443, train_loss: 1.236201535933392, val_loss: 1.3683889619822573 (8 / 15)
train_acc: 0.48825710754017304, val_acc: 0.4088669950738916, train_loss: 1.201846816410093, val_loss: 1.3044958534499107 (9 / 15)
train_acc: 0.4796044499381953, val_acc: 0.4827586206896552, train_loss: 1.1920632246250864, val_loss: 1.2856503193601598 (10 / 15)
train_acc: 0.4857849196538937, val_acc: 0.458128078817734, train_loss: 1.1836855880114883, val_loss: 1.324307065585564 (11 / 15)
train_acc: 0.5055624227441285, val_acc: 0.43349753694581283, train_loss: 1.153352328077677, val_loss: 1.3311305292721451 (12 / 15)
train_acc: 0.5327564894932015, val_acc: 0.5024630541871922, train_loss: 1.0851437305785228, val_loss: 1.262959517281631 (13 / 15)
train_acc: 0.5562422744128553, val_acc: 0.43842364532019706, train_loss: 1.099950419073494, val_loss: 1.300989607284809 (14 / 15)
train_acc: 0.5500618046971569, val_acc: 0.4975369458128079, train_loss: 1.0785226609709826, val_loss: 1.2506361106052775 (15 / 15)

val accuracy 0.5024630541871922
val loss 1.262959517281631
---------------------------------------------
lr 0.0011654446205974689, batch 15, decay 4.7492614161655476e-07, gamma 0.10983039821965576 [12 / 20]
train_acc: 0.27441285537700866, val_acc: 0.1921182266009852, train_loss: 1.8119353343294047, val_loss: 5.873081418616798 (1 / 15)
train_acc: 0.3164400494437577, val_acc: 0.3103448275862069, train_loss: 1.8166135562807137, val_loss: 1.9720367033199724 (2 / 15)
train_acc: 0.32756489493201485, val_acc: 0.30049261083743845, train_loss: 1.6665539707624721, val_loss: 1.7295758838724034 (3 / 15)
train_acc: 0.32756489493201485, val_acc: 0.3399014778325123, train_loss: 1.5960968962705917, val_loss: 2.15363916916213 (4 / 15)
train_acc: 0.33250927070457353, val_acc: 0.32019704433497537, train_loss: 1.7648539641878955, val_loss: 2.112702878237945 (5 / 15)
train_acc: 0.3399258343634116, val_acc: 0.24630541871921183, train_loss: 1.6168722468341974, val_loss: 2.7166554831665723 (6 / 15)
train_acc: 0.377008652657602, val_acc: 0.35467980295566504, train_loss: 1.5353788620019873, val_loss: 1.4675577547162624 (7 / 15)
train_acc: 0.4561186650185414, val_acc: 0.35960591133004927, train_loss: 1.3141786912168354, val_loss: 1.4229061301118635 (8 / 15)
train_acc: 0.45859085290482077, val_acc: 0.3645320197044335, train_loss: 1.2931986501986075, val_loss: 1.408148516281485 (9 / 15)
train_acc: 0.4610630407911001, val_acc: 0.39901477832512317, train_loss: 1.2925368055277613, val_loss: 1.3889135327832451 (10 / 15)
train_acc: 0.47713226205191595, val_acc: 0.3497536945812808, train_loss: 1.2636481951576817, val_loss: 1.42168847710041 (11 / 15)
train_acc: 0.47342398022249693, val_acc: 0.3497536945812808, train_loss: 1.2690972891372567, val_loss: 1.4143285381382908 (12 / 15)
train_acc: 0.48702101359703337, val_acc: 0.3645320197044335, train_loss: 1.2512009901228294, val_loss: 1.3825617059698245 (13 / 15)
train_acc: 0.5006180469715699, val_acc: 0.35467980295566504, train_loss: 1.2184303414689006, val_loss: 1.393235386592414 (14 / 15)
train_acc: 0.4796044499381953, val_acc: 0.3645320197044335, train_loss: 1.2770577687562912, val_loss: 1.4004413805571683 (15 / 15)

val accuracy 0.39901477832512317
val loss 1.3889135327832451
---------------------------------------------
lr 0.0013083716604176826, batch 4, decay 2.870506005368127e-06, gamma 0.07624309368517992 [13 / 20]
train_acc: 0.24598269468479605, val_acc: 0.27586206896551724, train_loss: 2.834733522129884, val_loss: 1.7363153144056573 (1 / 15)
train_acc: 0.26452410383189123, val_acc: 0.2857142857142857, train_loss: 2.0387831600693747, val_loss: 2.6583934587798095 (2 / 15)
train_acc: 0.2892459826946848, val_acc: 0.29064039408866993, train_loss: 1.8488495310098485, val_loss: 2.6617530167396435 (3 / 15)
train_acc: 0.27564894932014833, val_acc: 0.3497536945812808, train_loss: 1.8292255743472332, val_loss: 1.4457932883882758 (4 / 15)
train_acc: 0.31396786155747836, val_acc: 0.27586206896551724, train_loss: 1.7235516935992152, val_loss: 1.622454300889828 (5 / 15)
train_acc: 0.30407911001236093, val_acc: 0.30049261083743845, train_loss: 1.6609091643792, val_loss: 2.0259366581592655 (6 / 15)
train_acc: 0.39184177997527814, val_acc: 0.35960591133004927, train_loss: 1.4243945735788759, val_loss: 1.4914206002146153 (7 / 15)
train_acc: 0.3856613102595797, val_acc: 0.37438423645320196, train_loss: 1.4052558219329683, val_loss: 1.627394155328497 (8 / 15)
train_acc: 0.4177997527812114, val_acc: 0.33497536945812806, train_loss: 1.3766467456322518, val_loss: 1.511197253988294 (9 / 15)
train_acc: 0.4054388133498146, val_acc: 0.3448275862068966, train_loss: 1.3752043179440114, val_loss: 1.5077496291381385 (10 / 15)
train_acc: 0.4400494437577256, val_acc: 0.3448275862068966, train_loss: 1.337044778359393, val_loss: 1.4612623652801138 (11 / 15)
train_acc: 0.42398022249690975, val_acc: 0.35960591133004927, train_loss: 1.3233425239992083, val_loss: 1.657988239685303 (12 / 15)
train_acc: 0.45241038318912236, val_acc: 0.35467980295566504, train_loss: 1.276056414039527, val_loss: 1.5560299439970495 (13 / 15)
train_acc: 0.4289245982694685, val_acc: 0.3891625615763547, train_loss: 1.315787613612907, val_loss: 1.386180559696235 (14 / 15)
train_acc: 0.4264524103831891, val_acc: 0.35467980295566504, train_loss: 1.309192642735315, val_loss: 1.4476652254024749 (15 / 15)

val accuracy 0.3891625615763547
val loss 1.386180559696235
---------------------------------------------
lr 0.0014455401098923473, batch 15, decay 7.527600868050728e-07, gamma 0.2418048334564675 [14 / 20]
train_acc: 0.2867737948084054, val_acc: 0.29064039408866993, train_loss: 1.823700905435607, val_loss: 3.391610197245781 (1 / 15)
train_acc: 0.2843016069221261, val_acc: 0.3399014778325123, train_loss: 1.8720434786215405, val_loss: 1.5891328583209974 (2 / 15)
train_acc: 0.30284301606922126, val_acc: 0.29064039408866993, train_loss: 1.7558585484920828, val_loss: 1.6212900871126523 (3 / 15)
train_acc: 0.36341161928306553, val_acc: 0.29064039408866993, train_loss: 1.525827557401103, val_loss: 1.837200267267932 (4 / 15)
train_acc: 0.30284301606922126, val_acc: 0.2561576354679803, train_loss: 1.8176946863992547, val_loss: 1.6073596956107417 (5 / 15)
train_acc: 0.3584672435105068, val_acc: 0.3251231527093596, train_loss: 1.518662016530267, val_loss: 2.218766971615148 (6 / 15)
train_acc: 0.4004944375772559, val_acc: 0.35960591133004927, train_loss: 1.4905934660926885, val_loss: 1.490380307136498 (7 / 15)
train_acc: 0.4561186650185414, val_acc: 0.3793103448275862, train_loss: 1.3177746951359017, val_loss: 1.4597173063038604 (8 / 15)
train_acc: 0.47342398022249693, val_acc: 0.3793103448275862, train_loss: 1.295513767026116, val_loss: 1.5022045823153605 (9 / 15)
train_acc: 0.47095179233621753, val_acc: 0.3497536945812808, train_loss: 1.2733816133883444, val_loss: 1.432247192401604 (10 / 15)
train_acc: 0.47713226205191595, val_acc: 0.3891625615763547, train_loss: 1.261888762708058, val_loss: 1.400180108441508 (11 / 15)
train_acc: 0.522867737948084, val_acc: 0.35467980295566504, train_loss: 1.2038137355752574, val_loss: 1.488329373556992 (12 / 15)
train_acc: 0.5475896168108776, val_acc: 0.3694581280788177, train_loss: 1.1380627813975803, val_loss: 1.4166807582225707 (13 / 15)
train_acc: 0.5661310259579728, val_acc: 0.3891625615763547, train_loss: 1.1440978960142147, val_loss: 1.3947888363171093 (14 / 15)
train_acc: 0.5710754017305315, val_acc: 0.39408866995073893, train_loss: 1.1310742647008343, val_loss: 1.379054736914893 (15 / 15)

val accuracy 0.39408866995073893
val loss 1.379054736914893
---------------------------------------------
lr 0.0023573351897906033, batch 10, decay 6.279438905602635e-08, gamma 0.2379893246791383 [15 / 20]
train_acc: 0.26328800988875156, val_acc: 0.16748768472906403, train_loss: 2.590047583886513, val_loss: 2.1467337884339206 (1 / 15)
train_acc: 0.3189122373300371, val_acc: 0.32019704433497537, train_loss: 1.793588393845576, val_loss: 1.8653357952392746 (2 / 15)
train_acc: 0.33250927070457353, val_acc: 0.3399014778325123, train_loss: 1.6679633936864338, val_loss: 1.9030237081896495 (3 / 15)
train_acc: 0.380716934487021, val_acc: 0.35467980295566504, train_loss: 1.6582400667652653, val_loss: 1.542411832386637 (4 / 15)
train_acc: 0.34857849196538937, val_acc: 0.3399014778325123, train_loss: 1.6345477507936645, val_loss: 1.8260182406514736 (5 / 15)
train_acc: 0.3572311495673671, val_acc: 0.30049261083743845, train_loss: 1.5912369383575005, val_loss: 1.6763584596182912 (6 / 15)
train_acc: 0.43016069221260816, val_acc: 0.41379310344827586, train_loss: 1.3578496039724173, val_loss: 1.3824992439723367 (7 / 15)
train_acc: 0.4907292954264524, val_acc: 0.43349753694581283, train_loss: 1.2404619363978118, val_loss: 1.307140101646555 (8 / 15)
train_acc: 0.47713226205191595, val_acc: 0.4236453201970443, train_loss: 1.2223258124588448, val_loss: 1.2987164244275962 (9 / 15)
train_acc: 0.4932014833127318, val_acc: 0.4827586206896552, train_loss: 1.1435837692619106, val_loss: 1.3261853036622109 (10 / 15)
train_acc: 0.5315203955500618, val_acc: 0.47783251231527096, train_loss: 1.1108040189271509, val_loss: 1.3080625481206207 (11 / 15)
train_acc: 0.5550061804697157, val_acc: 0.43349753694581283, train_loss: 1.052088177543046, val_loss: 1.3349793397734318 (12 / 15)
train_acc: 0.6032138442521632, val_acc: 0.49261083743842365, train_loss: 0.9734105997533528, val_loss: 1.3039769304209743 (13 / 15)
train_acc: 0.5723114956736712, val_acc: 0.4827586206896552, train_loss: 0.9692119182408666, val_loss: 1.3148009468769204 (14 / 15)
train_acc: 0.5933250927070457, val_acc: 0.4482758620689655, train_loss: 0.9527482176003851, val_loss: 1.2758662251416097 (15 / 15)

val accuracy 0.49261083743842365
val loss 1.3039769304209743
---------------------------------------------
lr 0.005655767110106044, batch 14, decay 1.940273911130679e-07, gamma 0.19443228405521779 [16 / 20]
train_acc: 0.2620519159456119, val_acc: 0.3103448275862069, train_loss: 2.385532321977085, val_loss: 2.160266000649025 (1 / 15)
train_acc: 0.25339925834363414, val_acc: 0.1921182266009852, train_loss: 2.2372011690410902, val_loss: 1.9147206627089401 (2 / 15)
train_acc: 0.3065512978986403, val_acc: 0.35960591133004927, train_loss: 1.8082771650498228, val_loss: 1.5278841627055202 (3 / 15)
train_acc: 0.3547589616810878, val_acc: 0.3842364532019704, train_loss: 1.6687815827403876, val_loss: 2.2556046864082075 (4 / 15)
train_acc: 0.3547589616810878, val_acc: 0.3054187192118227, train_loss: 1.5356475823594555, val_loss: 1.5426828039103542 (5 / 15)
train_acc: 0.3757725587144623, val_acc: 0.29064039408866993, train_loss: 1.4470558957794246, val_loss: 1.5556529205420921 (6 / 15)
train_acc: 0.41285537700865266, val_acc: 0.41379310344827586, train_loss: 1.3044220705704281, val_loss: 1.3261293152282978 (7 / 15)
train_acc: 0.48702101359703337, val_acc: 0.41379310344827586, train_loss: 1.185472262509821, val_loss: 1.3861951581363021 (8 / 15)
train_acc: 0.47342398022249693, val_acc: 0.4039408866995074, train_loss: 1.157345484744191, val_loss: 1.4670107899041012 (9 / 15)
train_acc: 0.48702101359703337, val_acc: 0.42857142857142855, train_loss: 1.151504276193706, val_loss: 1.3334509750892376 (10 / 15)
train_acc: 0.5475896168108776, val_acc: 0.4236453201970443, train_loss: 1.0563675099603915, val_loss: 1.4140734569779758 (11 / 15)
train_acc: 0.5203955500618047, val_acc: 0.39901477832512317, train_loss: 1.0692770761081993, val_loss: 1.5232126322285882 (12 / 15)
train_acc: 0.5747836835599506, val_acc: 0.43842364532019706, train_loss: 0.985441172373457, val_loss: 1.35049097702421 (13 / 15)
train_acc: 0.619283065512979, val_acc: 0.458128078817734, train_loss: 0.9089887421417, val_loss: 1.3490784579309925 (14 / 15)
train_acc: 0.6081582200247219, val_acc: 0.41379310344827586, train_loss: 0.8971913594692688, val_loss: 1.4336642232434502 (15 / 15)

val accuracy 0.458128078817734
val loss 1.3490784579309925
---------------------------------------------
lr 0.0055822298081082, batch 4, decay 6.082666781908267e-09, gamma 0.7547931701705616 [17 / 20]
train_acc: 0.22744128553770088, val_acc: 0.3054187192118227, train_loss: 3.0042501322565918, val_loss: 2.0941869742764627 (1 / 15)
train_acc: 0.2521631644004944, val_acc: 0.28078817733990147, train_loss: 1.9119463658008646, val_loss: 4.31940970632243 (2 / 15)
train_acc: 0.2978986402966625, val_acc: 0.3399014778325123, train_loss: 1.7333617664238137, val_loss: 2.5758502213238494 (3 / 15)
train_acc: 0.32014833127317677, val_acc: 0.35960591133004927, train_loss: 1.6309454497065032, val_loss: 1.598985346667285 (4 / 15)
train_acc: 0.3646477132262052, val_acc: 0.3448275862068966, train_loss: 1.5566866044208352, val_loss: 1.5561865251052556 (5 / 15)
train_acc: 0.32138442521631644, val_acc: 0.3448275862068966, train_loss: 1.6174645126234322, val_loss: 1.4082699262449894 (6 / 15)
train_acc: 0.32509270704573545, val_acc: 0.39901477832512317, train_loss: 1.5416247855010816, val_loss: 1.4413005424837761 (7 / 15)
train_acc: 0.3473423980222497, val_acc: 0.30049261083743845, train_loss: 1.5549213310107313, val_loss: 2.127229992392028 (8 / 15)
train_acc: 0.34487021013597036, val_acc: 0.2315270935960591, train_loss: 1.505679485382344, val_loss: 5.322516541762893 (9 / 15)
train_acc: 0.38936959208899874, val_acc: 0.3694581280788177, train_loss: 1.4185384519315325, val_loss: 1.5872996701982809 (10 / 15)
train_acc: 0.38442521631644005, val_acc: 0.3842364532019704, train_loss: 1.4581192155703626, val_loss: 1.7012432672707318 (11 / 15)
train_acc: 0.3757725587144623, val_acc: 0.3399014778325123, train_loss: 1.4681598412239065, val_loss: 2.9042362802721597 (12 / 15)
train_acc: 0.3856613102595797, val_acc: 0.4039408866995074, train_loss: 1.3963872116192604, val_loss: 1.3286557514679256 (13 / 15)
train_acc: 0.4227441285537701, val_acc: 0.39901477832512317, train_loss: 1.3368459459583013, val_loss: 1.3722247772028882 (14 / 15)
train_acc: 0.40667490729295425, val_acc: 0.3793103448275862, train_loss: 1.3974882937479667, val_loss: 1.5570589949932006 (15 / 15)

val accuracy 0.4039408866995074
val loss 1.3286557514679256
---------------------------------------------
lr 0.00790124696312163, batch 7, decay 5.742904016347278e-09, gamma 0.03058471191726536 [18 / 20]
train_acc: 0.22991347342398022, val_acc: 0.26108374384236455, train_loss: 3.3608853388186293, val_loss: 1.7458235991412197 (1 / 15)
train_acc: 0.2843016069221261, val_acc: 0.3054187192118227, train_loss: 1.9253414338097732, val_loss: 2.6308047689240555 (2 / 15)
train_acc: 0.3547589616810878, val_acc: 0.31527093596059114, train_loss: 1.6106624218677856, val_loss: 1.5670380982859382 (3 / 15)
train_acc: 0.36093943139678614, val_acc: 0.39901477832512317, train_loss: 1.5090898044766543, val_loss: 1.477259610747469 (4 / 15)
train_acc: 0.3572311495673671, val_acc: 0.3645320197044335, train_loss: 1.4856066757580553, val_loss: 1.603595758306569 (5 / 15)
train_acc: 0.40667490729295425, val_acc: 0.3891625615763547, train_loss: 1.455250225565195, val_loss: 1.5694237937187325 (6 / 15)
train_acc: 0.4264524103831891, val_acc: 0.41379310344827586, train_loss: 1.2938594619187496, val_loss: 1.3475318468850235 (7 / 15)
train_acc: 0.45982694684796044, val_acc: 0.4630541871921182, train_loss: 1.2836465303623779, val_loss: 1.3704108184781567 (8 / 15)
train_acc: 0.453646477132262, val_acc: 0.43842364532019706, train_loss: 1.2845908839564093, val_loss: 1.4836114932750832 (9 / 15)
train_acc: 0.4796044499381953, val_acc: 0.4433497536945813, train_loss: 1.2263281863316322, val_loss: 1.288476352033944 (10 / 15)
train_acc: 0.47342398022249693, val_acc: 0.42857142857142855, train_loss: 1.2180735550645845, val_loss: 1.422151040414284 (11 / 15)

---------------------------------------------------------------------------

KeyboardInterrupt                         Traceback (most recent call last)

<ipython-input-3-536bba139a1a> in <module>()
     36 
     37   net = resnet152(num_classes=NUM_CLASSES)
---> 38   current_net, val_accuracy, val_loss = train_network(net, net.parameters(), LR, NUM_EPOCHS, BATCH_SIZE, WEIGHT_DECAY, STEP_SIZE, GAMMA, train_dataset, val_dataset=val_dataset, verbosity=True, plot=True)
     39   val_accuracies.append(val_accuracy)
     40   val_losses.append(val_loss)

9 frames

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __getattr__(self, name)
    560             self._load_state_dict_pre_hooks = OrderedDict()
    561 
--> 562     def __getattr__(self, name):
    563         if '_parameters' in self.__dict__:
    564             _parameters = self.__dict__['_parameters']

KeyboardInterrupt: 

