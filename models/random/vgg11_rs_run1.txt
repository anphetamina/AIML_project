training set 809
validation set 203
-------------------------------------
train_acc: 0.1841779975278121, val_acc: 0.2315270935960591, train_loss: 1.7710324662872239, val_loss: 1.7467262656817883 (1 / 35)
train_acc: 0.2360939431396786, val_acc: 0.2019704433497537, train_loss: 1.7491115103251265, val_loss: 1.7438402651566003 (2 / 35)
train_acc: 0.28059332509270707, val_acc: 0.2413793103448276, train_loss: 1.6514052005279787, val_loss: 1.6914695947628302 (3 / 35)
train_acc: 0.22991347342398022, val_acc: 0.1921182266009852, train_loss: 1.7497405879135037, val_loss: 1.783629024557292 (4 / 35)
train_acc: 0.2088998763906057, val_acc: 0.18226600985221675, train_loss: 1.7697573770551362, val_loss: 1.7520633794991254 (5 / 35)
train_acc: 0.276885043263288, val_acc: 0.30049261083743845, train_loss: 1.6978493671040011, val_loss: 1.5821349879203759 (6 / 35)
train_acc: 0.29295426452410384, val_acc: 0.2857142857142857, train_loss: 1.639930427590171, val_loss: 1.675443747947956 (7 / 35)
train_acc: 0.3411619283065513, val_acc: 0.29064039408866993, train_loss: 1.5796817651637847, val_loss: 1.538491714764111 (8 / 35)
train_acc: 0.3411619283065513, val_acc: 0.3793103448275862, train_loss: 1.4894252515988533, val_loss: 1.353819059327318 (9 / 35)
train_acc: 0.3695920889987639, val_acc: 0.4039408866995074, train_loss: 1.4729417226517894, val_loss: 1.3341451965529343 (10 / 35)
train_acc: 0.3621755253399258, val_acc: 0.42857142857142855, train_loss: 1.4148757353111898, val_loss: 1.3007458034407329 (11 / 35)
train_acc: 0.36341161928306553, val_acc: 0.3842364532019704, train_loss: 1.4275510327335637, val_loss: 1.342622566105697 (12 / 35)
train_acc: 0.3720642768850433, val_acc: 0.4729064039408867, train_loss: 1.4431874467651689, val_loss: 1.303438945944086 (13 / 35)
train_acc: 0.3930778739184178, val_acc: 0.45320197044334976, train_loss: 1.3645190276085815, val_loss: 1.2934145445894139 (14 / 35)
train_acc: 0.39060568603213847, val_acc: 0.4433497536945813, train_loss: 1.3782285674393397, val_loss: 1.2535211429220114 (15 / 35)
train_acc: 0.4004944375772559, val_acc: 0.4088669950738916, train_loss: 1.3082860633086215, val_loss: 1.417262525981283 (16 / 35)
train_acc: 0.4338689740420272, val_acc: 0.3793103448275862, train_loss: 1.3100111936904002, val_loss: 1.3874700709516778 (17 / 35)
train_acc: 0.453646477132262, val_acc: 0.4433497536945813, train_loss: 1.2510992443311053, val_loss: 1.2312383228922126 (18 / 35)
train_acc: 0.4956736711990111, val_acc: 0.47783251231527096, train_loss: 1.2142145204897716, val_loss: 1.2065272836262368 (19 / 35)
train_acc: 0.5377008652657602, val_acc: 0.4630541871921182, train_loss: 1.0956836364177898, val_loss: 1.3047876131945644 (20 / 35)
train_acc: 0.5574783683559951, val_acc: 0.4630541871921182, train_loss: 1.0951354323859857, val_loss: 1.2384144725470707 (21 / 35)
train_acc: 0.6402966625463535, val_acc: 0.5566502463054187, train_loss: 0.9108578375597083, val_loss: 1.1795575680403874 (22 / 35)
train_acc: 0.6674907292954264, val_acc: 0.5911330049261084, train_loss: 0.8434769672206954, val_loss: 1.1246547951486898 (23 / 35)
train_acc: 0.6724351050679852, val_acc: 0.5862068965517241, train_loss: 0.790926190072438, val_loss: 1.1062244714187284 (24 / 35)
train_acc: 0.7119901112484549, val_acc: 0.5960591133004927, train_loss: 0.7454302578390897, val_loss: 1.1751771566315825 (25 / 35)
train_acc: 0.7045735475896168, val_acc: 0.5911330049261084, train_loss: 0.709259114763498, val_loss: 1.1652706608983683 (26 / 35)
train_acc: 0.7515451174289246, val_acc: 0.5812807881773399, train_loss: 0.6368649597147339, val_loss: 1.2072312030298957 (27 / 35)
train_acc: 0.7416563658838071, val_acc: 0.5566502463054187, train_loss: 0.6191763486091522, val_loss: 1.1284732105109492 (28 / 35)
train_acc: 0.7849196538936959, val_acc: 0.5714285714285714, train_loss: 0.5890785379079717, val_loss: 1.1515678384621155 (29 / 35)
train_acc: 0.7737948084054388, val_acc: 0.6206896551724138, train_loss: 0.5569975733093927, val_loss: 1.199758934563604 (30 / 35)
train_acc: 0.7985166872682324, val_acc: 0.5812807881773399, train_loss: 0.5258476126695003, val_loss: 1.2652237976125895 (31 / 35)
train_acc: 0.8244746600741656, val_acc: 0.5960591133004927, train_loss: 0.46963487880486343, val_loss: 1.2663611285204959 (32 / 35)
train_acc: 0.8491965389369592, val_acc: 0.6009852216748769, train_loss: 0.4279424289543048, val_loss: 1.293110381206268 (33 / 35)
train_acc: 0.8553770086526576, val_acc: 0.5763546798029556, train_loss: 0.40204033568704084, val_loss: 1.3243666696431013 (34 / 35)
train_acc: 0.8800988875154512, val_acc: 0.6206896551724138, train_loss: 0.34771451354026794, val_loss: 1.301346871653214 (35 / 35)
lr 0.0048979615247910675, batch 12, decay 0.0007368105157485125, gamma 0.09877903626081189, val accuracy 0.6206896551724138, val loss 1.199758934563604 [1 / 50]
-------------------------------------
train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7736480511605224, val_loss: 1.7526064917371778 (1 / 35)
train_acc: 0.21137206427688504, val_acc: 0.2561576354679803, train_loss: 1.7443042594216516, val_loss: 1.6376527735752424 (2 / 35)
train_acc: 0.273176761433869, val_acc: 0.35467980295566504, train_loss: 1.654656814703688, val_loss: 1.5745465053713381 (3 / 35)
train_acc: 0.18046971569839307, val_acc: 0.22167487684729065, train_loss: 1.7715205008668276, val_loss: 1.7590791080972832 (4 / 35)
train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7585067292226406, val_loss: 1.7670292220092172 (5 / 35)
train_acc: 0.25710754017305315, val_acc: 0.2660098522167488, train_loss: 1.71813655842662, val_loss: 1.664226038115365 (6 / 35)
train_acc: 0.27070457354758964, val_acc: 0.2857142857142857, train_loss: 1.6743396380628437, val_loss: 1.7453631238984357 (7 / 35)
train_acc: 0.29295426452410384, val_acc: 0.2561576354679803, train_loss: 1.6124028073547798, val_loss: 1.6032950164649287 (8 / 35)
train_acc: 0.3164400494437577, val_acc: 0.21674876847290642, train_loss: 1.5707165643812258, val_loss: 1.7263645591406986 (9 / 35)
train_acc: 0.3288009888751545, val_acc: 0.3694581280788177, train_loss: 1.542294026157912, val_loss: 1.4088075296044937 (10 / 35)
train_acc: 0.37453646477132263, val_acc: 0.4088669950738916, train_loss: 1.4295319744624078, val_loss: 1.3160603933146435 (11 / 35)
train_acc: 0.34487021013597036, val_acc: 0.42857142857142855, train_loss: 1.4427891269749853, val_loss: 1.3269020213282168 (12 / 35)
train_acc: 0.36093943139678614, val_acc: 0.45320197044334976, train_loss: 1.3894022937463446, val_loss: 1.2865488235586382 (13 / 35)
train_acc: 0.4177997527812114, val_acc: 0.4039408866995074, train_loss: 1.345461735619308, val_loss: 1.3598436751389151 (14 / 35)
train_acc: 0.4437577255871446, val_acc: 0.4827586206896552, train_loss: 1.3195575331285947, val_loss: 1.3616349409366477 (15 / 35)
train_acc: 0.41903584672435107, val_acc: 0.4433497536945813, train_loss: 1.2938431704736906, val_loss: 1.2985044688426803 (16 / 35)
train_acc: 0.4635352286773795, val_acc: 0.4433497536945813, train_loss: 1.226008664986996, val_loss: 1.3651997626121408 (17 / 35)
train_acc: 0.4647713226205192, val_acc: 0.43349753694581283, train_loss: 1.2361429562822113, val_loss: 1.2307789522438801 (18 / 35)
train_acc: 0.4956736711990111, val_acc: 0.4039408866995074, train_loss: 1.189627605846108, val_loss: 1.3760334000798868 (19 / 35)
train_acc: 0.5315203955500618, val_acc: 0.5024630541871922, train_loss: 1.178057996613723, val_loss: 1.2655104225492242 (20 / 35)
train_acc: 0.5377008652657602, val_acc: 0.4482758620689655, train_loss: 1.1166344041877387, val_loss: 1.2910583019256592 (21 / 35)
train_acc: 0.5995055624227441, val_acc: 0.5714285714285714, train_loss: 1.026062799030535, val_loss: 1.103880543133308 (22 / 35)
train_acc: 0.630407911001236, val_acc: 0.5862068965517241, train_loss: 0.8652822756944097, val_loss: 1.0928738686838761 (23 / 35)
train_acc: 0.6724351050679852, val_acc: 0.5763546798029556, train_loss: 0.7871297903644434, val_loss: 1.1020997507231576 (24 / 35)
train_acc: 0.695920889987639, val_acc: 0.5960591133004927, train_loss: 0.7807023369042924, val_loss: 1.091816563030769 (25 / 35)
train_acc: 0.7119901112484549, val_acc: 0.5714285714285714, train_loss: 0.7453222184157636, val_loss: 1.1042851650068912 (26 / 35)
train_acc: 0.7379480840543882, val_acc: 0.5862068965517241, train_loss: 0.7068870488291765, val_loss: 1.1036973780599133 (27 / 35)
train_acc: 0.7416563658838071, val_acc: 0.6059113300492611, train_loss: 0.6629525985941751, val_loss: 1.1093842064218569 (28 / 35)
train_acc: 0.7367119901112484, val_acc: 0.6059113300492611, train_loss: 0.6489057744057718, val_loss: 1.1206492501233012 (29 / 35)
train_acc: 0.7441285537700866, val_acc: 0.5911330049261084, train_loss: 0.647850553022769, val_loss: 1.0941240554079046 (30 / 35)
train_acc: 0.7601977750309024, val_acc: 0.5862068965517241, train_loss: 0.603501689227756, val_loss: 1.1157133259209506 (31 / 35)
train_acc: 0.7824474660074165, val_acc: 0.6059113300492611, train_loss: 0.5613979977568236, val_loss: 1.1163707137695087 (32 / 35)
train_acc: 0.7911001236093943, val_acc: 0.6108374384236454, train_loss: 0.5727301674661295, val_loss: 1.1637333679962627 (33 / 35)
train_acc: 0.799752781211372, val_acc: 0.6206896551724138, train_loss: 0.5303083510496118, val_loss: 1.1612589919361576 (34 / 35)
train_acc: 0.8034610630407911, val_acc: 0.6157635467980296, train_loss: 0.5214012133139467, val_loss: 1.1499783876787852 (35 / 35)
lr 0.005563517269754794, batch 10, decay 0.0007977701264559719, gamma 0.03556976073448573, val accuracy 0.6206896551724138, val loss 1.1612589919361576 [2 / 50]
-------------------------------------
train_acc: 0.16440049443757726, val_acc: 0.22167487684729065, train_loss: 1.7805145807997111, val_loss: 1.7559497156753916 (1 / 35)
train_acc: 0.21508034610630408, val_acc: 0.29064039408866993, train_loss: 1.747242243387201, val_loss: 1.7354320046936937 (2 / 35)
train_acc: 0.29295426452410384, val_acc: 0.3054187192118227, train_loss: 1.6598902859705487, val_loss: 1.588314405215785 (3 / 35)
train_acc: 0.28182941903584674, val_acc: 0.2857142857142857, train_loss: 1.654859683422282, val_loss: 1.6462655302339 (4 / 35)
train_acc: 0.207663782447466, val_acc: 0.2857142857142857, train_loss: 1.7650609595519209, val_loss: 1.698088478572263 (5 / 35)
train_acc: 0.24969097651421507, val_acc: 0.29064039408866993, train_loss: 1.7321900861667026, val_loss: 1.6020249480684403 (6 / 35)
train_acc: 0.2830655129789864, val_acc: 0.33497536945812806, train_loss: 1.6736535905170795, val_loss: 1.6001322668761455 (7 / 35)
train_acc: 0.30778739184178, val_acc: 0.37438423645320196, train_loss: 1.5595755338374115, val_loss: 1.4167115535642125 (8 / 35)
train_acc: 0.32509270704573545, val_acc: 0.3399014778325123, train_loss: 1.5297795299841241, val_loss: 1.4671604169413375 (9 / 35)
train_acc: 0.3535228677379481, val_acc: 0.3891625615763547, train_loss: 1.4539535875226128, val_loss: 1.458125835569034 (10 / 35)
train_acc: 0.37824474660074164, val_acc: 0.3497536945812808, train_loss: 1.4263694710725612, val_loss: 1.3368913187769247 (11 / 35)
train_acc: 0.3967861557478368, val_acc: 0.39901477832512317, train_loss: 1.3771792264155611, val_loss: 1.3067347198871557 (12 / 35)
train_acc: 0.3980222496909765, val_acc: 0.3497536945812808, train_loss: 1.3868695068713024, val_loss: 1.402725012431591 (13 / 35)
train_acc: 0.4276885043263288, val_acc: 0.41379310344827586, train_loss: 1.3173313795120372, val_loss: 1.3293636861105858 (14 / 35)
train_acc: 0.4103831891223733, val_acc: 0.43349753694581283, train_loss: 1.3258165881424515, val_loss: 1.2297851484397362 (15 / 35)
train_acc: 0.41285537700865266, val_acc: 0.4039408866995074, train_loss: 1.317890340524492, val_loss: 1.3537449548984397 (16 / 35)
train_acc: 0.4363411619283066, val_acc: 0.4236453201970443, train_loss: 1.2633063719505433, val_loss: 1.2779797104191897 (17 / 35)
train_acc: 0.4635352286773795, val_acc: 0.4729064039408867, train_loss: 1.221674284917315, val_loss: 1.2122810256892238 (18 / 35)
train_acc: 0.4622991347342398, val_acc: 0.4187192118226601, train_loss: 1.2729653464848976, val_loss: 1.2548466051740599 (19 / 35)
train_acc: 0.4647713226205192, val_acc: 0.46798029556650245, train_loss: 1.204022017338662, val_loss: 1.2416861447794685 (20 / 35)
train_acc: 0.5129789864029666, val_acc: 0.4236453201970443, train_loss: 1.1509113397232829, val_loss: 1.2344410008397595 (21 / 35)
train_acc: 0.5587144622991347, val_acc: 0.4827586206896552, train_loss: 1.0849087818149288, val_loss: 1.1369734779367306 (22 / 35)
train_acc: 0.5723114956736712, val_acc: 0.4975369458128079, train_loss: 1.0384434897024346, val_loss: 1.1106367416569751 (23 / 35)
train_acc: 0.5908529048207664, val_acc: 0.4827586206896552, train_loss: 0.9984091200551527, val_loss: 1.0985411951694581 (24 / 35)
train_acc: 0.5908529048207664, val_acc: 0.4876847290640394, train_loss: 0.987083595379616, val_loss: 1.0934455060019281 (25 / 35)
train_acc: 0.5822002472187886, val_acc: 0.4876847290640394, train_loss: 0.9928815454723514, val_loss: 1.0865175383431571 (26 / 35)
train_acc: 0.5945611866501854, val_acc: 0.4876847290640394, train_loss: 0.9555827139776628, val_loss: 1.0837723991553772 (27 / 35)
train_acc: 0.6007416563658838, val_acc: 0.49261083743842365, train_loss: 0.955286411036667, val_loss: 1.0849437185108954 (28 / 35)
train_acc: 0.6007416563658838, val_acc: 0.4827586206896552, train_loss: 0.9674207944657806, val_loss: 1.090242438128429 (29 / 35)
train_acc: 0.5908529048207664, val_acc: 0.49261083743842365, train_loss: 0.9571324805688799, val_loss: 1.0791994362629105 (30 / 35)
train_acc: 0.6242274412855378, val_acc: 0.5024630541871922, train_loss: 0.9448048873943805, val_loss: 1.0821466122942018 (31 / 35)
train_acc: 0.5920889987639061, val_acc: 0.5073891625615764, train_loss: 0.9328995764034493, val_loss: 1.0794419831243054 (32 / 35)
train_acc: 0.6328800988875154, val_acc: 0.4975369458128079, train_loss: 0.917833435004957, val_loss: 1.079166317220979 (33 / 35)
train_acc: 0.6205191594561187, val_acc: 0.49261083743842365, train_loss: 0.9312858949190901, val_loss: 1.0789826544634815 (34 / 35)
train_acc: 0.6402966625463535, val_acc: 0.4975369458128079, train_loss: 0.8964742352110788, val_loss: 1.0878861332174592 (35 / 35)
lr 0.004038551874580517, batch 12, decay 0.00024010957050540167, gamma 0.016088536390075778, val accuracy 0.5073891625615764, val loss 1.0794419831243054 [3 / 50]
-------------------------------------
train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7813585027039271, val_loss: 1.7563240158146824 (1 / 35)
train_acc: 0.20642768850432633, val_acc: 0.30049261083743845, train_loss: 1.7460081014409201, val_loss: 1.7274332598512396 (2 / 35)
train_acc: 0.2583436341161928, val_acc: 0.2019704433497537, train_loss: 1.7389107622823254, val_loss: 1.7602327046135964 (3 / 35)
train_acc: 0.2867737948084054, val_acc: 0.35960591133004927, train_loss: 1.6788580508992463, val_loss: 1.5679490933277336 (4 / 35)
train_acc: 0.21137206427688504, val_acc: 0.21182266009852216, train_loss: 1.7563813803959247, val_loss: 1.7447256460565652 (5 / 35)
train_acc: 0.22620519159456118, val_acc: 0.22167487684729065, train_loss: 1.7541621179309557, val_loss: 1.7136098205162387 (6 / 35)
train_acc: 0.23362175525339926, val_acc: 0.22167487684729065, train_loss: 1.709781523835379, val_loss: 1.7024534887868195 (7 / 35)
train_acc: 0.31396786155747836, val_acc: 0.33497536945812806, train_loss: 1.6223760757811727, val_loss: 1.494070135313889 (8 / 35)
train_acc: 0.2941903584672435, val_acc: 0.3251231527093596, train_loss: 1.5920437820467577, val_loss: 1.6988512336326937 (9 / 35)
train_acc: 0.3337453646477132, val_acc: 0.3054187192118227, train_loss: 1.522382194533189, val_loss: 1.6927049631996107 (10 / 35)
train_acc: 0.3535228677379481, val_acc: 0.3645320197044335, train_loss: 1.5105860280460126, val_loss: 1.3876031525616574 (11 / 35)
train_acc: 0.3547589616810878, val_acc: 0.32019704433497537, train_loss: 1.4816197162800284, val_loss: 1.400275103270714 (12 / 35)
train_acc: 0.34363411619283063, val_acc: 0.33497536945812806, train_loss: 1.4248977274770938, val_loss: 1.3599773980126593 (13 / 35)
train_acc: 0.3757725587144623, val_acc: 0.4088669950738916, train_loss: 1.4413662295701035, val_loss: 1.354823926986732 (14 / 35)
train_acc: 0.3831891223733004, val_acc: 0.4039408866995074, train_loss: 1.3799872918535665, val_loss: 1.381907935653414 (15 / 35)
train_acc: 0.3930778739184178, val_acc: 0.4088669950738916, train_loss: 1.3813444359782894, val_loss: 1.3464442976002622 (16 / 35)
train_acc: 0.3943139678615575, val_acc: 0.43349753694581283, train_loss: 1.3771303750675894, val_loss: 1.2930264235130084 (17 / 35)
train_acc: 0.4326328800988875, val_acc: 0.45320197044334976, train_loss: 1.301336153181286, val_loss: 1.277239973615543 (18 / 35)
train_acc: 0.4758961681087763, val_acc: 0.4187192118226601, train_loss: 1.2497339661984863, val_loss: 1.2020986676216125 (19 / 35)
train_acc: 0.4796044499381953, val_acc: 0.43842364532019706, train_loss: 1.2517588089953542, val_loss: 1.2004026374206167 (20 / 35)
train_acc: 0.4894932014833127, val_acc: 0.4729064039408867, train_loss: 1.1786349403254033, val_loss: 1.1695662572466095 (21 / 35)
train_acc: 0.5216316440049443, val_acc: 0.46798029556650245, train_loss: 1.1000407338879163, val_loss: 1.1491495247544914 (22 / 35)
train_acc: 0.5673671199011124, val_acc: 0.458128078817734, train_loss: 1.0579096590190646, val_loss: 1.1199347611718578 (23 / 35)
train_acc: 0.5624227441285538, val_acc: 0.46798029556650245, train_loss: 1.051015780646957, val_loss: 1.1260921513212139 (24 / 35)
train_acc: 0.5661310259579728, val_acc: 0.4630541871921182, train_loss: 1.0579502534660037, val_loss: 1.1184431362915508 (25 / 35)
train_acc: 0.5673671199011124, val_acc: 0.47783251231527096, train_loss: 1.0455802924406101, val_loss: 1.1178725752337226 (26 / 35)
train_acc: 0.553770086526576, val_acc: 0.4729064039408867, train_loss: 1.0588787778641002, val_loss: 1.1165915235803632 (27 / 35)
train_acc: 0.5673671199011124, val_acc: 0.47783251231527096, train_loss: 1.0314655966310773, val_loss: 1.1147028225396067 (28 / 35)
train_acc: 0.546353522867738, val_acc: 0.4975369458128079, train_loss: 1.0315335184446224, val_loss: 1.1257490495155598 (29 / 35)
train_acc: 0.5871446229913473, val_acc: 0.4876847290640394, train_loss: 1.0181443455487453, val_loss: 1.1257411099126187 (30 / 35)
train_acc: 0.5624227441285538, val_acc: 0.49261083743842365, train_loss: 1.0223070708723976, val_loss: 1.1051338775991806 (31 / 35)
train_acc: 0.5822002472187886, val_acc: 0.4827586206896552, train_loss: 1.0075156199475301, val_loss: 1.111583941235331 (32 / 35)
train_acc: 0.5995055624227441, val_acc: 0.4975369458128079, train_loss: 0.9797030922361594, val_loss: 1.0968200344170256 (33 / 35)
train_acc: 0.588380716934487, val_acc: 0.4876847290640394, train_loss: 1.0152381746966406, val_loss: 1.1081405487553826 (34 / 35)
train_acc: 0.5735475896168108, val_acc: 0.4975369458128079, train_loss: 0.9980405246371539, val_loss: 1.1193869680606674 (35 / 35)
lr 0.0042237987628595194, batch 15, decay 0.00042683917479004744, gamma 0.031192333743237592, val accuracy 0.4975369458128079, val loss 1.1257490495155598 [4 / 50]
-------------------------------------
train_acc: 0.18912237330037082, val_acc: 0.1921182266009852, train_loss: 1.7782927997622118, val_loss: 1.7591932589197394 (1 / 35)
train_acc: 0.21878862793572312, val_acc: 0.19704433497536947, train_loss: 1.764888536355404, val_loss: 1.7326638786663562 (2 / 35)
train_acc: 0.23362175525339926, val_acc: 0.32019704433497537, train_loss: 1.7011883694987067, val_loss: 1.5855474454428762 (3 / 35)
train_acc: 0.27812113720642767, val_acc: 0.3448275862068966, train_loss: 1.6650880498556035, val_loss: 1.6015953753382115 (4 / 35)
train_acc: 0.33127317676143386, val_acc: 0.3448275862068966, train_loss: 1.5864642753883993, val_loss: 1.5273066594682891 (5 / 35)
train_acc: 0.3226205191594561, val_acc: 0.3497536945812808, train_loss: 1.6065929532787855, val_loss: 1.5429617084305862 (6 / 35)
train_acc: 0.3238566131025958, val_acc: 0.3399014778325123, train_loss: 1.5534617728444349, val_loss: 1.746960131552419 (7 / 35)
train_acc: 0.3176761433868974, val_acc: 0.2857142857142857, train_loss: 1.5835197561308834, val_loss: 1.634410598595154 (8 / 35)
train_acc: 0.35970333745364647, val_acc: 0.37438423645320196, train_loss: 1.4977778897739311, val_loss: 1.4883260665268734 (9 / 35)
train_acc: 0.3757725587144623, val_acc: 0.3497536945812808, train_loss: 1.4146581534549538, val_loss: 1.3572034794708778 (10 / 35)
train_acc: 0.4177997527812114, val_acc: 0.39901477832512317, train_loss: 1.3532472713326642, val_loss: 1.2954320155928287 (11 / 35)
train_acc: 0.39555006180469715, val_acc: 0.4088669950738916, train_loss: 1.3685078768264525, val_loss: 1.3522927828610236 (12 / 35)
train_acc: 0.4103831891223733, val_acc: 0.45320197044334976, train_loss: 1.3086729141042024, val_loss: 1.276670027836203 (13 / 35)
train_acc: 0.4622991347342398, val_acc: 0.47783251231527096, train_loss: 1.263232729122577, val_loss: 1.3495704625627678 (14 / 35)
train_acc: 0.4400494437577256, val_acc: 0.43349753694581283, train_loss: 1.2753808432073321, val_loss: 1.2257954034899257 (15 / 35)
train_acc: 0.4561186650185414, val_acc: 0.4729064039408867, train_loss: 1.2909379456335004, val_loss: 1.1669913959033384 (16 / 35)
train_acc: 0.49938195302843014, val_acc: 0.4729064039408867, train_loss: 1.1457669540447712, val_loss: 1.192607908119709 (17 / 35)
train_acc: 0.4932014833127318, val_acc: 0.5517241379310345, train_loss: 1.176971461333804, val_loss: 1.074009899728991 (18 / 35)
train_acc: 0.5500618046971569, val_acc: 0.47783251231527096, train_loss: 1.0733089855191733, val_loss: 1.3213121870468403 (19 / 35)
train_acc: 0.5587144622991347, val_acc: 0.4975369458128079, train_loss: 1.0790786682012201, val_loss: 1.1953439988526218 (20 / 35)
train_acc: 0.5970333745364648, val_acc: 0.5320197044334976, train_loss: 0.9738532080785895, val_loss: 1.157695013607664 (21 / 35)
train_acc: 0.5673671199011124, val_acc: 0.5467980295566502, train_loss: 1.0038948083395423, val_loss: 1.0162087145990926 (22 / 35)
train_acc: 0.6378244746600742, val_acc: 0.5517241379310345, train_loss: 0.8767687125760044, val_loss: 1.223518119069743 (23 / 35)
train_acc: 0.6613102595797281, val_acc: 0.6108374384236454, train_loss: 0.8223182224520057, val_loss: 0.9733363880312501 (24 / 35)
train_acc: 0.7070457354758962, val_acc: 0.5172413793103449, train_loss: 0.7544357309530043, val_loss: 1.2957371399907642 (25 / 35)
train_acc: 0.715698393077874, val_acc: 0.49261083743842365, train_loss: 0.7561901041729047, val_loss: 1.439497168134586 (26 / 35)
train_acc: 0.723114956736712, val_acc: 0.5073891625615764, train_loss: 0.7114892152905906, val_loss: 1.4874469223868083 (27 / 35)
train_acc: 0.7416563658838071, val_acc: 0.5024630541871922, train_loss: 0.6294794336458661, val_loss: 1.5114262749996092 (28 / 35)
train_acc: 0.7663782447466008, val_acc: 0.5566502463054187, train_loss: 0.5868667339438708, val_loss: 1.2197190186953897 (29 / 35)
train_acc: 0.8034610630407911, val_acc: 0.5467980295566502, train_loss: 0.520784446847601, val_loss: 1.5236440253962438 (30 / 35)
train_acc: 0.8108776266996292, val_acc: 0.5566502463054187, train_loss: 0.5250764113237301, val_loss: 1.4113283910481214 (31 / 35)
train_acc: 0.8417799752781211, val_acc: 0.541871921182266, train_loss: 0.40269944110450107, val_loss: 2.100176135747891 (32 / 35)
train_acc: 0.8380716934487021, val_acc: 0.5714285714285714, train_loss: 0.45722704227871, val_loss: 1.6193328209111255 (33 / 35)
train_acc: 0.865265760197775, val_acc: 0.5172413793103449, train_loss: 0.3650682800540376, val_loss: 2.6902276227626896 (34 / 35)
train_acc: 0.8059332509270705, val_acc: 0.5862068965517241, train_loss: 0.5110527146032037, val_loss: 1.397288083883342 (35 / 35)
lr 0.004189400775885228, batch 10, decay 0.00010163296545678008, gamma 0.867020722584907, val accuracy 0.6108374384236454, val loss 0.9733363880312501 [5 / 50]
-------------------------------------
train_acc: 0.1903584672435105, val_acc: 0.2512315270935961, train_loss: 1.7829452411206013, val_loss: 1.763297106832119 (1 / 35)
train_acc: 0.20519159456118666, val_acc: 0.18226600985221675, train_loss: 1.7627744098528944, val_loss: 1.7402000157116668 (2 / 35)
train_acc: 0.22620519159456118, val_acc: 0.35467980295566504, train_loss: 1.7415383308573322, val_loss: 1.7057915056867552 (3 / 35)
train_acc: 0.2880098887515451, val_acc: 0.2315270935960591, train_loss: 1.6764177850209296, val_loss: 1.7675595656404355 (4 / 35)
train_acc: 0.29913473423980225, val_acc: 0.2955665024630542, train_loss: 1.648133249748475, val_loss: 1.6543967292226593 (5 / 35)
train_acc: 0.31396786155747836, val_acc: 0.3054187192118227, train_loss: 1.5726994806520724, val_loss: 1.5371520705704618 (6 / 35)
train_acc: 0.34610630407911, val_acc: 0.39408866995073893, train_loss: 1.5832011894183635, val_loss: 1.4865828027278918 (7 / 35)
train_acc: 0.311495673671199, val_acc: 0.3103448275862069, train_loss: 1.6104984013789663, val_loss: 1.5585575329846348 (8 / 35)
train_acc: 0.3374536464771323, val_acc: 0.35467980295566504, train_loss: 1.512822423051992, val_loss: 1.4672659333116316 (9 / 35)
train_acc: 0.377008652657602, val_acc: 0.3645320197044335, train_loss: 1.4179231937795105, val_loss: 1.3704983329244436 (10 / 35)
train_acc: 0.37330037082818296, val_acc: 0.4088669950738916, train_loss: 1.5017297854382263, val_loss: 1.3271967800967213 (11 / 35)
train_acc: 0.3819530284301607, val_acc: 0.42857142857142855, train_loss: 1.4256686777502703, val_loss: 1.2879130919578627 (12 / 35)
train_acc: 0.4363411619283066, val_acc: 0.3399014778325123, train_loss: 1.3775962011333744, val_loss: 1.3791363171755975 (13 / 35)
train_acc: 0.37453646477132263, val_acc: 0.39901477832512317, train_loss: 1.3934164618974267, val_loss: 1.3369207722800118 (14 / 35)
train_acc: 0.40667490729295425, val_acc: 0.4187192118226601, train_loss: 1.3300335910470582, val_loss: 1.2739211787731188 (15 / 35)
train_acc: 0.44252163164400493, val_acc: 0.43349753694581283, train_loss: 1.300202596541241, val_loss: 1.23452613535773 (16 / 35)
train_acc: 0.4437577255871446, val_acc: 0.43842364532019706, train_loss: 1.311495336230812, val_loss: 1.3222443611163812 (17 / 35)
train_acc: 0.4264524103831891, val_acc: 0.43842364532019706, train_loss: 1.3233832457599004, val_loss: 1.2509075106658372 (18 / 35)
train_acc: 0.4400494437577256, val_acc: 0.4630541871921182, train_loss: 1.2566731732766914, val_loss: 1.1900387620691009 (19 / 35)
train_acc: 0.4252163164400494, val_acc: 0.46798029556650245, train_loss: 1.2525141796900698, val_loss: 1.2539047877776799 (20 / 35)
train_acc: 0.4635352286773795, val_acc: 0.4876847290640394, train_loss: 1.231956002223035, val_loss: 1.1385348809176479 (21 / 35)
train_acc: 0.5364647713226205, val_acc: 0.4975369458128079, train_loss: 1.1250023035832182, val_loss: 1.1806237330577645 (22 / 35)
train_acc: 0.5512978986402967, val_acc: 0.5024630541871922, train_loss: 1.090292848158531, val_loss: 1.113583680737782 (23 / 35)
train_acc: 0.5525339925834364, val_acc: 0.4876847290640394, train_loss: 1.0855487711055316, val_loss: 1.1747858608003907 (24 / 35)
train_acc: 0.5636588380716935, val_acc: 0.5221674876847291, train_loss: 1.0728763257911975, val_loss: 1.081966430682854 (25 / 35)
train_acc: 0.5822002472187886, val_acc: 0.5172413793103449, train_loss: 1.0040332542361672, val_loss: 1.1156141564176587 (26 / 35)
train_acc: 0.595797280593325, val_acc: 0.5123152709359606, train_loss: 0.9787440523597307, val_loss: 1.0918002240176272 (27 / 35)
train_acc: 0.6007416563658838, val_acc: 0.4975369458128079, train_loss: 0.990268266760964, val_loss: 1.1899239642573107 (28 / 35)
train_acc: 0.6205191594561187, val_acc: 0.5467980295566502, train_loss: 0.9599608484245791, val_loss: 1.0305330312898007 (29 / 35)
train_acc: 0.6291718170580964, val_acc: 0.5517241379310345, train_loss: 0.9210277954933818, val_loss: 1.0267447030603005 (30 / 35)
train_acc: 0.6143386897404203, val_acc: 0.5172413793103449, train_loss: 0.9649401038185186, val_loss: 1.1133585394603278 (31 / 35)
train_acc: 0.6402966625463535, val_acc: 0.5172413793103449, train_loss: 0.8870585533169792, val_loss: 1.138330170893904 (32 / 35)
train_acc: 0.6168108776266996, val_acc: 0.5665024630541872, train_loss: 0.869930945088159, val_loss: 1.0141521610062698 (33 / 35)
train_acc: 0.6711990111248455, val_acc: 0.5467980295566502, train_loss: 0.8387340850234768, val_loss: 1.15184840259 (34 / 35)
train_acc: 0.7008652657601978, val_acc: 0.5714285714285714, train_loss: 0.7875853433228835, val_loss: 1.0816205089609023 (35 / 35)
lr 0.0019921626228324653, batch 13, decay 1.662468058989626e-06, gamma 0.342088312815491, val accuracy 0.5714285714285714, val loss 1.0816205089609023 [6 / 50]
-------------------------------------
train_acc: 0.1681087762669963, val_acc: 0.21182266009852216, train_loss: 1.775088109103652, val_loss: 1.753412405845567 (1 / 35)
train_acc: 0.19777503090234858, val_acc: 0.22167487684729065, train_loss: 1.7638309309892217, val_loss: 1.7331564949063831 (2 / 35)
train_acc: 0.25092707045735474, val_acc: 0.18719211822660098, train_loss: 1.699836265318768, val_loss: 1.7748292379191357 (3 / 35)
train_acc: 0.22867737948084055, val_acc: 0.20689655172413793, train_loss: 1.7548576966204366, val_loss: 1.7137495561186316 (4 / 35)
train_acc: 0.2904820766378245, val_acc: 0.3054187192118227, train_loss: 1.691244036217113, val_loss: 1.5569172005348018 (5 / 35)
train_acc: 0.3288009888751545, val_acc: 0.3054187192118227, train_loss: 1.6086610038436682, val_loss: 1.549316883674396 (6 / 35)
train_acc: 0.33250927070457353, val_acc: 0.2561576354679803, train_loss: 1.5122009534918333, val_loss: 1.7940487321374452 (7 / 35)
train_acc: 0.32509270704573545, val_acc: 0.32019704433497537, train_loss: 1.537063990888843, val_loss: 1.4285990898245073 (8 / 35)
train_acc: 0.3362175525339926, val_acc: 0.37438423645320196, train_loss: 1.4472307899531682, val_loss: 1.380091325402847 (9 / 35)
train_acc: 0.3757725587144623, val_acc: 0.3054187192118227, train_loss: 1.4280988807583916, val_loss: 1.6691012511699659 (10 / 35)
train_acc: 0.3658838071693449, val_acc: 0.4187192118226601, train_loss: 1.4227017153915575, val_loss: 1.2785000765852153 (11 / 35)
train_acc: 0.4215080346106304, val_acc: 0.3891625615763547, train_loss: 1.3218343873843275, val_loss: 1.457481889889158 (12 / 35)
train_acc: 0.40296662546353523, val_acc: 0.4236453201970443, train_loss: 1.372006969781977, val_loss: 1.2974853732903016 (13 / 35)
train_acc: 0.4054388133498146, val_acc: 0.3842364532019704, train_loss: 1.3602406857776996, val_loss: 1.3414869044214635 (14 / 35)
train_acc: 0.4400494437577256, val_acc: 0.42857142857142855, train_loss: 1.252904897567221, val_loss: 1.2762779784320024 (15 / 35)
train_acc: 0.4622991347342398, val_acc: 0.4482758620689655, train_loss: 1.262585673550298, val_loss: 1.2055598923138209 (16 / 35)
train_acc: 0.45241038318912236, val_acc: 0.458128078817734, train_loss: 1.2388638467222857, val_loss: 1.2256670456214491 (17 / 35)
train_acc: 0.4894932014833127, val_acc: 0.458128078817734, train_loss: 1.1921869535528684, val_loss: 1.2423155166832685 (18 / 35)
train_acc: 0.5080346106304079, val_acc: 0.458128078817734, train_loss: 1.147345401742697, val_loss: 1.2796426623912867 (19 / 35)
train_acc: 0.5203955500618047, val_acc: 0.5073891625615764, train_loss: 1.1373282435208523, val_loss: 1.1762601242864072 (20 / 35)
train_acc: 0.5587144622991347, val_acc: 0.3448275862068966, train_loss: 1.0293337989354459, val_loss: 1.6982061252218161 (21 / 35)
train_acc: 0.6588380716934487, val_acc: 0.4975369458128079, train_loss: 0.8636062696336668, val_loss: 1.1931791734225645 (22 / 35)
train_acc: 0.6996291718170581, val_acc: 0.5270935960591133, train_loss: 0.7630448809954969, val_loss: 1.1365692210314897 (23 / 35)
train_acc: 0.7008652657601978, val_acc: 0.5517241379310345, train_loss: 0.7287297496394852, val_loss: 1.149295200561655 (24 / 35)
train_acc: 0.7243510506798516, val_acc: 0.5270935960591133, train_loss: 0.6767687432108762, val_loss: 1.1678240305097232 (25 / 35)
train_acc: 0.7503090234857849, val_acc: 0.5517241379310345, train_loss: 0.6472940527464462, val_loss: 1.1786103683152223 (26 / 35)
train_acc: 0.7527812113720643, val_acc: 0.541871921182266, train_loss: 0.6187858459239247, val_loss: 1.2078671713767968 (27 / 35)
train_acc: 0.7700865265760197, val_acc: 0.5665024630541872, train_loss: 0.5885912564540529, val_loss: 1.1847642927334225 (28 / 35)
train_acc: 0.8034610630407911, val_acc: 0.5615763546798029, train_loss: 0.5177139400404375, val_loss: 1.3405331420193751 (29 / 35)
train_acc: 0.8084054388133498, val_acc: 0.5763546798029556, train_loss: 0.5007249186153907, val_loss: 1.2566691214227912 (30 / 35)
train_acc: 0.799752781211372, val_acc: 0.6009852216748769, train_loss: 0.5152063125143534, val_loss: 1.2801211978414375 (31 / 35)
train_acc: 0.8071693448702101, val_acc: 0.6009852216748769, train_loss: 0.47329392244553536, val_loss: 1.2884564986957119 (32 / 35)
train_acc: 0.8454882571075402, val_acc: 0.5862068965517241, train_loss: 0.4229863943953449, val_loss: 1.5056029293924718 (33 / 35)
train_acc: 0.8479604449938195, val_acc: 0.5862068965517241, train_loss: 0.40157806829115367, val_loss: 1.4207099130000975 (34 / 35)
train_acc: 0.8603213844252163, val_acc: 0.5960591133004927, train_loss: 0.3707203178382185, val_loss: 1.4292544872302728 (35 / 35)
lr 0.002769949085723339, batch 8, decay 0.00011447095044880037, gamma 0.09277641094378263, val accuracy 0.6009852216748769, val loss 1.2801211978414375 [7 / 50]
-------------------------------------
train_acc: 0.16069221260815822, val_acc: 0.18226600985221675, train_loss: 1.7774352043019825, val_loss: 1.7563627582465486 (1 / 35)
train_acc: 0.21631644004944375, val_acc: 0.18226600985221675, train_loss: 1.738082394464349, val_loss: 1.7710922316377387 (2 / 35)
train_acc: 0.2595797280593325, val_acc: 0.2561576354679803, train_loss: 1.6692724168963722, val_loss: 1.7227710285797495 (3 / 35)
train_acc: 0.20519159456118666, val_acc: 0.21182266009852216, train_loss: 1.7682119641816807, val_loss: 1.7294712636271135 (4 / 35)
train_acc: 0.21137206427688504, val_acc: 0.2413793103448276, train_loss: 1.7583788338638795, val_loss: 1.7097577455595796 (5 / 35)
train_acc: 0.24969097651421507, val_acc: 0.2660098522167488, train_loss: 1.7071011200528798, val_loss: 1.6614492585506346 (6 / 35)
train_acc: 0.27935723114956734, val_acc: 0.270935960591133, train_loss: 1.6718877886959589, val_loss: 1.6789877191552975 (7 / 35)
train_acc: 0.33250927070457353, val_acc: 0.35467980295566504, train_loss: 1.5720450736683584, val_loss: 1.5079294079043009 (8 / 35)
train_acc: 0.35599505562422745, val_acc: 0.33004926108374383, train_loss: 1.521399873590882, val_loss: 1.4814303185552211 (9 / 35)
train_acc: 0.3621755253399258, val_acc: 0.3842364532019704, train_loss: 1.487493130126312, val_loss: 1.432254589837173 (10 / 35)
train_acc: 0.36711990111248455, val_acc: 0.39408866995073893, train_loss: 1.4177656668815095, val_loss: 1.404390582310155 (11 / 35)
train_acc: 0.39555006180469715, val_acc: 0.41379310344827586, train_loss: 1.3951420445082656, val_loss: 1.4053271368806586 (12 / 35)
train_acc: 0.40914709517923364, val_acc: 0.39901477832512317, train_loss: 1.3745329164898734, val_loss: 1.3698088547279095 (13 / 35)
train_acc: 0.44746600741656367, val_acc: 0.3103448275862069, train_loss: 1.295762135012925, val_loss: 1.653196808739836 (14 / 35)
train_acc: 0.43139678615574784, val_acc: 0.3645320197044335, train_loss: 1.2925833830579987, val_loss: 1.3717694772875368 (15 / 35)
train_acc: 0.4907292954264524, val_acc: 0.39901477832512317, train_loss: 1.2196413645490876, val_loss: 1.384580725225909 (16 / 35)
train_acc: 0.5142150803461063, val_acc: 0.45320197044334976, train_loss: 1.150225514387171, val_loss: 1.3552140244122208 (17 / 35)
train_acc: 0.5327564894932015, val_acc: 0.4039408866995074, train_loss: 1.1339430411165519, val_loss: 1.3565100754423094 (18 / 35)
train_acc: 0.5908529048207664, val_acc: 0.4433497536945813, train_loss: 1.030558724927961, val_loss: 1.4810550261307232 (19 / 35)
train_acc: 0.6291718170580964, val_acc: 0.4039408866995074, train_loss: 0.8971746457079874, val_loss: 1.7766964001021361 (20 / 35)
train_acc: 0.6514215080346106, val_acc: 0.3645320197044335, train_loss: 0.8826779647280171, val_loss: 1.5053982722935417 (21 / 35)
train_acc: 0.7898640296662547, val_acc: 0.4827586206896552, train_loss: 0.5835465318045598, val_loss: 1.4590294889628594 (22 / 35)
train_acc: 0.8714462299134734, val_acc: 0.458128078817734, train_loss: 0.37379122237192536, val_loss: 1.5622944887635744 (23 / 35)
train_acc: 0.9048207663782447, val_acc: 0.4876847290640394, train_loss: 0.27619838316744133, val_loss: 1.777060744797655 (24 / 35)
train_acc: 0.92336217552534, val_acc: 0.5123152709359606, train_loss: 0.2180372159766915, val_loss: 1.8795782304162463 (25 / 35)
train_acc: 0.9419035846724351, val_acc: 0.5024630541871922, train_loss: 0.15761975393307076, val_loss: 1.8952994936792722 (26 / 35)
train_acc: 0.9740420271940667, val_acc: 0.5221674876847291, train_loss: 0.09803077655905404, val_loss: 2.3864782203007215 (27 / 35)
train_acc: 0.9765142150803461, val_acc: 0.5369458128078818, train_loss: 0.08616552644665958, val_loss: 2.55171890505429 (28 / 35)
train_acc: 0.9678615574783683, val_acc: 0.5221674876847291, train_loss: 0.10616114142503963, val_loss: 2.311634160908572 (29 / 35)
train_acc: 0.9839307787391842, val_acc: 0.5270935960591133, train_loss: 0.07339102759202125, val_loss: 2.5725176310891587 (30 / 35)
train_acc: 0.9740420271940667, val_acc: 0.5172413793103449, train_loss: 0.07002676255328694, val_loss: 2.2854453190206896 (31 / 35)
train_acc: 0.9814585908529048, val_acc: 0.5517241379310345, train_loss: 0.052623165847344514, val_loss: 2.902159128870283 (32 / 35)
train_acc: 0.9851668726823238, val_acc: 0.5073891625615764, train_loss: 0.05836838844827433, val_loss: 2.8830012159394514 (33 / 35)
train_acc: 0.9938195302843016, val_acc: 0.5270935960591133, train_loss: 0.03385858188011443, val_loss: 2.919295038495745 (34 / 35)
train_acc: 0.9901112484548825, val_acc: 0.5320197044334976, train_loss: 0.03333206111922105, val_loss: 2.7508338959933503 (35 / 35)
lr 0.004505781266978887, batch 8, decay 3.7680671759241114e-05, gamma 0.13384037348449876, val accuracy 0.5517241379310345, val loss 2.902159128870283 [8 / 50]
-------------------------------------
train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7777289720931366, val_loss: 1.759886389295456 (1 / 35)
train_acc: 0.20395550061804696, val_acc: 0.18719211822660098, train_loss: 1.7640624920134196, val_loss: 1.7478779642452746 (2 / 35)
train_acc: 0.2200247218788628, val_acc: 0.29064039408866993, train_loss: 1.7417277495557504, val_loss: 1.700667796463802 (3 / 35)
train_acc: 0.3053152039555006, val_acc: 0.3793103448275862, train_loss: 1.6589095017818645, val_loss: 1.5359722105740325 (4 / 35)
train_acc: 0.32014833127317677, val_acc: 0.3793103448275862, train_loss: 1.6037571717546366, val_loss: 1.492073650430576 (5 / 35)
train_acc: 0.3362175525339926, val_acc: 0.39408866995073893, train_loss: 1.5380188598768378, val_loss: 1.377736015860083 (6 / 35)
train_acc: 0.3374536464771323, val_acc: 0.3645320197044335, train_loss: 1.5275520367734365, val_loss: 1.4491975257549379 (7 / 35)
train_acc: 0.3868974042027194, val_acc: 0.3891625615763547, train_loss: 1.447697677777341, val_loss: 1.3511543077201091 (8 / 35)
train_acc: 0.3757725587144623, val_acc: 0.39901477832512317, train_loss: 1.3939310391842215, val_loss: 1.343105879616855 (9 / 35)
train_acc: 0.3683559950556242, val_acc: 0.3793103448275862, train_loss: 1.3853663658476878, val_loss: 1.3807529337300455 (10 / 35)
train_acc: 0.39184177997527814, val_acc: 0.3793103448275862, train_loss: 1.3734457635761634, val_loss: 1.261102474381771 (11 / 35)
train_acc: 0.4289245982694685, val_acc: 0.45320197044334976, train_loss: 1.341609590575191, val_loss: 1.2445505939680954 (12 / 35)
train_acc: 0.411619283065513, val_acc: 0.4482758620689655, train_loss: 1.3292553287059325, val_loss: 1.2747835998464687 (13 / 35)
train_acc: 0.4622991347342398, val_acc: 0.43842364532019706, train_loss: 1.2678917408431563, val_loss: 1.2654647903489362 (14 / 35)
train_acc: 0.4215080346106304, val_acc: 0.4433497536945813, train_loss: 1.3122761204599303, val_loss: 1.235061399161522 (15 / 35)
train_acc: 0.4622991347342398, val_acc: 0.4236453201970443, train_loss: 1.2179687261876129, val_loss: 1.339995025413964 (16 / 35)
train_acc: 0.47713226205191595, val_acc: 0.46798029556650245, train_loss: 1.181505857498301, val_loss: 1.2111038706572772 (17 / 35)
train_acc: 0.511742892459827, val_acc: 0.46798029556650245, train_loss: 1.163525467897375, val_loss: 1.1542523752879628 (18 / 35)
train_acc: 0.5241038318912238, val_acc: 0.4975369458128079, train_loss: 1.1313594887666265, val_loss: 1.141069413112302 (19 / 35)
train_acc: 0.5327564894932015, val_acc: 0.5172413793103449, train_loss: 1.121046991740228, val_loss: 1.2028562696696503 (20 / 35)
train_acc: 0.5451174289245982, val_acc: 0.5270935960591133, train_loss: 1.060566203113834, val_loss: 1.1485621826402073 (21 / 35)
train_acc: 0.6291718170580964, val_acc: 0.5270935960591133, train_loss: 0.907656138405959, val_loss: 1.1624213198722877 (22 / 35)
train_acc: 0.6477132262051916, val_acc: 0.5221674876847291, train_loss: 0.8549319254453162, val_loss: 1.0832253735640953 (23 / 35)
train_acc: 0.6674907292954264, val_acc: 0.5320197044334976, train_loss: 0.8056370965924634, val_loss: 1.0581164775517187 (24 / 35)
train_acc: 0.6711990111248455, val_acc: 0.5221674876847291, train_loss: 0.7929937090213572, val_loss: 1.1985975564113391 (25 / 35)
train_acc: 0.6971569839307787, val_acc: 0.541871921182266, train_loss: 0.7427344072623955, val_loss: 1.1157158266734608 (26 / 35)
train_acc: 0.7033374536464772, val_acc: 0.5517241379310345, train_loss: 0.7140486924845739, val_loss: 1.1564096592623612 (27 / 35)
train_acc: 0.7280593325092707, val_acc: 0.5665024630541872, train_loss: 0.669510917846293, val_loss: 1.0684686116690707 (28 / 35)
train_acc: 0.7354758961681088, val_acc: 0.5517241379310345, train_loss: 0.6549434600529181, val_loss: 1.1587972635118833 (29 / 35)
train_acc: 0.7255871446229913, val_acc: 0.5812807881773399, train_loss: 0.6578556963407213, val_loss: 1.1249560076614906 (30 / 35)
train_acc: 0.7787391841779975, val_acc: 0.5467980295566502, train_loss: 0.5711697551464415, val_loss: 1.2257172494686295 (31 / 35)
train_acc: 0.7713226205191595, val_acc: 0.5714285714285714, train_loss: 0.5701093072870606, val_loss: 1.1950469974226552 (32 / 35)
train_acc: 0.7564894932014833, val_acc: 0.5566502463054187, train_loss: 0.5753904275494836, val_loss: 1.2153446389536553 (33 / 35)
train_acc: 0.7688504326328801, val_acc: 0.5862068965517241, train_loss: 0.5764595017444955, val_loss: 1.0978303556078173 (34 / 35)
train_acc: 0.7948084054388134, val_acc: 0.5615763546798029, train_loss: 0.5230950245032794, val_loss: 1.4870188689877835 (35 / 35)
lr 0.002977101119836985, batch 11, decay 4.150884120307582e-05, gamma 0.13388785635608252, val accuracy 0.5862068965517241, val loss 1.0978303556078173 [9 / 50]
-------------------------------------
train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.783722990375514, val_loss: 1.764444936085217 (1 / 35)
train_acc: 0.20519159456118666, val_acc: 0.18226600985221675, train_loss: 1.747963473587602, val_loss: 1.7570637223755785 (2 / 35)
train_acc: 0.21631644004944375, val_acc: 0.18226600985221675, train_loss: 1.7438685873382909, val_loss: 1.747198921119051 (3 / 35)
train_acc: 0.27070457354758964, val_acc: 0.3399014778325123, train_loss: 1.6951605516252175, val_loss: 1.5710579614921156 (4 / 35)
train_acc: 0.30778739184178, val_acc: 0.21674876847290642, train_loss: 1.6483755916246525, val_loss: 1.919127398523791 (5 / 35)
train_acc: 0.32014833127317677, val_acc: 0.3645320197044335, train_loss: 1.608503840615045, val_loss: 1.4479244988540123 (6 / 35)
train_acc: 0.33868974042027195, val_acc: 0.41379310344827586, train_loss: 1.520148241328368, val_loss: 1.3715711514938054 (7 / 35)
train_acc: 0.3572311495673671, val_acc: 0.3251231527093596, train_loss: 1.5099773881461918, val_loss: 1.4103359348081015 (8 / 35)
train_acc: 0.37453646477132263, val_acc: 0.35467980295566504, train_loss: 1.4405363242617348, val_loss: 1.5208500846853397 (9 / 35)
train_acc: 0.36711990111248455, val_acc: 0.3054187192118227, train_loss: 1.415419070770773, val_loss: 1.4705473848164374 (10 / 35)
train_acc: 0.35599505562422745, val_acc: 0.3694581280788177, train_loss: 1.4488970691105638, val_loss: 1.4238544661423256 (11 / 35)
train_acc: 0.380716934487021, val_acc: 0.42857142857142855, train_loss: 1.3866783034226804, val_loss: 1.3481607842327925 (12 / 35)
train_acc: 0.42027194066749074, val_acc: 0.4729064039408867, train_loss: 1.3451155372544183, val_loss: 1.2490838402010538 (13 / 35)
train_acc: 0.40667490729295425, val_acc: 0.4236453201970443, train_loss: 1.3453229207485657, val_loss: 1.3063845734290889 (14 / 35)
train_acc: 0.4400494437577256, val_acc: 0.43349753694581283, train_loss: 1.2993955081708646, val_loss: 1.2699194022000129 (15 / 35)
train_acc: 0.4326328800988875, val_acc: 0.4630541871921182, train_loss: 1.2934255048872072, val_loss: 1.2277976139425644 (16 / 35)
train_acc: 0.4561186650185414, val_acc: 0.4630541871921182, train_loss: 1.2762718362772862, val_loss: 1.233036385381163 (17 / 35)
train_acc: 0.4610630407911001, val_acc: 0.4630541871921182, train_loss: 1.2472830937141541, val_loss: 1.2835186217806023 (18 / 35)
train_acc: 0.4437577255871446, val_acc: 0.5024630541871922, train_loss: 1.304946999025286, val_loss: 1.1952551136463148 (19 / 35)
train_acc: 0.48702101359703337, val_acc: 0.4482758620689655, train_loss: 1.2052636992356687, val_loss: 1.330090036533149 (20 / 35)
train_acc: 0.48825710754017304, val_acc: 0.458128078817734, train_loss: 1.1956934330637288, val_loss: 1.1759917642095405 (21 / 35)
train_acc: 0.5006180469715699, val_acc: 0.49261083743842365, train_loss: 1.1489119292484373, val_loss: 1.194458676676445 (22 / 35)
train_acc: 0.511742892459827, val_acc: 0.4975369458128079, train_loss: 1.1070038597427576, val_loss: 1.1819156760652665 (23 / 35)
train_acc: 0.5525339925834364, val_acc: 0.541871921182266, train_loss: 1.0729638112637552, val_loss: 1.1094482273891055 (24 / 35)
train_acc: 0.5661310259579728, val_acc: 0.5172413793103449, train_loss: 1.0527097736801883, val_loss: 1.243190728972111 (25 / 35)
train_acc: 0.5648949320148331, val_acc: 0.5123152709359606, train_loss: 1.0359333336574332, val_loss: 1.116585764685288 (26 / 35)
train_acc: 0.6069221260815822, val_acc: 0.5320197044334976, train_loss: 0.954275896286935, val_loss: 1.081476121113218 (27 / 35)
train_acc: 0.6279357231149567, val_acc: 0.5172413793103449, train_loss: 0.9283272971034198, val_loss: 1.2281454534366214 (28 / 35)
train_acc: 0.6180469715698393, val_acc: 0.5566502463054187, train_loss: 0.9719327409718328, val_loss: 1.0998677452796786 (29 / 35)
train_acc: 0.65389369592089, val_acc: 0.5714285714285714, train_loss: 0.8592370190048689, val_loss: 1.090796104205653 (30 / 35)
train_acc: 0.6662546353522868, val_acc: 0.5714285714285714, train_loss: 0.852346097583087, val_loss: 1.086410992251241 (31 / 35)
train_acc: 0.6897404202719407, val_acc: 0.5763546798029556, train_loss: 0.7817242566970429, val_loss: 0.950794367073792 (32 / 35)
train_acc: 0.69221260815822, val_acc: 0.5763546798029556, train_loss: 0.7927190936245346, val_loss: 1.1491598419367974 (33 / 35)
train_acc: 0.7095179233621756, val_acc: 0.5221674876847291, train_loss: 0.735212870669748, val_loss: 1.485970807192948 (34 / 35)
train_acc: 0.7367119901112484, val_acc: 0.5763546798029556, train_loss: 0.6770164825123527, val_loss: 1.009009408319525 (35 / 35)
lr 0.0011772079063449023, batch 8, decay 0.000809601695815408, gamma 0.6494556876110809, val accuracy 0.5763546798029556, val loss 0.950794367073792 [10 / 50]
-------------------------------------
train_acc: 0.17181705809641531, val_acc: 0.18226600985221675, train_loss: 1.7730902337615364, val_loss: 1.746286363437258 (1 / 35)
train_acc: 0.2138442521631644, val_acc: 0.27586206896551724, train_loss: 1.739457838467526, val_loss: 1.7289673336620988 (2 / 35)
train_acc: 0.2941903584672435, val_acc: 0.2413793103448276, train_loss: 1.6499318345662837, val_loss: 1.8279977571494475 (3 / 35)
train_acc: 0.3065512978986403, val_acc: 0.28078817733990147, train_loss: 1.6184794787865486, val_loss: 1.6961555886151167 (4 / 35)
train_acc: 0.315203955500618, val_acc: 0.24630541871921183, train_loss: 1.6111235264941994, val_loss: 1.759165776186976 (5 / 35)
train_acc: 0.27935723114956734, val_acc: 0.2512315270935961, train_loss: 1.659845726009647, val_loss: 1.7036204619947912 (6 / 35)
train_acc: 0.32014833127317677, val_acc: 0.2660098522167488, train_loss: 1.5704509573902277, val_loss: 1.7390686495257128 (7 / 35)
train_acc: 0.31396786155747836, val_acc: 0.3645320197044335, train_loss: 1.585432004132878, val_loss: 1.3719806069223752 (8 / 35)
train_acc: 0.35599505562422745, val_acc: 0.39408866995073893, train_loss: 1.4432223662457744, val_loss: 1.3758706529739455 (9 / 35)
train_acc: 0.37453646477132263, val_acc: 0.4187192118226601, train_loss: 1.4285648629456131, val_loss: 1.3300027430351145 (10 / 35)
train_acc: 0.41903584672435107, val_acc: 0.4088669950738916, train_loss: 1.3630853627609234, val_loss: 1.3163885726717306 (11 / 35)
train_acc: 0.380716934487021, val_acc: 0.4039408866995074, train_loss: 1.405480369059795, val_loss: 1.3122904758735243 (12 / 35)
train_acc: 0.4326328800988875, val_acc: 0.4630541871921182, train_loss: 1.314797665560054, val_loss: 1.2349614421722337 (13 / 35)
train_acc: 0.4363411619283066, val_acc: 0.4482758620689655, train_loss: 1.2820840617929607, val_loss: 1.2429259416505034 (14 / 35)
train_acc: 0.4289245982694685, val_acc: 0.4482758620689655, train_loss: 1.251398192274261, val_loss: 1.3473532888396034 (15 / 35)
train_acc: 0.4796044499381953, val_acc: 0.4975369458128079, train_loss: 1.2035924126250193, val_loss: 1.1860004757425469 (16 / 35)
train_acc: 0.519159456118665, val_acc: 0.49261083743842365, train_loss: 1.168804095834678, val_loss: 1.4498757449864166 (17 / 35)
train_acc: 0.5315203955500618, val_acc: 0.5221674876847291, train_loss: 1.1326922178268433, val_loss: 1.2681057180089903 (18 / 35)
train_acc: 0.5203955500618047, val_acc: 0.49261083743842365, train_loss: 1.1220547767593185, val_loss: 1.3019797164084288 (19 / 35)
train_acc: 0.5871446229913473, val_acc: 0.541871921182266, train_loss: 0.9813052085775086, val_loss: 1.284005668803389 (20 / 35)
train_acc: 0.6081582200247219, val_acc: 0.4975369458128079, train_loss: 0.9543679510191433, val_loss: 1.1406841407268506 (21 / 35)
train_acc: 0.6588380716934487, val_acc: 0.5517241379310345, train_loss: 0.8103429042217316, val_loss: 1.0905149849911628 (22 / 35)
train_acc: 0.7021013597033374, val_acc: 0.541871921182266, train_loss: 0.7573872618054872, val_loss: 1.2000866295962498 (23 / 35)
train_acc: 0.7243510506798516, val_acc: 0.5024630541871922, train_loss: 0.6812270717656215, val_loss: 1.5309180884525693 (24 / 35)
train_acc: 0.7515451174289246, val_acc: 0.5467980295566502, train_loss: 0.6058312073736757, val_loss: 1.22282400463015 (25 / 35)
train_acc: 0.757725587144623, val_acc: 0.6108374384236454, train_loss: 0.563677055026751, val_loss: 1.3896605913421791 (26 / 35)
train_acc: 0.7799752781211372, val_acc: 0.541871921182266, train_loss: 0.5564562559533031, val_loss: 1.6261730715265414 (27 / 35)
train_acc: 0.7725587144622992, val_acc: 0.5812807881773399, train_loss: 0.536017076129598, val_loss: 2.0548902094969757 (28 / 35)
train_acc: 0.7849196538936959, val_acc: 0.5960591133004927, train_loss: 0.5318110604829193, val_loss: 1.3678953465569783 (29 / 35)
train_acc: 0.8145859085290482, val_acc: 0.5960591133004927, train_loss: 0.46746133482463575, val_loss: 1.5416578974923476 (30 / 35)
train_acc: 0.8590852904820766, val_acc: 0.5911330049261084, train_loss: 0.3458995374313317, val_loss: 1.8177106215301992 (31 / 35)
train_acc: 0.8899876390605687, val_acc: 0.5123152709359606, train_loss: 0.33204408265806834, val_loss: 1.6627048343933861 (32 / 35)
train_acc: 0.8368355995055624, val_acc: 0.5369458128078818, train_loss: 0.4399317605432459, val_loss: 1.9893337422110178 (33 / 35)
train_acc: 0.8702101359703337, val_acc: 0.5812807881773399, train_loss: 0.3215132925995569, val_loss: 1.8747549126007286 (34 / 35)
train_acc: 0.899876390605686, val_acc: 0.6206896551724138, train_loss: 0.29802447748376226, val_loss: 1.605207937251171 (35 / 35)
lr 0.004593548898918902, batch 11, decay 0.00011682260793565572, gamma 0.6092083519476216, val accuracy 0.6206896551724138, val loss 1.605207937251171 [11 / 50]
-------------------------------------
train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7756793949895795, val_loss: 1.7480084649447738 (1 / 35)
train_acc: 0.21013597033374537, val_acc: 0.30049261083743845, train_loss: 1.748150199099141, val_loss: 1.703658757538631 (2 / 35)
train_acc: 0.3003708281829419, val_acc: 0.33497536945812806, train_loss: 1.6645109033113061, val_loss: 1.6011636914878056 (3 / 35)
train_acc: 0.29666254635352285, val_acc: 0.35467980295566504, train_loss: 1.628960478438435, val_loss: 1.5147459219241965 (4 / 35)
train_acc: 0.3176761433868974, val_acc: 0.3251231527093596, train_loss: 1.6055201810723625, val_loss: 1.4967409536756318 (5 / 35)
train_acc: 0.3337453646477132, val_acc: 0.33004926108374383, train_loss: 1.5201023111237288, val_loss: 1.4583559817281262 (6 / 35)
train_acc: 0.35599505562422745, val_acc: 0.4187192118226601, train_loss: 1.4569443252089587, val_loss: 1.341003315202121 (7 / 35)
train_acc: 0.3584672435105068, val_acc: 0.4088669950738916, train_loss: 1.4238003585188292, val_loss: 1.3405479583246955 (8 / 35)
train_acc: 0.3621755253399258, val_acc: 0.3399014778325123, train_loss: 1.46917311797478, val_loss: 1.4704624928277115 (9 / 35)
train_acc: 0.38442521631644005, val_acc: 0.3793103448275862, train_loss: 1.3724058285926564, val_loss: 1.5240618565986896 (10 / 35)
train_acc: 0.3992583436341162, val_acc: 0.4039408866995074, train_loss: 1.3464578950066206, val_loss: 1.2974876617563182 (11 / 35)
train_acc: 0.43757725587144625, val_acc: 0.4187192118226601, train_loss: 1.2853584993902332, val_loss: 1.2293622534850548 (12 / 35)
train_acc: 0.4276885043263288, val_acc: 0.4729064039408867, train_loss: 1.2871062496241887, val_loss: 1.1817575097084045 (13 / 35)
train_acc: 0.46600741656365885, val_acc: 0.46798029556650245, train_loss: 1.2519217532114282, val_loss: 1.1868019062897255 (14 / 35)
train_acc: 0.4796044499381953, val_acc: 0.41379310344827586, train_loss: 1.2120543025774773, val_loss: 1.3129890211697282 (15 / 35)
train_acc: 0.484548825710754, val_acc: 0.5073891625615764, train_loss: 1.186096363220875, val_loss: 1.1667365534552212 (16 / 35)
train_acc: 0.49814585908529047, val_acc: 0.39408866995073893, train_loss: 1.153597867680421, val_loss: 1.627859606311239 (17 / 35)
train_acc: 0.5203955500618047, val_acc: 0.47783251231527096, train_loss: 1.1213451703487722, val_loss: 1.1975967657977138 (18 / 35)
train_acc: 0.522867737948084, val_acc: 0.47783251231527096, train_loss: 1.137606324312566, val_loss: 1.1769562260857944 (19 / 35)
train_acc: 0.546353522867738, val_acc: 0.5123152709359606, train_loss: 1.0472092380924485, val_loss: 1.199777027656292 (20 / 35)
train_acc: 0.5822002472187886, val_acc: 0.45320197044334976, train_loss: 1.0266858085861017, val_loss: 1.534004875298204 (21 / 35)
train_acc: 0.6427688504326329, val_acc: 0.5566502463054187, train_loss: 0.8113290316094869, val_loss: 1.051854182933939 (22 / 35)
train_acc: 0.7280593325092707, val_acc: 0.5812807881773399, train_loss: 0.6792279613032771, val_loss: 1.0814191699028015 (23 / 35)
train_acc: 0.7404202719406675, val_acc: 0.5615763546798029, train_loss: 0.6242967631083042, val_loss: 1.1547907673079392 (24 / 35)
train_acc: 0.7478368355995055, val_acc: 0.6206896551724138, train_loss: 0.6044683791650978, val_loss: 1.1631359622396271 (25 / 35)
train_acc: 0.796044499381953, val_acc: 0.625615763546798, train_loss: 0.5315942629453425, val_loss: 1.067136653538408 (26 / 35)
train_acc: 0.8022249690976514, val_acc: 0.5665024630541872, train_loss: 0.4996716299627266, val_loss: 1.3682252127548744 (27 / 35)
train_acc: 0.823238566131026, val_acc: 0.5911330049261084, train_loss: 0.47180106797530713, val_loss: 1.2254583342321987 (28 / 35)
train_acc: 0.8034610630407911, val_acc: 0.5812807881773399, train_loss: 0.4889798306195786, val_loss: 1.3059082123739967 (29 / 35)
train_acc: 0.8405438813349815, val_acc: 0.5911330049261084, train_loss: 0.430335882641329, val_loss: 1.3400519777988564 (30 / 35)
train_acc: 0.8491965389369592, val_acc: 0.5960591133004927, train_loss: 0.38525785997565215, val_loss: 1.3669714126093635 (31 / 35)
train_acc: 0.8689740420271941, val_acc: 0.6305418719211823, train_loss: 0.38302991367020034, val_loss: 1.416145349371022 (32 / 35)
train_acc: 0.8813349814585909, val_acc: 0.625615763546798, train_loss: 0.30532311265010326, val_loss: 1.4129076908374656 (33 / 35)
train_acc: 0.8899876390605687, val_acc: 0.6206896551724138, train_loss: 0.3097636854213896, val_loss: 1.5287479121109535 (34 / 35)
train_acc: 0.896168108776267, val_acc: 0.6009852216748769, train_loss: 0.28594726316713726, val_loss: 1.6063515543937683 (35 / 35)
lr 0.005109640841069928, batch 14, decay 0.0005937746882460918, gamma 0.17826825106278887, val accuracy 0.6305418719211823, val loss 1.416145349371022 [12 / 50]
-------------------------------------
train_acc: 0.1681087762669963, val_acc: 0.18226600985221675, train_loss: 1.7840599415476155, val_loss: 1.7697211545089195 (1 / 35)
train_acc: 0.19777503090234858, val_acc: 0.18226600985221675, train_loss: 1.7651224534208017, val_loss: 1.7492527456706382 (2 / 35)
train_acc: 0.21508034610630408, val_acc: 0.29064039408866993, train_loss: 1.7571387226118882, val_loss: 1.7336184121117804 (3 / 35)
train_acc: 0.22867737948084055, val_acc: 0.22660098522167488, train_loss: 1.7421930561254286, val_loss: 1.7101416059315497 (4 / 35)
train_acc: 0.2694684796044499, val_acc: 0.22167487684729065, train_loss: 1.702896379717201, val_loss: 1.651519917502192 (5 / 35)
train_acc: 0.3127317676143387, val_acc: 0.3645320197044335, train_loss: 1.62818651662032, val_loss: 1.503387519878707 (6 / 35)
train_acc: 0.3288009888751545, val_acc: 0.4236453201970443, train_loss: 1.578582261784557, val_loss: 1.4942313227160224 (7 / 35)
train_acc: 0.3399258343634116, val_acc: 0.31527093596059114, train_loss: 1.5247577337458342, val_loss: 1.5448335259418768 (8 / 35)
train_acc: 0.3288009888751545, val_acc: 0.37438423645320196, train_loss: 1.5118039592971613, val_loss: 1.4752149455653036 (9 / 35)
train_acc: 0.3980222496909765, val_acc: 0.3694581280788177, train_loss: 1.4178971882508917, val_loss: 1.3498248965869397 (10 / 35)
train_acc: 0.38936959208899874, val_acc: 0.3891625615763547, train_loss: 1.428168265501265, val_loss: 1.2987492865529553 (11 / 35)
train_acc: 0.38936959208899874, val_acc: 0.43349753694581283, train_loss: 1.4076439994669374, val_loss: 1.2674428958610948 (12 / 35)
train_acc: 0.40296662546353523, val_acc: 0.42857142857142855, train_loss: 1.3600188431840008, val_loss: 1.4171057179056366 (13 / 35)
train_acc: 0.3943139678615575, val_acc: 0.39408866995073893, train_loss: 1.3664470802720898, val_loss: 1.3158685999550843 (14 / 35)
train_acc: 0.41903584672435107, val_acc: 0.39408866995073893, train_loss: 1.3312855102075782, val_loss: 1.3070062630869486 (15 / 35)
train_acc: 0.4042027194066749, val_acc: 0.4433497536945813, train_loss: 1.30929146296309, val_loss: 1.2326440041875604 (16 / 35)
train_acc: 0.40914709517923364, val_acc: 0.4433497536945813, train_loss: 1.2954354636896084, val_loss: 1.2627336788060042 (17 / 35)
train_acc: 0.4276885043263288, val_acc: 0.43842364532019706, train_loss: 1.2969367052480227, val_loss: 1.2297170074115247 (18 / 35)
train_acc: 0.4437577255871446, val_acc: 0.43842364532019706, train_loss: 1.2787110084214228, val_loss: 1.1877019675494416 (19 / 35)
train_acc: 0.4573547589616811, val_acc: 0.4482758620689655, train_loss: 1.2234154600000795, val_loss: 1.247140158279776 (20 / 35)
train_acc: 0.44746600741656367, val_acc: 0.49261083743842365, train_loss: 1.2339057904680815, val_loss: 1.2072289900239466 (21 / 35)
train_acc: 0.5364647713226205, val_acc: 0.4729064039408867, train_loss: 1.1399882076255177, val_loss: 1.1522233615367872 (22 / 35)
train_acc: 0.49814585908529047, val_acc: 0.46798029556650245, train_loss: 1.150644217050267, val_loss: 1.1676073599918722 (23 / 35)
train_acc: 0.5166872682323856, val_acc: 0.4827586206896552, train_loss: 1.0980267411993518, val_loss: 1.138956366795037 (24 / 35)
train_acc: 0.5203955500618047, val_acc: 0.5024630541871922, train_loss: 1.061674182656669, val_loss: 1.1523811840658704 (25 / 35)
train_acc: 0.5624227441285538, val_acc: 0.5073891625615764, train_loss: 1.0625547726899938, val_loss: 1.1253143093856097 (26 / 35)
train_acc: 0.5574783683559951, val_acc: 0.5172413793103449, train_loss: 1.0249667464581644, val_loss: 1.1236802909174577 (27 / 35)
train_acc: 0.5673671199011124, val_acc: 0.49261083743842365, train_loss: 1.0308114551790566, val_loss: 1.1008480975193342 (28 / 35)
train_acc: 0.5648949320148331, val_acc: 0.5270935960591133, train_loss: 1.0023006950821658, val_loss: 1.0939095134805576 (29 / 35)
train_acc: 0.5673671199011124, val_acc: 0.5024630541871922, train_loss: 0.9923268856459996, val_loss: 1.1016444314289562 (30 / 35)
train_acc: 0.5896168108776267, val_acc: 0.5270935960591133, train_loss: 0.9860227706258465, val_loss: 1.0980013512038245 (31 / 35)
train_acc: 0.5908529048207664, val_acc: 0.5467980295566502, train_loss: 0.9894793977033075, val_loss: 1.0886381547439274 (32 / 35)
train_acc: 0.5859085290482077, val_acc: 0.5369458128078818, train_loss: 0.963836417799385, val_loss: 1.0622650603649064 (33 / 35)
train_acc: 0.6081582200247219, val_acc: 0.541871921182266, train_loss: 0.9410783004554447, val_loss: 1.0531397432529281 (34 / 35)
train_acc: 0.619283065512979, val_acc: 0.5270935960591133, train_loss: 0.9278659163726717, val_loss: 1.0728720494091804 (35 / 35)
lr 0.0013885391753410857, batch 11, decay 1.0234302534651643e-05, gamma 0.19729638683881684, val accuracy 0.5467980295566502, val loss 1.0886381547439274 [13 / 50]
-------------------------------------
train_acc: 0.17428924598269468, val_acc: 0.21182266009852216, train_loss: 1.78039643732078, val_loss: 1.757135935017628 (1 / 35)
train_acc: 0.21631644004944375, val_acc: 0.18719211822660098, train_loss: 1.7529229986652897, val_loss: 1.7375868094965743 (2 / 35)
train_acc: 0.24969097651421507, val_acc: 0.33497536945812806, train_loss: 1.707702415245867, val_loss: 1.6273412898256274 (3 / 35)
train_acc: 0.29171817058096416, val_acc: 0.33004926108374383, train_loss: 1.6648308414463944, val_loss: 1.6491254714909445 (4 / 35)
train_acc: 0.30407911001236093, val_acc: 0.39901477832512317, train_loss: 1.6010693061189687, val_loss: 1.4769656458511728 (5 / 35)
train_acc: 0.32756489493201485, val_acc: 0.3103448275862069, train_loss: 1.5632897636799052, val_loss: 1.567420644713153 (6 / 35)
train_acc: 0.3584672435105068, val_acc: 0.3842364532019704, train_loss: 1.4960368002300946, val_loss: 1.3871031754416199 (7 / 35)
train_acc: 0.39555006180469715, val_acc: 0.35960591133004927, train_loss: 1.436753485641904, val_loss: 1.3921537311206311 (8 / 35)
train_acc: 0.3658838071693449, val_acc: 0.3891625615763547, train_loss: 1.470909606541043, val_loss: 1.4375971127026186 (9 / 35)
train_acc: 0.380716934487021, val_acc: 0.4088669950738916, train_loss: 1.386450932921821, val_loss: 1.3137768664089917 (10 / 35)
train_acc: 0.40173053152039556, val_acc: 0.43349753694581283, train_loss: 1.3689004822331394, val_loss: 1.3012263017334962 (11 / 35)
train_acc: 0.40667490729295425, val_acc: 0.42857142857142855, train_loss: 1.3938275506381494, val_loss: 1.2865800196901331 (12 / 35)
train_acc: 0.4042027194066749, val_acc: 0.42857142857142855, train_loss: 1.366742755117169, val_loss: 1.3117145126676324 (13 / 35)
train_acc: 0.3930778739184178, val_acc: 0.43842364532019706, train_loss: 1.342033024918753, val_loss: 1.2412108315035628 (14 / 35)
train_acc: 0.4289245982694685, val_acc: 0.4482758620689655, train_loss: 1.310045230152875, val_loss: 1.2184461217208449 (15 / 35)
train_acc: 0.4561186650185414, val_acc: 0.43842364532019706, train_loss: 1.2544079300647024, val_loss: 1.213279895741364 (16 / 35)
train_acc: 0.4610630407911001, val_acc: 0.41379310344827586, train_loss: 1.2414020882696097, val_loss: 1.2950321911591027 (17 / 35)
train_acc: 0.45982694684796044, val_acc: 0.45320197044334976, train_loss: 1.247909028759993, val_loss: 1.1969098944969365 (18 / 35)
train_acc: 0.47713226205191595, val_acc: 0.4482758620689655, train_loss: 1.170304676289906, val_loss: 1.2273060215517806 (19 / 35)
train_acc: 0.4969097651421508, val_acc: 0.4236453201970443, train_loss: 1.1553398920372773, val_loss: 1.2924360340011531 (20 / 35)
train_acc: 0.484548825710754, val_acc: 0.46798029556650245, train_loss: 1.1625919574860146, val_loss: 1.2184109637889955 (21 / 35)
train_acc: 0.5710754017305315, val_acc: 0.5369458128078818, train_loss: 1.027467166316524, val_loss: 1.1216952430790867 (22 / 35)
train_acc: 0.5933250927070457, val_acc: 0.5566502463054187, train_loss: 1.0142638446079344, val_loss: 1.0989681198972787 (23 / 35)
train_acc: 0.5871446229913473, val_acc: 0.5517241379310345, train_loss: 1.0046421767607312, val_loss: 1.090967190383103 (24 / 35)
train_acc: 0.5995055624227441, val_acc: 0.5369458128078818, train_loss: 0.9717621918956488, val_loss: 1.0957471762091069 (25 / 35)
train_acc: 0.5982694684796045, val_acc: 0.5320197044334976, train_loss: 0.9699762952784525, val_loss: 1.097665169702962 (26 / 35)
train_acc: 0.584672435105068, val_acc: 0.541871921182266, train_loss: 0.9893992272089379, val_loss: 1.0911218861640968 (27 / 35)
train_acc: 0.6081582200247219, val_acc: 0.5221674876847291, train_loss: 0.9474723768617374, val_loss: 1.0954438721017885 (28 / 35)
train_acc: 0.61557478368356, val_acc: 0.5320197044334976, train_loss: 0.9649574889240807, val_loss: 1.0923698798482642 (29 / 35)
train_acc: 0.6106304079110012, val_acc: 0.5270935960591133, train_loss: 0.9371635396268371, val_loss: 1.090869935715727 (30 / 35)
train_acc: 0.5995055624227441, val_acc: 0.5172413793103449, train_loss: 0.9297277673950006, val_loss: 1.0962487071605738 (31 / 35)
train_acc: 0.6056860321384425, val_acc: 0.5320197044334976, train_loss: 0.9427801590914779, val_loss: 1.0879029418740953 (32 / 35)
train_acc: 0.6180469715698393, val_acc: 0.5369458128078818, train_loss: 0.9359011716394695, val_loss: 1.0800885674107839 (33 / 35)
train_acc: 0.6093943139678616, val_acc: 0.5172413793103449, train_loss: 0.9320137881672721, val_loss: 1.0835999480902856 (34 / 35)
train_acc: 0.6341161928306551, val_acc: 0.5270935960591133, train_loss: 0.9231168619266105, val_loss: 1.08390245017747 (35 / 35)
lr 0.002012613859565053, batch 11, decay 7.395805296094665e-06, gamma 0.022057312149965858, val accuracy 0.5566502463054187, val loss 1.0989681198972787 [14 / 50]
-------------------------------------
train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.7757940566436616, val_loss: 1.759127798338829 (1 / 35)
train_acc: 0.24103831891223734, val_acc: 0.22660098522167488, train_loss: 1.7458158996697557, val_loss: 1.716014188498699 (2 / 35)
train_acc: 0.2904820766378245, val_acc: 0.33004926108374383, train_loss: 1.7008389993415334, val_loss: 1.6158697182321784 (3 / 35)
train_acc: 0.311495673671199, val_acc: 0.2512315270935961, train_loss: 1.6408852494986008, val_loss: 1.6048823618536512 (4 / 35)
train_acc: 0.3053152039555006, val_acc: 0.4088669950738916, train_loss: 1.6273595834397268, val_loss: 1.4909534695113233 (5 / 35)
train_acc: 0.35599505562422745, val_acc: 0.3842364532019704, train_loss: 1.5236040987396713, val_loss: 1.4348232414334865 (6 / 35)
train_acc: 0.4004944375772559, val_acc: 0.3842364532019704, train_loss: 1.4712052398323276, val_loss: 1.3832161042637425 (7 / 35)
train_acc: 0.3621755253399258, val_acc: 0.3694581280788177, train_loss: 1.461655443766798, val_loss: 1.4095436537207053 (8 / 35)