\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

%\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
%\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Emotion from music spectrograms}

\author{Matteo Cerutti\\
Politecnico di Torino\\
{\tt\small s265476@studenti.polito.it}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Antonio Santoro\\
Politecnico di Torino\\
{\tt\small s264014@studenti.polito.it}
\and
Marco Testa\\
Politecnico di Torino\\
{\tt\small s265861@studenti.polito.it}
}


\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Nowadays people need to have the possibility to select music and make playlists based on their mood. Many music platforms feature different music playlists made by hand that include popular and commercial songs aiming to maximise ratings. One of the most used feature on these platforms is to create playlists similar to other ones, the point is that all the songs that will be included are selected on the "similarity". 
Since the intention was to stay inside the computer vision domain, we have to treat audio files as images so the first idea was to exploit spectrograms. After some researches, we found that our idea was applied to classify song genres, therefore starting from the article of Piotr Kozakowski and Bartosz Michalak \cite{Kozakowski}, we adapted their work to our objective.
The interest is to train a neural network on different audio speeches that represent different human emotions, extract features and try to see whatever those peculiarities can be matched from music. Amiriparian \etal \cite{Amiriparian} showed that processing spectrograms into networks characterized by a different depth the result will change. This report presents results obtained from three networks, ResNet, VGG and GoogLeNet, trained on the RAVDESS Emotional song audio dataset \cite{Ravdess} and tested on the CAL500 dataset \cite{CAL500}.

\subsection{Classification pipeline}

Figure \ref{fig:pipeline} shows the model structure that classifies each slice and then after collecting all the predicted labels, the song will be classified by means of a voting algorithm.

\begin{figure*}
   \begin{center}
   \includegraphics[width=0.8\linewidth]{img/Pipeline-project}
   \end{center}
      \caption{Classification pipeline.}
   \label{fig:pipeline}
   \end{figure*}

\section{Data preparation}

\subsection{Training dataset}

The RAVDESS Emotional song audio consists of 1012 files of actors singing four seconds in a neutral North American accent. The portion used for this work includes calm, happy, sad, angry, and fearful emotions, each vocal is produced at two levels of emotional intensity, normal and strong.

Files are provided as .wav (16bit, 48kHz, mono) that need to be converted into a raw spectrogram. For the purpose "SoX (Sound eXchange) sound processing utilities" has been used. This tool can process audio files and do things like trimming or filters frequencies. Spectrograms for the training dataset have been generated to fit the input size of the three networks, furthermore, to cope with the limited size of the dataset, augmentation has been applied like random grey colorizing, brightness, contrast and hue variations. Amiriparian \etal \cite{Amiriparian} showed that using different shade of colour could exhibit different outcomes but, since the hue transformation made by the PyTorch framework has a not negligible impact on the brightness of the image and the goal is to not alter any information on the spectrogram, the best choice was to stick with the original shades (Figure \ref{fig:training_sample}), hence playing with the contrast and with a monochrome image could have a positive effect on capturing some features.

\begin{figure}[t]
   \begin{center}
   \includegraphics[width=0.8\linewidth]{img/happy_92.png}
   \end{center}
      \caption{Training spectrogram sample.}
   \label{fig:training_sample}
   \end{figure}

\subsection{Test dataset}

The CAL500 dataset contains 500 songs performed by 500 unique artists, each song has been annotated by at least three people using a standard survey. Files are provided as .mp3 (32kbps, mono) along with one or more labels.

In order to make the dataset compatible with our testing environment, two actions have been performed:
\begin{itemize}
\item \textbf{Filtering}: since the training label set was a subset of the CAL500 labels we selected only the songs which classes belong to the first set. Furthermore, an additional filtering step has been performed to remove all the redundant classes keeping only the relevant ones.
\item \textbf{Slicing}: the most challenging step was making the test dataset compatible with the training samples. To cope with the variable duration of each song, the extracted spectrograms have been sliced into squared images to fit to the network input size without losing any information. Each slice has been generated by sampling a proportional quantity of information equal to the training samples of four seconds. Given that, a different testing approach has been implemented.
\end{itemize}

\section{Training phase}

The research method used to find the optimal set of hyperparameters was the same for all networks\footnote {ResNet50 \& ResNet152, VGG11 \& VGG19 and GoogLeNet (Inception v1)} except for a slight difference related to GoogLeNet due to its three output branches. A lot of experiments have been done on different network variants of the same model to evaluate the impact of the networks' depth on the results.

The first step was to find a good starting hyperparameters set to make the network accomplish a full training. Due to Google Colab limitations, a random search has been used to evaluate 70 different hyperparameters sets, the best ones have been reported in Table \ref{tab:sets}.

\begin{table}
   \begin{center}
   \begin{tabular}{l|c|c|c|c}
   Network & LR & BS & WD & G \\
   \hline
   ResNet & toadd & toadd & toadd & toadd \\
   \hline
   VGG & 0.0008 & 8 & 2e-05 & 0.05 \\
   \hline
   GoogLeNet & 0.0001 & 8 & 5e-05 & 0.1 \\
   \end{tabular}
   \end{center}
   \caption{Best values per hyperparameter.}
   \label{tab:sets}
   \end{table}

Using the reported sets, all the networks have been trained for 100 epochs and evaluated using different split ratios between training set and validation set. The final values have been selected by doing some tuning by hand after evaluating the networks' perfomance during the epochs, moreover the values of each hyperparameter have been adjusted to address the problem of the high epochs number and to prevent the overfit. Since the training dataset is very small, we dealt with the overfit problem trying to solve it by means of data augmentation which led our training dataset to increase its size.

Table \ref{tab:accuracy} contains validation accuracies calculated on the validation set per network. The results referred to the first two networks are the averages between the single result of each network variant. 

\begin{table}
   \begin{center}
   \begin{tabular}{l|c}
   Network & Validation accuracy \\
   \hline
   ResNet & toadd  \\
   \hline
   VGG & 70\% \\
   \hline
   GoogLeNet & 82\% \\
   \end{tabular}
   \end{center}
   \caption{Average validation accuracy per network.}
   \label{tab:accuracy}
   \end{table}

\section{Testing}

\begin{figure*}
   \begin{center}
   \includegraphics[width=0.8\linewidth]{img/voting-system}
   \end{center}
      \caption{Voting system.}
   \label{fig:voting}
   \end{figure*}

\section{Conclusions}



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\end{document}
